{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPARK SQL APPLICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "print('Hello World!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2+4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPARK CONTEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on SparkContext in module pyspark.context object:\n",
      "\n",
      "class SparkContext(builtins.object)\n",
      " |  SparkContext(master: Optional[str] = None, appName: Optional[str] = None, sparkHome: Optional[str] = None, pyFiles: Optional[List[str]] = None, environment: Optional[Dict[str, Any]] = None, batchSize: int = 0, serializer: 'Serializer' = CloudPickleSerializer(), conf: Optional[pyspark.conf.SparkConf] = None, gateway: Optional[py4j.java_gateway.JavaGateway] = None, jsc: Optional[py4j.java_gateway.JavaObject] = None, profiler_cls: Type[pyspark.profiler.BasicProfiler] = <class 'pyspark.profiler.BasicProfiler'>, udf_profiler_cls: Type[pyspark.profiler.UDFBasicProfiler] = <class 'pyspark.profiler.UDFBasicProfiler'>, memory_profiler_cls: Type[pyspark.profiler.MemoryProfiler] = <class 'pyspark.profiler.MemoryProfiler'>)\n",
      " |  \n",
      " |  Main entry point for Spark functionality. A SparkContext represents the\n",
      " |  connection to a Spark cluster, and can be used to create :class:`RDD` and\n",
      " |  broadcast variables on that cluster.\n",
      " |  \n",
      " |  When you create a new SparkContext, at least the master and app name should\n",
      " |  be set, either through the named parameters here or through `conf`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  master : str, optional\n",
      " |      Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).\n",
      " |  appName : str, optional\n",
      " |      A name for your job, to display on the cluster web UI.\n",
      " |  sparkHome : str, optional\n",
      " |      Location where Spark is installed on cluster nodes.\n",
      " |  pyFiles : list, optional\n",
      " |      Collection of .zip or .py files to send to the cluster\n",
      " |      and add to PYTHONPATH.  These can be paths on the local file\n",
      " |      system or HDFS, HTTP, HTTPS, or FTP URLs.\n",
      " |  environment : dict, optional\n",
      " |      A dictionary of environment variables to set on\n",
      " |      worker nodes.\n",
      " |  batchSize : int, optional, default 0\n",
      " |      The number of Python objects represented as a single\n",
      " |      Java object. Set 1 to disable batching, 0 to automatically choose\n",
      " |      the batch size based on object sizes, or -1 to use an unlimited\n",
      " |      batch size\n",
      " |  serializer : :class:`Serializer`, optional, default :class:`CPickleSerializer`\n",
      " |      The serializer for RDDs.\n",
      " |  conf : :class:`SparkConf`, optional\n",
      " |      An object setting Spark properties.\n",
      " |  gateway : class:`py4j.java_gateway.JavaGateway`,  optional\n",
      " |      Use an existing gateway and JVM, otherwise a new JVM\n",
      " |      will be instantiated. This is only used internally.\n",
      " |  jsc : class:`py4j.java_gateway.JavaObject`, optional\n",
      " |      The JavaSparkContext instance. This is only used internally.\n",
      " |  profiler_cls : type, optional, default :class:`BasicProfiler`\n",
      " |      A class of custom Profiler used to do profiling\n",
      " |  udf_profiler_cls : type, optional, default :class:`UDFBasicProfiler`\n",
      " |      A class of custom Profiler used to do udf profiling\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  Only one :class:`SparkContext` should be active per JVM. You must `stop()`\n",
      " |  the active :class:`SparkContext` before creating a new one.\n",
      " |  \n",
      " |  :class:`SparkContext` instance is not supported to share across multiple\n",
      " |  processes out of the box, and PySpark does not guarantee multi-processing execution.\n",
      " |  Use threads instead for concurrent processing purpose.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from pyspark.context import SparkContext\n",
      " |  >>> sc = SparkContext('local', 'test')\n",
      " |  >>> sc2 = SparkContext('local', 'test2') # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |  Traceback (most recent call last):\n",
      " |      ...\n",
      " |  ValueError: ...\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __enter__(self) -> 'SparkContext'\n",
      " |      Enable 'with SparkContext(...) as sc: app(sc)' syntax.\n",
      " |  \n",
      " |  __exit__(self, type: Optional[Type[BaseException]], value: Optional[BaseException], trace: Optional[traceback]) -> None\n",
      " |      Enable 'with SparkContext(...) as sc: app' syntax.\n",
      " |      \n",
      " |      Specifically stop the context on exit of the with block.\n",
      " |  \n",
      " |  __getnewargs__(self) -> NoReturn\n",
      " |  \n",
      " |  __init__(self, master: Optional[str] = None, appName: Optional[str] = None, sparkHome: Optional[str] = None, pyFiles: Optional[List[str]] = None, environment: Optional[Dict[str, Any]] = None, batchSize: int = 0, serializer: 'Serializer' = CloudPickleSerializer(), conf: Optional[pyspark.conf.SparkConf] = None, gateway: Optional[py4j.java_gateway.JavaGateway] = None, jsc: Optional[py4j.java_gateway.JavaObject] = None, profiler_cls: Type[pyspark.profiler.BasicProfiler] = <class 'pyspark.profiler.BasicProfiler'>, udf_profiler_cls: Type[pyspark.profiler.UDFBasicProfiler] = <class 'pyspark.profiler.UDFBasicProfiler'>, memory_profiler_cls: Type[pyspark.profiler.MemoryProfiler] = <class 'pyspark.profiler.MemoryProfiler'>)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  accumulator(self, value: ~T, accum_param: Optional[ForwardRef('AccumulatorParam[T]')] = None) -> 'Accumulator[T]'\n",
      " |      Create an :class:`Accumulator` with the given initial value, using a given\n",
      " |      :class:`AccumulatorParam` helper object to define how to add values of the\n",
      " |      data type if provided. Default AccumulatorParams are used for integers\n",
      " |      and floating-point numbers if you do not provide one. For other types,\n",
      " |      a custom AccumulatorParam can be used.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      value : T\n",
      " |          initialized value\n",
      " |      accum_param : :class:`pyspark.AccumulatorParam`, optional\n",
      " |          helper object to define how to add values\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`Accumulator`\n",
      " |          `Accumulator` object, a shared variable that can be accumulated\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> acc = sc.accumulator(9)\n",
      " |      >>> acc.value\n",
      " |      9\n",
      " |      >>> acc += 1\n",
      " |      >>> acc.value\n",
      " |      10\n",
      " |      \n",
      " |      Accumulator object can be accumulated in RDD operations:\n",
      " |      \n",
      " |      >>> rdd = sc.range(5)\n",
      " |      >>> def f(x):\n",
      " |      ...     global acc\n",
      " |      ...     acc += 1\n",
      " |      >>> rdd.foreach(f)\n",
      " |      >>> acc.value\n",
      " |      15\n",
      " |  \n",
      " |  addArchive(self, path: str) -> None\n",
      " |      Add an archive to be downloaded with this Spark job on every node.\n",
      " |      The `path` passed can be either a local file, a file in HDFS\n",
      " |      (or other Hadoop-supported filesystems), or an HTTP, HTTPS or\n",
      " |      FTP URI.\n",
      " |      \n",
      " |      To access the file in Spark jobs, use :meth:`SparkFiles.get` with the\n",
      " |      filename to find its download/unpacked location. The given path should\n",
      " |      be one of .zip, .tar, .tar.gz, .tgz and .jar.\n",
      " |      \n",
      " |      .. versionadded:: 3.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          can be either a local file, a file in HDFS (or other Hadoop-supported\n",
      " |          filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs,\n",
      " |          use :meth:`SparkFiles.get` to find its download location.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.listArchives`\n",
      " |      :meth:`SparkFiles.get`\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      A path can be added only once. Subsequent additions of the same path are ignored.\n",
      " |      This API is experimental.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Creates a zipped file that contains a text file written '100'.\n",
      " |      \n",
      " |      >>> import os\n",
      " |      >>> import tempfile\n",
      " |      >>> import zipfile\n",
      " |      >>> from pyspark import SparkFiles\n",
      " |      \n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     path = os.path.join(d, \"test.txt\")\n",
      " |      ...     with open(path, \"w\") as f:\n",
      " |      ...         _ = f.write(\"100\")\n",
      " |      ...\n",
      " |      ...     zip_path1 = os.path.join(d, \"test1.zip\")\n",
      " |      ...     with zipfile.ZipFile(zip_path1, \"w\", zipfile.ZIP_DEFLATED) as z:\n",
      " |      ...         z.write(path, os.path.basename(path))\n",
      " |      ...\n",
      " |      ...     zip_path2 = os.path.join(d, \"test2.zip\")\n",
      " |      ...     with zipfile.ZipFile(zip_path2, \"w\", zipfile.ZIP_DEFLATED) as z:\n",
      " |      ...         z.write(path, os.path.basename(path))\n",
      " |      ...\n",
      " |      ...     sc.addArchive(zip_path1)\n",
      " |      ...     arch_list1 = sorted(sc.listArchives)\n",
      " |      ...\n",
      " |      ...     sc.addArchive(zip_path2)\n",
      " |      ...     arch_list2 = sorted(sc.listArchives)\n",
      " |      ...\n",
      " |      ...     # add zip_path2 twice, this addition will be ignored\n",
      " |      ...     sc.addArchive(zip_path2)\n",
      " |      ...     arch_list3 = sorted(sc.listArchives)\n",
      " |      ...\n",
      " |      ...     def func(iterator):\n",
      " |      ...         with open(\"%s/test.txt\" % SparkFiles.get(\"test1.zip\")) as f:\n",
      " |      ...             mul = int(f.readline())\n",
      " |      ...             return [x * mul for x in iterator]\n",
      " |      ...\n",
      " |      ...     collected = sc.parallelize([1, 2, 3, 4]).mapPartitions(func).collect()\n",
      " |      \n",
      " |      >>> arch_list1\n",
      " |      ['file:/.../test1.zip']\n",
      " |      >>> arch_list2\n",
      " |      ['file:/.../test1.zip', 'file:/.../test2.zip']\n",
      " |      >>> arch_list3\n",
      " |      ['file:/.../test1.zip', 'file:/.../test2.zip']\n",
      " |      >>> collected\n",
      " |      [100, 200, 300, 400]\n",
      " |  \n",
      " |  addFile(self, path: str, recursive: bool = False) -> None\n",
      " |      Add a file to be downloaded with this Spark job on every node.\n",
      " |      The `path` passed can be either a local file, a file in HDFS\n",
      " |      (or other Hadoop-supported filesystems), or an HTTP, HTTPS or\n",
      " |      FTP URI.\n",
      " |      \n",
      " |      To access the file in Spark jobs, use :meth:`SparkFiles.get` with the\n",
      " |      filename to find its download location.\n",
      " |      \n",
      " |      A directory can be given if the recursive option is set to True.\n",
      " |      Currently directories are only supported for Hadoop-supported filesystems.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          can be either a local file, a file in HDFS (or other Hadoop-supported\n",
      " |          filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs,\n",
      " |          use :meth:`SparkFiles.get` to find its download location.\n",
      " |      recursive : bool, default False\n",
      " |          whether to recursively add files in the input directory\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.listFiles`\n",
      " |      :meth:`SparkContext.addPyFile`\n",
      " |      :meth:`SparkFiles.get`\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      A path can be added only once. Subsequent additions of the same path are ignored.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import os\n",
      " |      >>> import tempfile\n",
      " |      >>> from pyspark import SparkFiles\n",
      " |      \n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     path1 = os.path.join(d, \"test1.txt\")\n",
      " |      ...     with open(path1, \"w\") as f:\n",
      " |      ...         _ = f.write(\"100\")\n",
      " |      ...\n",
      " |      ...     path2 = os.path.join(d, \"test2.txt\")\n",
      " |      ...     with open(path2, \"w\") as f:\n",
      " |      ...         _ = f.write(\"200\")\n",
      " |      ...\n",
      " |      ...     sc.addFile(path1)\n",
      " |      ...     file_list1 = sorted(sc.listFiles)\n",
      " |      ...\n",
      " |      ...     sc.addFile(path2)\n",
      " |      ...     file_list2 = sorted(sc.listFiles)\n",
      " |      ...\n",
      " |      ...     # add path2 twice, this addition will be ignored\n",
      " |      ...     sc.addFile(path2)\n",
      " |      ...     file_list3 = sorted(sc.listFiles)\n",
      " |      ...\n",
      " |      ...     def func(iterator):\n",
      " |      ...         with open(SparkFiles.get(\"test1.txt\")) as f:\n",
      " |      ...             mul = int(f.readline())\n",
      " |      ...             return [x * mul for x in iterator]\n",
      " |      ...\n",
      " |      ...     collected = sc.parallelize([1, 2, 3, 4]).mapPartitions(func).collect()\n",
      " |      \n",
      " |      >>> file_list1\n",
      " |      ['file:/.../test1.txt']\n",
      " |      >>> file_list2\n",
      " |      ['file:/.../test1.txt', 'file:/.../test2.txt']\n",
      " |      >>> file_list3\n",
      " |      ['file:/.../test1.txt', 'file:/.../test2.txt']\n",
      " |      >>> collected\n",
      " |      [100, 200, 300, 400]\n",
      " |  \n",
      " |  addPyFile(self, path: str) -> None\n",
      " |      Add a .py or .zip dependency for all tasks to be executed on this\n",
      " |      SparkContext in the future.  The `path` passed can be either a local\n",
      " |      file, a file in HDFS (or other Hadoop-supported filesystems), or an\n",
      " |      HTTP, HTTPS or FTP URI.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          can be either a .py file or .zip dependency.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.addFile`\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      A path can be added only once. Subsequent additions of the same path are ignored.\n",
      " |  \n",
      " |  binaryFiles(self, path: str, minPartitions: Optional[int] = None) -> pyspark.rdd.RDD[typing.Tuple[str, bytes]]\n",
      " |      Read a directory of binary files from HDFS, a local file system\n",
      " |      (available on all nodes), or any Hadoop-supported file system URI\n",
      " |      as a byte array. Each file is read as a single record and returned\n",
      " |      in a key-value pair, where the key is the path of each file, the\n",
      " |      value is the content of each file.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          directory to the input data files, the path can be comma separated\n",
      " |          paths as a list of inputs\n",
      " |      minPartitions : int, optional\n",
      " |          suggested minimum number of partitions for the resulting RDD\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          RDD representing path-content pairs from the file(s).\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Small files are preferred, large file is also allowable, but may cause bad performance.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.binaryRecords`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import os\n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a temporary binary file\n",
      " |      ...     with open(os.path.join(d, \"1.bin\"), \"wb\") as f1:\n",
      " |      ...         _ = f1.write(b\"binary data I\")\n",
      " |      ...\n",
      " |      ...     # Write another temporary binary file\n",
      " |      ...     with open(os.path.join(d, \"2.bin\"), \"wb\") as f2:\n",
      " |      ...         _ = f2.write(b\"binary data II\")\n",
      " |      ...\n",
      " |      ...     collected = sorted(sc.binaryFiles(d).collect())\n",
      " |      \n",
      " |      >>> collected\n",
      " |      [('.../1.bin', b'binary data I'), ('.../2.bin', b'binary data II')]\n",
      " |  \n",
      " |  binaryRecords(self, path: str, recordLength: int) -> pyspark.rdd.RDD[bytes]\n",
      " |      Load data from a flat binary file, assuming each record is a set of numbers\n",
      " |      with the specified numerical format (see ByteBuffer), and the number of\n",
      " |      bytes per record is constant.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          Directory to the input data files\n",
      " |      recordLength : int\n",
      " |          The length at which to split the records\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          RDD of data with values, represented as byte arrays\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.binaryFiles`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import os\n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a temporary file\n",
      " |      ...     with open(os.path.join(d, \"1.bin\"), \"w\") as f:\n",
      " |      ...         for i in range(3):\n",
      " |      ...             _ = f.write(\"%04d\" % i)\n",
      " |      ...\n",
      " |      ...     # Write another file\n",
      " |      ...     with open(os.path.join(d, \"2.bin\"), \"w\") as f:\n",
      " |      ...         for i in [-1, -2, -10]:\n",
      " |      ...             _ = f.write(\"%04d\" % i)\n",
      " |      ...\n",
      " |      ...     collected = sorted(sc.binaryRecords(d, 4).collect())\n",
      " |      \n",
      " |      >>> collected\n",
      " |      [b'-001', b'-002', b'-010', b'0000', b'0001', b'0002']\n",
      " |  \n",
      " |  broadcast(self, value: ~T) -> 'Broadcast[T]'\n",
      " |      Broadcast a read-only variable to the cluster, returning a :class:`Broadcast`\n",
      " |      object for reading it in distributed functions. The variable will\n",
      " |      be sent to each cluster only once.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      value : T\n",
      " |          value to broadcast to the Spark nodes\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`Broadcast`\n",
      " |          :class:`Broadcast` object, a read-only variable cached on each machine\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> mapping = {1: 10001, 2: 10002}\n",
      " |      >>> bc = sc.broadcast(mapping)\n",
      " |      \n",
      " |      >>> rdd = sc.range(5)\n",
      " |      >>> rdd2 = rdd.map(lambda i: bc.value[i] if i in bc.value else -1)\n",
      " |      >>> rdd2.collect()\n",
      " |      [-1, 10001, 10002, -1, -1]\n",
      " |      \n",
      " |      >>> bc.destroy()\n",
      " |  \n",
      " |  cancelAllJobs(self) -> None\n",
      " |      Cancel all jobs that have been scheduled or are running.\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.cancelJobGroup`\n",
      " |      :meth:`SparkContext.runJob`\n",
      " |  \n",
      " |  cancelJobGroup(self, groupId: str) -> None\n",
      " |      Cancel active jobs for the specified group. See :meth:`SparkContext.setJobGroup`.\n",
      " |      for more information.\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      groupId : str\n",
      " |          The group ID to cancel the job.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.setJobGroup`\n",
      " |      :meth:`SparkContext.cancelJobGroup`\n",
      " |  \n",
      " |  dump_profiles(self, path: str) -> None\n",
      " |      Dump the profile stats into directory `path`\n",
      " |      \n",
      " |      .. versionadded:: 1.2.0\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.show_profiles`\n",
      " |  \n",
      " |  emptyRDD(self) -> pyspark.rdd.RDD[typing.Any]\n",
      " |      Create an :class:`RDD` that has no partitions or elements.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          An empty RDD\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.emptyRDD()\n",
      " |      EmptyRDD...\n",
      " |      >>> sc.emptyRDD().count()\n",
      " |      0\n",
      " |  \n",
      " |  getCheckpointDir(self) -> Optional[str]\n",
      " |      Return the directory where RDDs are checkpointed. Returns None if no\n",
      " |      checkpoint directory has been set.\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.setCheckpointDir`\n",
      " |      :meth:`RDD.checkpoint`\n",
      " |      :meth:`RDD.getCheckpointFile`\n",
      " |  \n",
      " |  getConf(self) -> pyspark.conf.SparkConf\n",
      " |      Return a copy of this SparkContext's configuration :class:`SparkConf`.\n",
      " |      \n",
      " |      .. versionadded:: 2.1.0\n",
      " |  \n",
      " |  getLocalProperty(self, key: str) -> Optional[str]\n",
      " |      Get a local property set in this thread, or null if it is missing. See\n",
      " |      :meth:`setLocalProperty`.\n",
      " |      \n",
      " |      .. versionadded:: 1.0.0\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.setLocalProperty`\n",
      " |  \n",
      " |  hadoopFile(self, path: str, inputFormatClass: str, keyClass: str, valueClass: str, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None, batchSize: int = 0) -> pyspark.rdd.RDD[typing.Tuple[~T, ~U]]\n",
      " |      Read an 'old' Hadoop InputFormat with arbitrary key and value class from HDFS,\n",
      " |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
      " |      The mechanism is the same as for meth:`SparkContext.sequenceFile`.\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |      \n",
      " |      A Hadoop configuration can be passed in as a Python dict. This will be converted into a\n",
      " |      Configuration in Java.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          path to Hadoop file\n",
      " |      inputFormatClass : str\n",
      " |          fully qualified classname of Hadoop InputFormat\n",
      " |          (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n",
      " |      keyClass : str\n",
      " |          fully qualified classname of key Writable class (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      valueClass : str\n",
      " |          fully qualified classname of value Writable class\n",
      " |          (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      keyConverter : str, optional\n",
      " |          fully qualified name of a function returning key WritableConverter\n",
      " |      valueConverter : str, optional\n",
      " |          fully qualified name of a function returning value WritableConverter\n",
      " |      conf : dict, optional\n",
      " |          Hadoop configuration, passed in as a dict\n",
      " |      batchSize : int, optional, default 0\n",
      " |          The number of Python objects represented as a single\n",
      " |          Java object. (default 0, choose batchSize automatically)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          RDD of tuples of key and corresponding value\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.saveAsSequenceFile`\n",
      " |      :meth:`RDD.saveAsNewAPIHadoopFile`\n",
      " |      :meth:`RDD.saveAsHadoopFile`\n",
      " |      :meth:`SparkContext.newAPIHadoopFile`\n",
      " |      :meth:`SparkContext.hadoopRDD`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import os\n",
      " |      >>> import tempfile\n",
      " |      \n",
      " |      Set the related classes\n",
      " |      \n",
      " |      >>> output_format_class = \"org.apache.hadoop.mapred.TextOutputFormat\"\n",
      " |      >>> input_format_class = \"org.apache.hadoop.mapred.TextInputFormat\"\n",
      " |      >>> key_class = \"org.apache.hadoop.io.IntWritable\"\n",
      " |      >>> value_class = \"org.apache.hadoop.io.Text\"\n",
      " |      \n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     path = os.path.join(d, \"old_hadoop_file\")\n",
      " |      ...\n",
      " |      ...     # Write a temporary Hadoop file\n",
      " |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n",
      " |      ...     rdd.saveAsHadoopFile(path, output_format_class, key_class, value_class)\n",
      " |      ...\n",
      " |      ...     loaded = sc.hadoopFile(path, input_format_class, key_class, value_class)\n",
      " |      ...     collected = sorted(loaded.collect())\n",
      " |      \n",
      " |      >>> collected\n",
      " |      [(0, '1\\t'), (0, '1\\ta'), (0, '3\\tx')]\n",
      " |  \n",
      " |  hadoopRDD(self, inputFormatClass: str, keyClass: str, valueClass: str, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None, batchSize: int = 0) -> pyspark.rdd.RDD[typing.Tuple[~T, ~U]]\n",
      " |      Read an 'old' Hadoop InputFormat with arbitrary key and value class, from an arbitrary\n",
      " |      Hadoop configuration, which is passed in as a Python dict.\n",
      " |      This will be converted into a Configuration in Java.\n",
      " |      The mechanism is the same as for meth:`SparkContext.sequenceFile`.\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      inputFormatClass : str\n",
      " |          fully qualified classname of Hadoop InputFormat\n",
      " |          (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n",
      " |      keyClass : str\n",
      " |          fully qualified classname of key Writable class (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      valueClass : str\n",
      " |          fully qualified classname of value Writable class\n",
      " |          (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      keyConverter : str, optional\n",
      " |          fully qualified name of a function returning key WritableConverter\n",
      " |      valueConverter : str, optional\n",
      " |          fully qualified name of a function returning value WritableConverter\n",
      " |      conf : dict, optional\n",
      " |          Hadoop configuration, passed in as a dict\n",
      " |      batchSize : int, optional, default 0\n",
      " |          The number of Python objects represented as a single\n",
      " |          Java object. (default 0, choose batchSize automatically)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          RDD of tuples of key and corresponding value\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.saveAsNewAPIHadoopDataset`\n",
      " |      :meth:`RDD.saveAsHadoopDataset`\n",
      " |      :meth:`SparkContext.newAPIHadoopRDD`\n",
      " |      :meth:`SparkContext.hadoopFile`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import os\n",
      " |      >>> import tempfile\n",
      " |      \n",
      " |      Set the related classes\n",
      " |      \n",
      " |      >>> output_format_class = \"org.apache.hadoop.mapred.TextOutputFormat\"\n",
      " |      >>> input_format_class = \"org.apache.hadoop.mapred.TextInputFormat\"\n",
      " |      >>> key_class = \"org.apache.hadoop.io.IntWritable\"\n",
      " |      >>> value_class = \"org.apache.hadoop.io.Text\"\n",
      " |      \n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     path = os.path.join(d, \"old_hadoop_file\")\n",
      " |      ...\n",
      " |      ...     # Create the conf for writing\n",
      " |      ...     write_conf = {\n",
      " |      ...         \"mapred.output.format.class\": output_format_class,\n",
      " |      ...         \"mapreduce.job.output.key.class\": key_class,\n",
      " |      ...         \"mapreduce.job.output.value.class\": value_class,\n",
      " |      ...         \"mapreduce.output.fileoutputformat.outputdir\": path,\n",
      " |      ...     }\n",
      " |      ...\n",
      " |      ...     # Write a temporary Hadoop file\n",
      " |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n",
      " |      ...     rdd.saveAsHadoopDataset(conf=write_conf)\n",
      " |      ...\n",
      " |      ...     # Create the conf for reading\n",
      " |      ...     read_conf = {\"mapreduce.input.fileinputformat.inputdir\": path}\n",
      " |      ...\n",
      " |      ...     loaded = sc.hadoopRDD(input_format_class, key_class, value_class, conf=read_conf)\n",
      " |      ...     collected = sorted(loaded.collect())\n",
      " |      \n",
      " |      >>> collected\n",
      " |      [(0, '1\\t'), (0, '1\\ta'), (0, '3\\tx')]\n",
      " |  \n",
      " |  newAPIHadoopFile(self, path: str, inputFormatClass: str, keyClass: str, valueClass: str, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None, batchSize: int = 0) -> pyspark.rdd.RDD[typing.Tuple[~T, ~U]]\n",
      " |      Read a 'new API' Hadoop InputFormat with arbitrary key and value class from HDFS,\n",
      " |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
      " |      The mechanism is the same as for meth:`SparkContext.sequenceFile`.\n",
      " |      \n",
      " |      A Hadoop configuration can be passed in as a Python dict. This will be converted into a\n",
      " |      Configuration in Java\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          path to Hadoop file\n",
      " |      inputFormatClass : str\n",
      " |          fully qualified classname of Hadoop InputFormat\n",
      " |          (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n",
      " |      keyClass : str\n",
      " |          fully qualified classname of key Writable class\n",
      " |          (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      valueClass : str\n",
      " |          fully qualified classname of value Writable class\n",
      " |          (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      keyConverter : str, optional\n",
      " |          fully qualified name of a function returning key WritableConverter\n",
      " |          None by default\n",
      " |      valueConverter : str, optional\n",
      " |          fully qualified name of a function returning value WritableConverter\n",
      " |          None by default\n",
      " |      conf : dict, optional\n",
      " |          Hadoop configuration, passed in as a dict\n",
      " |          None by default\n",
      " |      batchSize : int, optional, default 0\n",
      " |          The number of Python objects represented as a single\n",
      " |          Java object. (default 0, choose batchSize automatically)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          RDD of tuples of key and corresponding value\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.saveAsSequenceFile`\n",
      " |      :meth:`RDD.saveAsNewAPIHadoopFile`\n",
      " |      :meth:`RDD.saveAsHadoopFile`\n",
      " |      :meth:`SparkContext.sequenceFile`\n",
      " |      :meth:`SparkContext.hadoopFile`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import os\n",
      " |      >>> import tempfile\n",
      " |      \n",
      " |      Set the related classes\n",
      " |      \n",
      " |      >>> output_format_class = \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\"\n",
      " |      >>> input_format_class = \"org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat\"\n",
      " |      >>> key_class = \"org.apache.hadoop.io.IntWritable\"\n",
      " |      >>> value_class = \"org.apache.hadoop.io.Text\"\n",
      " |      \n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     path = os.path.join(d, \"new_hadoop_file\")\n",
      " |      ...\n",
      " |      ...     # Write a temporary Hadoop file\n",
      " |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n",
      " |      ...     rdd.saveAsNewAPIHadoopFile(path, output_format_class, key_class, value_class)\n",
      " |      ...\n",
      " |      ...     loaded = sc.newAPIHadoopFile(path, input_format_class, key_class, value_class)\n",
      " |      ...     collected = sorted(loaded.collect())\n",
      " |      \n",
      " |      >>> collected\n",
      " |      [(1, ''), (1, 'a'), (3, 'x')]\n",
      " |  \n",
      " |  newAPIHadoopRDD(self, inputFormatClass: str, keyClass: str, valueClass: str, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None, batchSize: int = 0) -> pyspark.rdd.RDD[typing.Tuple[~T, ~U]]\n",
      " |      Read a 'new API' Hadoop InputFormat with arbitrary key and value class, from an arbitrary\n",
      " |      Hadoop configuration, which is passed in as a Python dict.\n",
      " |      This will be converted into a Configuration in Java.\n",
      " |      The mechanism is the same as for meth:`SparkContext.sequenceFile`.\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      inputFormatClass : str\n",
      " |          fully qualified classname of Hadoop InputFormat\n",
      " |          (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n",
      " |      keyClass : str\n",
      " |          fully qualified classname of key Writable class (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      valueClass : str\n",
      " |          fully qualified classname of value Writable class\n",
      " |          (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      keyConverter : str, optional\n",
      " |          fully qualified name of a function returning key WritableConverter\n",
      " |          (None by default)\n",
      " |      valueConverter : str, optional\n",
      " |          fully qualified name of a function returning value WritableConverter\n",
      " |          (None by default)\n",
      " |      conf : dict, optional\n",
      " |          Hadoop configuration, passed in as a dict (None by default)\n",
      " |      batchSize : int, optional, default 0\n",
      " |          The number of Python objects represented as a single\n",
      " |          Java object. (default 0, choose batchSize automatically)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          RDD of tuples of key and corresponding value\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.saveAsNewAPIHadoopDataset`\n",
      " |      :meth:`RDD.saveAsHadoopDataset`\n",
      " |      :meth:`SparkContext.hadoopRDD`\n",
      " |      :meth:`SparkContext.hadoopFile`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import os\n",
      " |      >>> import tempfile\n",
      " |      \n",
      " |      Set the related classes\n",
      " |      \n",
      " |      >>> output_format_class = \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\"\n",
      " |      >>> input_format_class = \"org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat\"\n",
      " |      >>> key_class = \"org.apache.hadoop.io.IntWritable\"\n",
      " |      >>> value_class = \"org.apache.hadoop.io.Text\"\n",
      " |      \n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     path = os.path.join(d, \"new_hadoop_file\")\n",
      " |      ...\n",
      " |      ...     # Create the conf for writing\n",
      " |      ...     write_conf = {\n",
      " |      ...         \"mapreduce.job.outputformat.class\": (output_format_class),\n",
      " |      ...         \"mapreduce.job.output.key.class\": key_class,\n",
      " |      ...         \"mapreduce.job.output.value.class\": value_class,\n",
      " |      ...         \"mapreduce.output.fileoutputformat.outputdir\": path,\n",
      " |      ...     }\n",
      " |      ...\n",
      " |      ...     # Write a temporary Hadoop file\n",
      " |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n",
      " |      ...     rdd.saveAsNewAPIHadoopDataset(conf=write_conf)\n",
      " |      ...\n",
      " |      ...     # Create the conf for reading\n",
      " |      ...     read_conf = {\"mapreduce.input.fileinputformat.inputdir\": path}\n",
      " |      ...\n",
      " |      ...     loaded = sc.newAPIHadoopRDD(input_format_class,\n",
      " |      ...         key_class, value_class, conf=read_conf)\n",
      " |      ...     collected = sorted(loaded.collect())\n",
      " |      \n",
      " |      >>> collected\n",
      " |      [(1, ''), (1, 'a'), (3, 'x')]\n",
      " |  \n",
      " |  parallelize(self, c: Iterable[~T], numSlices: Optional[int] = None) -> pyspark.rdd.RDD[~T]\n",
      " |      Distribute a local Python collection to form an RDD. Using range\n",
      " |      is recommended if the input represents a range for performance.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      c : :class:`collections.abc.Iterable`\n",
      " |          iterable collection to distribute\n",
      " |      numSlices : int, optional\n",
      " |          the number of partitions of the new RDD\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          RDD representing distributed collection.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()\n",
      " |      [[0], [2], [3], [4], [6]]\n",
      " |      >>> sc.parallelize(range(0, 6, 2), 5).glom().collect()\n",
      " |      [[], [0], [], [2], [4]]\n",
      " |      \n",
      " |      Deal with a list of strings.\n",
      " |      \n",
      " |      >>> strings = [\"a\", \"b\", \"c\"]\n",
      " |      >>> sc.parallelize(strings, 2).glom().collect()\n",
      " |      [['a'], ['b', 'c']]\n",
      " |  \n",
      " |  pickleFile(self, name: str, minPartitions: Optional[int] = None) -> pyspark.rdd.RDD[typing.Any]\n",
      " |      Load an RDD previously saved using :meth:`RDD.saveAsPickleFile` method.\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          directory to the input data files, the path can be comma separated\n",
      " |          paths as a list of inputs\n",
      " |      minPartitions : int, optional\n",
      " |          suggested minimum number of partitions for the resulting RDD\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          RDD representing unpickled data from the file(s).\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.saveAsPickleFile`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import os\n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a temporary pickled file\n",
      " |      ...     path1 = os.path.join(d, \"pickled1\")\n",
      " |      ...     sc.parallelize(range(10)).saveAsPickleFile(path1, 3)\n",
      " |      ...\n",
      " |      ...     # Write another temporary pickled file\n",
      " |      ...     path2 = os.path.join(d, \"pickled2\")\n",
      " |      ...     sc.parallelize(range(-10, -5)).saveAsPickleFile(path2, 3)\n",
      " |      ...\n",
      " |      ...     # Load picked file\n",
      " |      ...     collected1 = sorted(sc.pickleFile(path1, 3).collect())\n",
      " |      ...     collected2 = sorted(sc.pickleFile(path2, 4).collect())\n",
      " |      ...\n",
      " |      ...     # Load two picked files together\n",
      " |      ...     collected3 = sorted(sc.pickleFile('{},{}'.format(path1, path2), 5).collect())\n",
      " |      \n",
      " |      >>> collected1\n",
      " |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      " |      >>> collected2\n",
      " |      [-10, -9, -8, -7, -6]\n",
      " |      >>> collected3\n",
      " |      [-10, -9, -8, -7, -6, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      " |  \n",
      " |  range(self, start: int, end: Optional[int] = None, step: int = 1, numSlices: Optional[int] = None) -> pyspark.rdd.RDD[int]\n",
      " |      Create a new RDD of int containing elements from `start` to `end`\n",
      " |      (exclusive), increased by `step` every element. Can be called the same\n",
      " |      way as python's built-in range() function. If called with a single argument,\n",
      " |      the argument is interpreted as `end`, and `start` is set to 0.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      start : int\n",
      " |          the start value\n",
      " |      end : int, optional\n",
      " |          the end value (exclusive)\n",
      " |      step : int, optional, default 1\n",
      " |          the incremental step\n",
      " |      numSlices : int, optional\n",
      " |          the number of partitions of the new RDD\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          An RDD of int\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`pyspark.sql.SparkSession.range`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.range(5).collect()\n",
      " |      [0, 1, 2, 3, 4]\n",
      " |      >>> sc.range(2, 4).collect()\n",
      " |      [2, 3]\n",
      " |      >>> sc.range(1, 7, 2).collect()\n",
      " |      [1, 3, 5]\n",
      " |      \n",
      " |      Generate RDD with a negative step\n",
      " |      \n",
      " |      >>> sc.range(5, 0, -1).collect()\n",
      " |      [5, 4, 3, 2, 1]\n",
      " |      >>> sc.range(0, 5, -1).collect()\n",
      " |      []\n",
      " |      \n",
      " |      Control the number of partitions\n",
      " |      \n",
      " |      >>> sc.range(5, numSlices=1).getNumPartitions()\n",
      " |      1\n",
      " |      >>> sc.range(5, numSlices=10).getNumPartitions()\n",
      " |      10\n",
      " |  \n",
      " |  runJob(self, rdd: pyspark.rdd.RDD[~T], partitionFunc: Callable[[Iterable[~T]], Iterable[~U]], partitions: Optional[Sequence[int]] = None, allowLocal: bool = False) -> List[~U]\n",
      " |      Executes the given partitionFunc on the specified set of partitions,\n",
      " |      returning the result as an array of elements.\n",
      " |      \n",
      " |      If 'partitions' is not specified, this will run over all partitions.\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      rdd : :class:`RDD`\n",
      " |          target RDD to run tasks on\n",
      " |      partitionFunc : function\n",
      " |          a function to run on each partition of the RDD\n",
      " |      partitions : list, optional\n",
      " |          set of partitions to run on; some jobs may not want to compute on all\n",
      " |          partitions of the target RDD, e.g. for operations like `first`\n",
      " |      allowLocal : bool, default False\n",
      " |          this parameter takes no effect\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          results of specified partitions\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.cancelAllJobs`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> myRDD = sc.parallelize(range(6), 3)\n",
      " |      >>> sc.runJob(myRDD, lambda part: [x * x for x in part])\n",
      " |      [0, 1, 4, 9, 16, 25]\n",
      " |      \n",
      " |      >>> myRDD = sc.parallelize(range(6), 3)\n",
      " |      >>> sc.runJob(myRDD, lambda part: [x * x for x in part], [0, 2], True)\n",
      " |      [0, 1, 16, 25]\n",
      " |  \n",
      " |  sequenceFile(self, path: str, keyClass: Optional[str] = None, valueClass: Optional[str] = None, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, minSplits: Optional[int] = None, batchSize: int = 0) -> pyspark.rdd.RDD[typing.Tuple[~T, ~U]]\n",
      " |      Read a Hadoop SequenceFile with arbitrary key and value Writable class from HDFS,\n",
      " |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
      " |      The mechanism is as follows:\n",
      " |      \n",
      " |          1. A Java RDD is created from the SequenceFile or other InputFormat, and the key\n",
      " |             and value Writable classes\n",
      " |          2. Serialization is attempted via Pickle pickling\n",
      " |          3. If this fails, the fallback is to call 'toString' on each key and value\n",
      " |          4. :class:`CPickleSerializer` is used to deserialize pickled objects on the Python side\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          path to sequencefile\n",
      " |      keyClass: str, optional\n",
      " |          fully qualified classname of key Writable class (e.g. \"org.apache.hadoop.io.Text\")\n",
      " |      valueClass : str, optional\n",
      " |          fully qualified classname of value Writable class\n",
      " |          (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
      " |      keyConverter : str, optional\n",
      " |          fully qualified name of a function returning key WritableConverter\n",
      " |      valueConverter : str, optional\n",
      " |          fully qualifiedname of a function returning value WritableConverter\n",
      " |      minSplits : int, optional\n",
      " |          minimum splits in dataset (default min(2, sc.defaultParallelism))\n",
      " |      batchSize : int, optional, default 0\n",
      " |          The number of Python objects represented as a single\n",
      " |          Java object. (default 0, choose batchSize automatically)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          RDD of tuples of key and corresponding value\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.saveAsSequenceFile`\n",
      " |      :meth:`RDD.saveAsNewAPIHadoopFile`\n",
      " |      :meth:`RDD.saveAsHadoopFile`\n",
      " |      :meth:`SparkContext.newAPIHadoopFile`\n",
      " |      :meth:`SparkContext.hadoopFile`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import os\n",
      " |      >>> import tempfile\n",
      " |      \n",
      " |      Set the class of output format\n",
      " |      \n",
      " |      >>> output_format_class = \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\"\n",
      " |      \n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     path = os.path.join(d, \"hadoop_file\")\n",
      " |      ...\n",
      " |      ...     # Write a temporary Hadoop file\n",
      " |      ...     rdd = sc.parallelize([(1, {3.0: \"bb\"}), (2, {1.0: \"aa\"}), (3, {2.0: \"dd\"})])\n",
      " |      ...     rdd.saveAsNewAPIHadoopFile(path, output_format_class)\n",
      " |      ...\n",
      " |      ...     collected = sorted(sc.sequenceFile(path).collect())\n",
      " |      \n",
      " |      >>> collected\n",
      " |      [(1, {3.0: 'bb'}), (2, {1.0: 'aa'}), (3, {2.0: 'dd'})]\n",
      " |  \n",
      " |  setCheckpointDir(self, dirName: str) -> None\n",
      " |      Set the directory under which RDDs are going to be checkpointed. The\n",
      " |      directory must be an HDFS path if running on a cluster.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dirName : str\n",
      " |          path to the directory where checkpoint files will be stored\n",
      " |          (must be HDFS path if running in cluster)\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.getCheckpointDir`\n",
      " |      :meth:`RDD.checkpoint`\n",
      " |      :meth:`RDD.getCheckpointFile`\n",
      " |  \n",
      " |  setJobDescription(self, value: str) -> None\n",
      " |      Set a human readable description of the current job.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      value : str\n",
      " |          The job description to set.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If you run jobs in parallel, use :class:`pyspark.InheritableThread` for thread\n",
      " |      local inheritance.\n",
      " |  \n",
      " |  setJobGroup(self, groupId: str, description: str, interruptOnCancel: bool = False) -> None\n",
      " |      Assigns a group ID to all the jobs started by this thread until the group ID is set to a\n",
      " |      different value or cleared.\n",
      " |      \n",
      " |      Often, a unit of execution in an application consists of multiple Spark actions or jobs.\n",
      " |      Application programmers can use this method to group all those jobs together and give a\n",
      " |      group description. Once set, the Spark web UI will associate such jobs with this group.\n",
      " |      \n",
      " |      The application can use :meth:`SparkContext.cancelJobGroup` to cancel all\n",
      " |      running jobs in this group.\n",
      " |      \n",
      " |      .. versionadded:: 1.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      groupId : str\n",
      " |          The group ID to assign.\n",
      " |      description : str\n",
      " |          The description to set for the job group.\n",
      " |      interruptOnCancel : bool, optional, default False\n",
      " |          whether to interrupt jobs on job cancellation.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If interruptOnCancel is set to true for the job group, then job cancellation will result\n",
      " |      in Thread.interrupt() being called on the job's executor threads. This is useful to help\n",
      " |      ensure that the tasks are actually stopped in a timely manner, but is off by default due\n",
      " |      to HDFS-1208, where HDFS may respond to Thread.interrupt() by marking nodes as dead.\n",
      " |      \n",
      " |      If you run jobs in parallel, use :class:`pyspark.InheritableThread` for thread\n",
      " |      local inheritance.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.cancelJobGroup`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import threading\n",
      " |      >>> from time import sleep\n",
      " |      >>> from pyspark import InheritableThread\n",
      " |      >>> result = \"Not Set\"\n",
      " |      >>> lock = threading.Lock()\n",
      " |      >>> def map_func(x):\n",
      " |      ...     sleep(100)\n",
      " |      ...     raise RuntimeError(\"Task should have been cancelled\")\n",
      " |      >>> def start_job(x):\n",
      " |      ...     global result\n",
      " |      ...     try:\n",
      " |      ...         sc.setJobGroup(\"job_to_cancel\", \"some description\")\n",
      " |      ...         result = sc.parallelize(range(x)).map(map_func).collect()\n",
      " |      ...     except Exception as e:\n",
      " |      ...         result = \"Cancelled\"\n",
      " |      ...     lock.release()\n",
      " |      >>> def stop_job():\n",
      " |      ...     sleep(5)\n",
      " |      ...     sc.cancelJobGroup(\"job_to_cancel\")\n",
      " |      >>> suppress = lock.acquire()\n",
      " |      >>> suppress = InheritableThread(target=start_job, args=(10,)).start()\n",
      " |      >>> suppress = InheritableThread(target=stop_job).start()\n",
      " |      >>> suppress = lock.acquire()\n",
      " |      >>> print(result)\n",
      " |      Cancelled\n",
      " |  \n",
      " |  setLocalProperty(self, key: str, value: str) -> None\n",
      " |      Set a local property that affects jobs submitted from this thread, such as the\n",
      " |      Spark fair scheduler pool.\n",
      " |      \n",
      " |      .. versionadded:: 1.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : str\n",
      " |          The key of the local property to set.\n",
      " |      value : str\n",
      " |          The value of the local property to set.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.getLocalProperty`\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If you run jobs in parallel, use :class:`pyspark.InheritableThread` for thread\n",
      " |      local inheritance.\n",
      " |  \n",
      " |  setLogLevel(self, logLevel: str) -> None\n",
      " |      Control our logLevel. This overrides any user-defined log settings.\n",
      " |      Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      logLevel : str\n",
      " |          The desired log level as a string.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.setLogLevel(\"WARN\")  # doctest :+SKIP\n",
      " |  \n",
      " |  show_profiles(self) -> None\n",
      " |      Print the profile stats to stdout\n",
      " |      \n",
      " |      .. versionadded:: 1.2.0\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.dump_profiles`\n",
      " |  \n",
      " |  sparkUser(self) -> str\n",
      " |      Get SPARK_USER for user who is running SparkContext.\n",
      " |      \n",
      " |      .. versionadded:: 1.0.0\n",
      " |  \n",
      " |  statusTracker(self) -> pyspark.status.StatusTracker\n",
      " |      Return :class:`StatusTracker` object\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  stop(self) -> None\n",
      " |      Shut down the :class:`SparkContext`.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |  \n",
      " |  textFile(self, name: str, minPartitions: Optional[int] = None, use_unicode: bool = True) -> pyspark.rdd.RDD[str]\n",
      " |      Read a text file from HDFS, a local file system (available on all\n",
      " |      nodes), or any Hadoop-supported file system URI, and return it as an\n",
      " |      RDD of Strings. The text files must be encoded as UTF-8.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          directory to the input data files, the path can be comma separated\n",
      " |          paths as a list of inputs\n",
      " |      minPartitions : int, optional\n",
      " |          suggested minimum number of partitions for the resulting RDD\n",
      " |      use_unicode : bool, default True\n",
      " |          If `use_unicode` is False, the strings will be kept as `str` (encoding\n",
      " |          as `utf-8`), which is faster and smaller than unicode.\n",
      " |      \n",
      " |          .. versionadded:: 1.2.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          RDD representing text data from the file(s).\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.saveAsTextFile`\n",
      " |      :meth:`SparkContext.wholeTextFiles`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import os\n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     path1 = os.path.join(d, \"text1\")\n",
      " |      ...     path2 = os.path.join(d, \"text2\")\n",
      " |      ...\n",
      " |      ...     # Write a temporary text file\n",
      " |      ...     sc.parallelize([\"x\", \"y\", \"z\"]).saveAsTextFile(path1)\n",
      " |      ...\n",
      " |      ...     # Write another temporary text file\n",
      " |      ...     sc.parallelize([\"aa\", \"bb\", \"cc\"]).saveAsTextFile(path2)\n",
      " |      ...\n",
      " |      ...     # Load text file\n",
      " |      ...     collected1 = sorted(sc.textFile(path1, 3).collect())\n",
      " |      ...     collected2 = sorted(sc.textFile(path2, 4).collect())\n",
      " |      ...\n",
      " |      ...     # Load two text files together\n",
      " |      ...     collected3 = sorted(sc.textFile('{},{}'.format(path1, path2), 5).collect())\n",
      " |      \n",
      " |      >>> collected1\n",
      " |      ['x', 'y', 'z']\n",
      " |      >>> collected2\n",
      " |      ['aa', 'bb', 'cc']\n",
      " |      >>> collected3\n",
      " |      ['aa', 'bb', 'cc', 'x', 'y', 'z']\n",
      " |  \n",
      " |  union(self, rdds: List[pyspark.rdd.RDD[~T]]) -> pyspark.rdd.RDD[~T]\n",
      " |      Build the union of a list of RDDs.\n",
      " |      \n",
      " |      This supports unions() of RDDs with different serialized formats,\n",
      " |      although this forces them to be reserialized using the default\n",
      " |      serializer:\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.union`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import os\n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # generate a text RDD\n",
      " |      ...     with open(os.path.join(d, \"union-text.txt\"), \"w\") as f:\n",
      " |      ...         _ = f.write(\"Hello\")\n",
      " |      ...     text_rdd = sc.textFile(d)\n",
      " |      ...\n",
      " |      ...     # generate another RDD\n",
      " |      ...     parallelized = sc.parallelize([\"World!\"])\n",
      " |      ...\n",
      " |      ...     unioned = sorted(sc.union([text_rdd, parallelized]).collect())\n",
      " |      \n",
      " |      >>> unioned\n",
      " |      ['Hello', 'World!']\n",
      " |  \n",
      " |  wholeTextFiles(self, path: str, minPartitions: Optional[int] = None, use_unicode: bool = True) -> pyspark.rdd.RDD[typing.Tuple[str, str]]\n",
      " |      Read a directory of text files from HDFS, a local file system\n",
      " |      (available on all nodes), or any  Hadoop-supported file system\n",
      " |      URI. Each file is read as a single record and returned in a\n",
      " |      key-value pair, where the key is the path of each file, the\n",
      " |      value is the content of each file.\n",
      " |      The text files must be encoded as UTF-8.\n",
      " |      \n",
      " |      .. versionadded:: 1.0.0\n",
      " |      \n",
      " |      For example, if you have the following files:\n",
      " |      \n",
      " |      .. code-block:: text\n",
      " |      \n",
      " |          hdfs://a-hdfs-path/part-00000\n",
      " |          hdfs://a-hdfs-path/part-00001\n",
      " |          ...\n",
      " |          hdfs://a-hdfs-path/part-nnnnn\n",
      " |      \n",
      " |      Do ``rdd = sparkContext.wholeTextFiles(\"hdfs://a-hdfs-path\")``,\n",
      " |      then ``rdd`` contains:\n",
      " |      \n",
      " |      .. code-block:: text\n",
      " |      \n",
      " |          (a-hdfs-path/part-00000, its content)\n",
      " |          (a-hdfs-path/part-00001, its content)\n",
      " |          ...\n",
      " |          (a-hdfs-path/part-nnnnn, its content)\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          directory to the input data files, the path can be comma separated\n",
      " |          paths as a list of inputs\n",
      " |      minPartitions : int, optional\n",
      " |          suggested minimum number of partitions for the resulting RDD\n",
      " |      use_unicode : bool, default True\n",
      " |          If `use_unicode` is False, the strings will be kept as `str` (encoding\n",
      " |          as `utf-8`), which is faster and smaller than unicode.\n",
      " |      \n",
      " |          .. versionadded:: 1.2.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          RDD representing path-content pairs from the file(s).\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Small files are preferred, as each file will be loaded fully in memory.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.saveAsTextFile`\n",
      " |      :meth:`SparkContext.textFile`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import os\n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a temporary text file\n",
      " |      ...     with open(os.path.join(d, \"1.txt\"), \"w\") as f:\n",
      " |      ...         _ = f.write(\"123\")\n",
      " |      ...\n",
      " |      ...     # Write another temporary text file\n",
      " |      ...     with open(os.path.join(d, \"2.txt\"), \"w\") as f:\n",
      " |      ...         _ = f.write(\"xyz\")\n",
      " |      ...\n",
      " |      ...     collected = sorted(sc.wholeTextFiles(d).collect())\n",
      " |      >>> collected\n",
      " |      [('.../1.txt', '123'), ('.../2.txt', 'xyz')]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  getOrCreate(conf: Optional[pyspark.conf.SparkConf] = None) -> 'SparkContext' from builtins.type\n",
      " |      Get or instantiate a :class:`SparkContext` and register it as a singleton object.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      conf : :class:`SparkConf`, optional\n",
      " |          :class:`SparkConf` that will be used for initialization of the :class:`SparkContext`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`SparkContext`\n",
      " |          current :class:`SparkContext`, or a new one if it wasn't created before the function\n",
      " |          call.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> SparkContext.getOrCreate()\n",
      " |      <SparkContext ...>\n",
      " |  \n",
      " |  setSystemProperty(key: str, value: str) -> None from builtins.type\n",
      " |      Set a Java system property, such as `spark.executor.memory`. This must\n",
      " |      be invoked before instantiating :class:`SparkContext`.\n",
      " |      \n",
      " |      .. versionadded:: 0.9.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : str\n",
      " |          The key of a new Java system property.\n",
      " |      value : str\n",
      " |          The value of a new Java system property.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  applicationId\n",
      " |      A unique identifier for the Spark application.\n",
      " |      Its format depends on the scheduler implementation.\n",
      " |      \n",
      " |      * in case of local spark app something like 'local-1433865536131'\n",
      " |      * in case of YARN something like 'application_1433865536131_34483'\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.applicationId  # doctest: +ELLIPSIS\n",
      " |      'local-...'\n",
      " |  \n",
      " |  defaultMinPartitions\n",
      " |      Default min number of partitions for Hadoop RDDs when not given by user\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.defaultMinPartitions > 0\n",
      " |      True\n",
      " |  \n",
      " |  defaultParallelism\n",
      " |      Default level of parallelism to use when not given by user (e.g. for reduce tasks)\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.defaultParallelism > 0\n",
      " |      True\n",
      " |  \n",
      " |  listArchives\n",
      " |      Returns a list of archive paths that are added to resources.\n",
      " |      \n",
      " |      .. versionadded:: 3.4.0\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.addArchive`\n",
      " |  \n",
      " |  listFiles\n",
      " |      Returns a list of file paths that are added to resources.\n",
      " |      \n",
      " |      .. versionadded:: 3.4.0\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.addFile`\n",
      " |  \n",
      " |  resources\n",
      " |      Return the resource information of this :class:`SparkContext`.\n",
      " |      A resource could be a GPU, FPGA, etc.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  startTime\n",
      " |      Return the epoch time when the :class:`SparkContext` was started.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> _ = sc.startTime\n",
      " |  \n",
      " |  uiWebUrl\n",
      " |      Return the URL of the SparkUI instance started by this :class:`SparkContext`\n",
      " |      \n",
      " |      .. versionadded:: 2.1.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      When the web ui is disabled, e.g., by ``spark.ui.enabled`` set to ``False``,\n",
      " |      it returns ``None``.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.uiWebUrl\n",
      " |      'http://...'\n",
      " |  \n",
      " |  version\n",
      " |      The version of Spark on which this application is running.\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> _ = sc.version\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  PACKAGE_EXTENSIONS = ('.zip', '.egg', '.jar')\n",
      " |  \n",
      " |  __annotations__ = {'PACKAGE_EXTENSIONS': typing.Iterable[str], '_activ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PACKAGE_EXTENSIONS',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getnewargs__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_accumulatorServer',\n",
       " '_active_spark_context',\n",
       " '_assert_on_driver',\n",
       " '_batchSize',\n",
       " '_callsite',\n",
       " '_checkpointFile',\n",
       " '_conf',\n",
       " '_dictToJavaMap',\n",
       " '_do_init',\n",
       " '_encryption_enabled',\n",
       " '_ensure_initialized',\n",
       " '_gateway',\n",
       " '_getJavaStorageLevel',\n",
       " '_initialize_context',\n",
       " '_javaAccumulator',\n",
       " '_jsc',\n",
       " '_jvm',\n",
       " '_lock',\n",
       " '_next_accum_id',\n",
       " '_pickled_broadcast_vars',\n",
       " '_python_includes',\n",
       " '_repr_html_',\n",
       " '_serialize_to_jvm',\n",
       " '_temp_dir',\n",
       " '_unbatched_serializer',\n",
       " 'accumulator',\n",
       " 'addArchive',\n",
       " 'addFile',\n",
       " 'addPyFile',\n",
       " 'appName',\n",
       " 'applicationId',\n",
       " 'binaryFiles',\n",
       " 'binaryRecords',\n",
       " 'broadcast',\n",
       " 'cancelAllJobs',\n",
       " 'cancelJobGroup',\n",
       " 'defaultMinPartitions',\n",
       " 'defaultParallelism',\n",
       " 'dump_profiles',\n",
       " 'emptyRDD',\n",
       " 'environment',\n",
       " 'getCheckpointDir',\n",
       " 'getConf',\n",
       " 'getLocalProperty',\n",
       " 'getOrCreate',\n",
       " 'hadoopFile',\n",
       " 'hadoopRDD',\n",
       " 'listArchives',\n",
       " 'listFiles',\n",
       " 'master',\n",
       " 'newAPIHadoopFile',\n",
       " 'newAPIHadoopRDD',\n",
       " 'parallelize',\n",
       " 'pickleFile',\n",
       " 'profiler_collector',\n",
       " 'pythonExec',\n",
       " 'pythonVer',\n",
       " 'range',\n",
       " 'resources',\n",
       " 'runJob',\n",
       " 'sequenceFile',\n",
       " 'serializer',\n",
       " 'setCheckpointDir',\n",
       " 'setJobDescription',\n",
       " 'setJobGroup',\n",
       " 'setLocalProperty',\n",
       " 'setLogLevel',\n",
       " 'setSystemProperty',\n",
       " 'show_profiles',\n",
       " 'sparkHome',\n",
       " 'sparkUser',\n",
       " 'startTime',\n",
       " 'statusTracker',\n",
       " 'stop',\n",
       " 'textFile',\n",
       " 'uiWebUrl',\n",
       " 'union',\n",
       " 'version',\n",
       " 'wholeTextFiles']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(sc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1,2,3,4]\n",
    "distributedData = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date,Open,High,Low,Close,AdjClose,Volume',\n",
       " '2019-07-15,248.000000,254.419998,244.860001,253.500000,253.500000,11000100',\n",
       " '2019-07-16,249.300003,253.529999,247.929993,252.380005,252.380005,8149000',\n",
       " '2019-07-17,255.669998,258.309998,253.350006,254.860001,254.860001,9764700',\n",
       " '2019-07-18,255.050003,255.750000,251.889999,253.539993,253.539993,4764500']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tesla_file = 'TSLA.csv'\n",
    "tesla_rdd = sc.textFile(tesla_file)\n",
    "tesla_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tesla_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_rdd = tesla_rdd.map(lambda row:row.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Date', 'Open', 'High', 'Low', 'Close', 'AdjClose', 'Volume'],\n",
       " ['2019-07-15',\n",
       "  '248.000000',\n",
       "  '254.419998',\n",
       "  '244.860001',\n",
       "  '253.500000',\n",
       "  '253.500000',\n",
       "  '11000100'],\n",
       " ['2019-07-16',\n",
       "  '249.300003',\n",
       "  '253.529999',\n",
       "  '247.929993',\n",
       "  '252.380005',\n",
       "  '252.380005',\n",
       "  '8149000'],\n",
       " ['2019-07-17',\n",
       "  '255.669998',\n",
       "  '258.309998',\n",
       "  '253.350006',\n",
       "  '254.860001',\n",
       "  '254.860001',\n",
       "  '9764700'],\n",
       " ['2019-07-18',\n",
       "  '255.050003',\n",
       "  '255.750000',\n",
       "  '251.889999',\n",
       "  '253.539993',\n",
       "  '253.539993',\n",
       "  '4764500'],\n",
       " ['2019-07-19',\n",
       "  '255.690002',\n",
       "  '259.959991',\n",
       "  '254.619995',\n",
       "  '258.179993',\n",
       "  '258.179993',\n",
       "  '7048400'],\n",
       " ['2019-07-22',\n",
       "  '258.750000',\n",
       "  '262.149994',\n",
       "  '254.190002',\n",
       "  '255.679993',\n",
       "  '255.679993',\n",
       "  '6842400'],\n",
       " ['2019-07-23',\n",
       "  '256.709991',\n",
       "  '260.480011',\n",
       "  '254.500000',\n",
       "  '260.170013',\n",
       "  '260.170013',\n",
       "  '5023100'],\n",
       " ['2019-07-24',\n",
       "  '259.170013',\n",
       "  '266.070007',\n",
       "  '258.160004',\n",
       "  '264.880005',\n",
       "  '264.880005',\n",
       "  '11072800'],\n",
       " ['2019-07-25',\n",
       "  '233.500000',\n",
       "  '234.500000',\n",
       "  '225.550003',\n",
       "  '228.820007',\n",
       "  '228.820007',\n",
       "  '22418300'],\n",
       " ['2019-07-26',\n",
       "  '226.919998',\n",
       "  '230.259995',\n",
       "  '222.250000',\n",
       "  '228.039993',\n",
       "  '228.039993',\n",
       "  '10027700'],\n",
       " ['2019-07-29',\n",
       "  '227.089996',\n",
       "  '235.940002',\n",
       "  '226.029999',\n",
       "  '235.770004',\n",
       "  '235.770004',\n",
       "  '9273300'],\n",
       " ['2019-07-30',\n",
       "  '232.899994',\n",
       "  '243.360001',\n",
       "  '232.179993',\n",
       "  '242.259995',\n",
       "  '242.259995',\n",
       "  '8109000'],\n",
       " ['2019-07-31',\n",
       "  '243.000000',\n",
       "  '246.679993',\n",
       "  '236.649994',\n",
       "  '241.610001',\n",
       "  '241.610001',\n",
       "  '9178200'],\n",
       " ['2019-08-01',\n",
       "  '242.649994',\n",
       "  '244.509995',\n",
       "  '231.770004',\n",
       "  '233.850006',\n",
       "  '233.850006',\n",
       "  '8259500'],\n",
       " ['2019-08-02',\n",
       "  '231.350006',\n",
       "  '236.270004',\n",
       "  '229.229996',\n",
       "  '234.339996',\n",
       "  '234.339996',\n",
       "  '6136500'],\n",
       " ['2019-08-05',\n",
       "  '229.600006',\n",
       "  '231.369995',\n",
       "  '225.779999',\n",
       "  '228.320007',\n",
       "  '228.320007',\n",
       "  '7028300'],\n",
       " ['2019-08-06',\n",
       "  '231.880005',\n",
       "  '232.500000',\n",
       "  '225.750000',\n",
       "  '230.750000',\n",
       "  '230.750000',\n",
       "  '5564200'],\n",
       " ['2019-08-07',\n",
       "  '226.500000',\n",
       "  '233.570007',\n",
       "  '225.800003',\n",
       "  '233.419998',\n",
       "  '233.419998',\n",
       "  '4776500'],\n",
       " ['2019-08-08',\n",
       "  '234.449997',\n",
       "  '239.800003',\n",
       "  '232.649994',\n",
       "  '238.300003',\n",
       "  '238.300003',\n",
       "  '5274300'],\n",
       " ['2019-08-09',\n",
       "  '236.050003',\n",
       "  '238.960007',\n",
       "  '233.809998',\n",
       "  '235.009995',\n",
       "  '235.009995',\n",
       "  '3898200'],\n",
       " ['2019-08-12',\n",
       "  '232.990005',\n",
       "  '235.770004',\n",
       "  '228.750000',\n",
       "  '229.009995',\n",
       "  '229.009995',\n",
       "  '4663900'],\n",
       " ['2019-08-13',\n",
       "  '228.809998',\n",
       "  '236.000000',\n",
       "  '227.550003',\n",
       "  '235.000000',\n",
       "  '235.000000',\n",
       "  '4848100'],\n",
       " ['2019-08-14',\n",
       "  '231.210007',\n",
       "  '231.500000',\n",
       "  '216.690002',\n",
       "  '219.619995',\n",
       "  '219.619995',\n",
       "  '9562600'],\n",
       " ['2019-08-15',\n",
       "  '220.860001',\n",
       "  '221.559998',\n",
       "  '211.550003',\n",
       "  '215.639999',\n",
       "  '215.639999',\n",
       "  '8159600'],\n",
       " ['2019-08-16',\n",
       "  '216.660004',\n",
       "  '222.240005',\n",
       "  '216.020004',\n",
       "  '219.940002',\n",
       "  '219.940002',\n",
       "  '5098500'],\n",
       " ['2019-08-19',\n",
       "  '224.210007',\n",
       "  '227.830002',\n",
       "  '221.699997',\n",
       "  '226.830002',\n",
       "  '226.830002',\n",
       "  '5309600'],\n",
       " ['2019-08-20',\n",
       "  '227.619995',\n",
       "  '229.089996',\n",
       "  '224.539993',\n",
       "  '225.860001',\n",
       "  '225.860001',\n",
       "  '4125200'],\n",
       " ['2019-08-21',\n",
       "  '222.009995',\n",
       "  '223.220001',\n",
       "  '217.600006',\n",
       "  '220.830002',\n",
       "  '220.830002',\n",
       "  '7794300'],\n",
       " ['2019-08-22',\n",
       "  '222.800003',\n",
       "  '225.399994',\n",
       "  '218.220001',\n",
       "  '222.149994',\n",
       "  '222.149994',\n",
       "  '6559000'],\n",
       " ['2019-08-23',\n",
       "  '219.970001',\n",
       "  '221.169998',\n",
       "  '211.000000',\n",
       "  '211.399994',\n",
       "  '211.399994',\n",
       "  '8538600'],\n",
       " ['2019-08-26',\n",
       "  '213.600006',\n",
       "  '215.020004',\n",
       "  '211.539993',\n",
       "  '215.000000',\n",
       "  '215.000000',\n",
       "  '5051900'],\n",
       " ['2019-08-27',\n",
       "  '215.740005',\n",
       "  '218.800003',\n",
       "  '212.029999',\n",
       "  '214.080002',\n",
       "  '214.080002',\n",
       "  '5416200'],\n",
       " ['2019-08-28',\n",
       "  '213.690002',\n",
       "  '217.250000',\n",
       "  '212.309998',\n",
       "  '215.589996',\n",
       "  '215.589996',\n",
       "  '3225500'],\n",
       " ['2019-08-29',\n",
       "  '219.000000',\n",
       "  '223.399994',\n",
       "  '218.000000',\n",
       "  '221.710007',\n",
       "  '221.710007',\n",
       "  '5179500'],\n",
       " ['2019-08-30',\n",
       "  '229.149994',\n",
       "  '232.440002',\n",
       "  '224.210007',\n",
       "  '225.610001',\n",
       "  '225.610001',\n",
       "  '9320600'],\n",
       " ['2019-09-03',\n",
       "  '224.080002',\n",
       "  '228.949997',\n",
       "  '223.160004',\n",
       "  '225.009995',\n",
       "  '225.009995',\n",
       "  '5354100'],\n",
       " ['2019-09-04',\n",
       "  '226.889999',\n",
       "  '228.460007',\n",
       "  '219.210007',\n",
       "  '220.679993',\n",
       "  '220.679993',\n",
       "  '5761000'],\n",
       " ['2019-09-05',\n",
       "  '222.500000',\n",
       "  '229.800003',\n",
       "  '220.850006',\n",
       "  '229.580002',\n",
       "  '229.580002',\n",
       "  '7395300'],\n",
       " ['2019-09-06',\n",
       "  '227.199997',\n",
       "  '229.639999',\n",
       "  '225.169998',\n",
       "  '227.449997',\n",
       "  '227.449997',\n",
       "  '4189400'],\n",
       " ['2019-09-09',\n",
       "  '230.000000',\n",
       "  '233.759995',\n",
       "  '229.229996',\n",
       "  '231.789993',\n",
       "  '231.789993',\n",
       "  '4802700'],\n",
       " ['2019-09-10',\n",
       "  '230.800003',\n",
       "  '235.539993',\n",
       "  '228.940002',\n",
       "  '235.539993',\n",
       "  '235.539993',\n",
       "  '4883700'],\n",
       " ['2019-09-11',\n",
       "  '237.380005',\n",
       "  '248.169998',\n",
       "  '236.000000',\n",
       "  '247.100006',\n",
       "  '247.100006',\n",
       "  '10042800'],\n",
       " ['2019-09-12',\n",
       "  '247.699997',\n",
       "  '253.500000',\n",
       "  '244.399994',\n",
       "  '245.869995',\n",
       "  '245.869995',\n",
       "  '8581200'],\n",
       " ['2019-09-13',\n",
       "  '246.960007',\n",
       "  '248.449997',\n",
       "  '244.869995',\n",
       "  '245.199997',\n",
       "  '245.199997',\n",
       "  '5313100'],\n",
       " ['2019-09-16',\n",
       "  '246.000000',\n",
       "  '247.429993',\n",
       "  '241.169998',\n",
       "  '242.809998',\n",
       "  '242.809998',\n",
       "  '4728100'],\n",
       " ['2019-09-17',\n",
       "  '242.470001',\n",
       "  '245.600006',\n",
       "  '240.369995',\n",
       "  '244.789993',\n",
       "  '244.789993',\n",
       "  '3865400'],\n",
       " ['2019-09-18',\n",
       "  '245.000000',\n",
       "  '248.169998',\n",
       "  '242.369995',\n",
       "  '243.490005',\n",
       "  '243.490005',\n",
       "  '4170200'],\n",
       " ['2019-09-19',\n",
       "  '246.000000',\n",
       "  '247.940002',\n",
       "  '244.839996',\n",
       "  '246.600006',\n",
       "  '246.600006',\n",
       "  '4795800'],\n",
       " ['2019-09-20',\n",
       "  '246.490005',\n",
       "  '246.949997',\n",
       "  '238.160004',\n",
       "  '240.619995',\n",
       "  '240.619995',\n",
       "  '6353000'],\n",
       " ['2019-09-23',\n",
       "  '240.000000',\n",
       "  '245.179993',\n",
       "  '239.220001',\n",
       "  '241.229996',\n",
       "  '241.229996',\n",
       "  '4340200'],\n",
       " ['2019-09-24',\n",
       "  '241.520004',\n",
       "  '241.990005',\n",
       "  '222.610001',\n",
       "  '223.210007',\n",
       "  '223.210007',\n",
       "  '12891500'],\n",
       " ['2019-09-25',\n",
       "  '224.559998',\n",
       "  '228.979996',\n",
       "  '218.360001',\n",
       "  '228.699997',\n",
       "  '228.699997',\n",
       "  '9427100'],\n",
       " ['2019-09-26',\n",
       "  '230.660004',\n",
       "  '243.309998',\n",
       "  '227.399994',\n",
       "  '242.559998',\n",
       "  '242.559998',\n",
       "  '11884500'],\n",
       " ['2019-09-27',\n",
       "  '242.199997',\n",
       "  '248.710007',\n",
       "  '238.729996',\n",
       "  '242.130005',\n",
       "  '242.130005',\n",
       "  '11116400'],\n",
       " ['2019-09-30',\n",
       "  '243.000000',\n",
       "  '243.979996',\n",
       "  '236.110001',\n",
       "  '240.869995',\n",
       "  '240.869995',\n",
       "  '5879800'],\n",
       " ['2019-10-01',\n",
       "  '241.500000',\n",
       "  '245.949997',\n",
       "  '239.130005',\n",
       "  '244.690002',\n",
       "  '244.690002',\n",
       "  '6162600'],\n",
       " ['2019-10-02',\n",
       "  '243.289993',\n",
       "  '244.649994',\n",
       "  '239.429993',\n",
       "  '243.130005',\n",
       "  '243.130005',\n",
       "  '5631400'],\n",
       " ['2019-10-03',\n",
       "  '231.860001',\n",
       "  '234.479996',\n",
       "  '224.279999',\n",
       "  '233.029999',\n",
       "  '233.029999',\n",
       "  '15084500'],\n",
       " ['2019-10-04',\n",
       "  '231.610001',\n",
       "  '234.779999',\n",
       "  '228.070007',\n",
       "  '231.429993',\n",
       "  '231.429993',\n",
       "  '7995000'],\n",
       " ['2019-10-07',\n",
       "  '229.800003',\n",
       "  '238.559998',\n",
       "  '228.550003',\n",
       "  '237.720001',\n",
       "  '237.720001',\n",
       "  '8064200'],\n",
       " ['2019-10-08',\n",
       "  '235.869995',\n",
       "  '243.940002',\n",
       "  '234.500000',\n",
       "  '240.050003',\n",
       "  '240.050003',\n",
       "  '8678200'],\n",
       " ['2019-10-09',\n",
       "  '241.320007',\n",
       "  '247.300003',\n",
       "  '240.649994',\n",
       "  '244.529999',\n",
       "  '244.529999',\n",
       "  '6894400'],\n",
       " ['2019-10-10',\n",
       "  '245.279999',\n",
       "  '249.279999',\n",
       "  '241.580002',\n",
       "  '244.740005',\n",
       "  '244.740005',\n",
       "  '6283300'],\n",
       " ['2019-10-11',\n",
       "  '247.149994',\n",
       "  '251.080002',\n",
       "  '246.809998',\n",
       "  '247.889999',\n",
       "  '247.889999',\n",
       "  '8475400'],\n",
       " ['2019-10-14',\n",
       "  '247.899994',\n",
       "  '258.549988',\n",
       "  '247.130005',\n",
       "  '256.959991',\n",
       "  '256.959991',\n",
       "  '10205000'],\n",
       " ['2019-10-15',\n",
       "  '257.700012',\n",
       "  '260.000000',\n",
       "  '254.119995',\n",
       "  '257.890015',\n",
       "  '257.890015',\n",
       "  '6432800'],\n",
       " ['2019-10-16',\n",
       "  '257.390015',\n",
       "  '262.100006',\n",
       "  '256.920013',\n",
       "  '259.750000',\n",
       "  '259.750000',\n",
       "  '6684100'],\n",
       " ['2019-10-17',\n",
       "  '262.500000',\n",
       "  '264.779999',\n",
       "  '260.170013',\n",
       "  '261.970001',\n",
       "  '261.970001',\n",
       "  '4769300'],\n",
       " ['2019-10-18',\n",
       "  '260.700012',\n",
       "  '262.799988',\n",
       "  '255.100006',\n",
       "  '256.950012',\n",
       "  '256.950012',\n",
       "  '5749800'],\n",
       " ['2019-10-21',\n",
       "  '258.329987',\n",
       "  '259.500000',\n",
       "  '250.179993',\n",
       "  '253.500000',\n",
       "  '253.500000',\n",
       "  '5020300'],\n",
       " ['2019-10-22',\n",
       "  '254.320007',\n",
       "  '258.329987',\n",
       "  '250.850006',\n",
       "  '255.580002',\n",
       "  '255.580002',\n",
       "  '4600800'],\n",
       " ['2019-10-23',\n",
       "  '254.500000',\n",
       "  '256.140015',\n",
       "  '251.350006',\n",
       "  '254.679993',\n",
       "  '254.679993',\n",
       "  '5261100'],\n",
       " ['2019-10-24',\n",
       "  '298.369995',\n",
       "  '304.929993',\n",
       "  '289.200012',\n",
       "  '299.679993',\n",
       "  '299.679993',\n",
       "  '29720900'],\n",
       " ['2019-10-25',\n",
       "  '297.720001',\n",
       "  '330.000000',\n",
       "  '296.109985',\n",
       "  '328.130005',\n",
       "  '328.130005',\n",
       "  '30006100'],\n",
       " ['2019-10-28',\n",
       "  '327.540009',\n",
       "  '340.839996',\n",
       "  '322.600006',\n",
       "  '327.709991',\n",
       "  '327.709991',\n",
       "  '18870300'],\n",
       " ['2019-10-29',\n",
       "  '319.989990',\n",
       "  '324.299988',\n",
       "  '314.750000',\n",
       "  '316.220001',\n",
       "  '316.220001',\n",
       "  '12684300'],\n",
       " ['2019-10-30',\n",
       "  '313.000000',\n",
       "  '318.790009',\n",
       "  '309.970001',\n",
       "  '315.010010',\n",
       "  '315.010010',\n",
       "  '9641800'],\n",
       " ['2019-10-31',\n",
       "  '313.100006',\n",
       "  '319.000000',\n",
       "  '313.000000',\n",
       "  '314.920013',\n",
       "  '314.920013',\n",
       "  '5067000'],\n",
       " ['2019-11-01',\n",
       "  '316.320007',\n",
       "  '316.480011',\n",
       "  '309.799988',\n",
       "  '313.309998',\n",
       "  '313.309998',\n",
       "  '6383900'],\n",
       " ['2019-11-04',\n",
       "  '314.799988',\n",
       "  '321.940002',\n",
       "  '309.260010',\n",
       "  '317.470001',\n",
       "  '317.470001',\n",
       "  '8787000'],\n",
       " ['2019-11-05',\n",
       "  '319.619995',\n",
       "  '323.510010',\n",
       "  '316.119995',\n",
       "  '317.220001',\n",
       "  '317.220001',\n",
       "  '6943400'],\n",
       " ['2019-11-06',\n",
       "  '318.000000',\n",
       "  '326.720001',\n",
       "  '314.500000',\n",
       "  '326.579987',\n",
       "  '326.579987',\n",
       "  '7940900'],\n",
       " ['2019-11-07',\n",
       "  '329.140015',\n",
       "  '341.500000',\n",
       "  '328.019989',\n",
       "  '335.540009',\n",
       "  '335.540009',\n",
       "  '14467300'],\n",
       " ['2019-11-08',\n",
       "  '334.500000',\n",
       "  '337.459991',\n",
       "  '332.500000',\n",
       "  '337.140015',\n",
       "  '337.140015',\n",
       "  '6069200'],\n",
       " ['2019-11-11',\n",
       "  '343.950012',\n",
       "  '349.190002',\n",
       "  '342.000000',\n",
       "  '345.089996',\n",
       "  '345.089996',\n",
       "  '9986700'],\n",
       " ['2019-11-12',\n",
       "  '346.899994',\n",
       "  '350.369995',\n",
       "  '344.040009',\n",
       "  '349.929993',\n",
       "  '349.929993',\n",
       "  '7359400'],\n",
       " ['2019-11-13',\n",
       "  '355.000000',\n",
       "  '356.329987',\n",
       "  '345.179993',\n",
       "  '346.109985',\n",
       "  '346.109985',\n",
       "  '8420100'],\n",
       " ['2019-11-14',\n",
       "  '346.109985',\n",
       "  '353.839996',\n",
       "  '342.910004',\n",
       "  '349.350006',\n",
       "  '349.350006',\n",
       "  '6464900'],\n",
       " ['2019-11-15',\n",
       "  '350.640015',\n",
       "  '352.799988',\n",
       "  '348.359985',\n",
       "  '352.170013',\n",
       "  '352.170013',\n",
       "  '4809000'],\n",
       " ['2019-11-18',\n",
       "  '352.920013',\n",
       "  '353.149994',\n",
       "  '346.100006',\n",
       "  '349.989990',\n",
       "  '349.989990',\n",
       "  '4400400'],\n",
       " ['2019-11-19',\n",
       "  '351.750000',\n",
       "  '359.989990',\n",
       "  '347.799988',\n",
       "  '359.519989',\n",
       "  '359.519989',\n",
       "  '7724800'],\n",
       " ['2019-11-20',\n",
       "  '360.000000',\n",
       "  '361.200012',\n",
       "  '349.570007',\n",
       "  '352.220001',\n",
       "  '352.220001',\n",
       "  '6725100'],\n",
       " ['2019-11-21',\n",
       "  '354.510010',\n",
       "  '360.839996',\n",
       "  '354.000000',\n",
       "  '354.829987',\n",
       "  '354.829987',\n",
       "  '6110000'],\n",
       " ['2019-11-22',\n",
       "  '340.160004',\n",
       "  '341.000000',\n",
       "  '330.000000',\n",
       "  '333.040009',\n",
       "  '333.040009',\n",
       "  '16870600'],\n",
       " ['2019-11-25',\n",
       "  '344.320007',\n",
       "  '344.570007',\n",
       "  '334.459991',\n",
       "  '336.339996',\n",
       "  '336.339996',\n",
       "  '12339500'],\n",
       " ['2019-11-26',\n",
       "  '335.269989',\n",
       "  '335.500000',\n",
       "  '327.100006',\n",
       "  '328.920013',\n",
       "  '328.920013',\n",
       "  '7947400'],\n",
       " ['2019-11-27',\n",
       "  '331.119995',\n",
       "  '333.929993',\n",
       "  '328.570007',\n",
       "  '331.290009',\n",
       "  '331.290009',\n",
       "  '5555600'],\n",
       " ['2019-11-29',\n",
       "  '331.109985',\n",
       "  '331.260010',\n",
       "  '327.500000',\n",
       "  '329.940002',\n",
       "  '329.940002',\n",
       "  '2465600'],\n",
       " ['2019-12-02',\n",
       "  '329.399994',\n",
       "  '336.380005',\n",
       "  '328.690002',\n",
       "  '334.869995',\n",
       "  '334.869995',\n",
       "  '6074500'],\n",
       " ['2019-12-03',\n",
       "  '332.619995',\n",
       "  '337.910004',\n",
       "  '332.190002',\n",
       "  '336.200012',\n",
       "  '336.200012',\n",
       "  '6573700'],\n",
       " ['2019-12-04',\n",
       "  '337.750000',\n",
       "  '337.859985',\n",
       "  '332.850006',\n",
       "  '333.029999',\n",
       "  '333.029999',\n",
       "  '5533000'],\n",
       " ['2019-12-05',\n",
       "  '332.829987',\n",
       "  '334.420013',\n",
       "  '327.250000',\n",
       "  '330.369995',\n",
       "  '330.369995',\n",
       "  '3724600'],\n",
       " ['2019-12-06',\n",
       "  '335.000000',\n",
       "  '338.859985',\n",
       "  '334.769989',\n",
       "  '335.890015',\n",
       "  '335.890015',\n",
       "  '7612400'],\n",
       " ['2019-12-09',\n",
       "  '336.589996',\n",
       "  '344.450012',\n",
       "  '335.079987',\n",
       "  '339.529999',\n",
       "  '339.529999',\n",
       "  '9023100'],\n",
       " ['2019-12-10',\n",
       "  '339.959991',\n",
       "  '350.730011',\n",
       "  '339.309998',\n",
       "  '348.839996',\n",
       "  '348.839996',\n",
       "  '8828300'],\n",
       " ['2019-12-11',\n",
       "  '351.880005',\n",
       "  '357.190002',\n",
       "  '351.089996',\n",
       "  '352.700012',\n",
       "  '352.700012',\n",
       "  '6897800'],\n",
       " ['2019-12-12',\n",
       "  '354.920013',\n",
       "  '362.739990',\n",
       "  '353.230011',\n",
       "  '359.679993',\n",
       "  '359.679993',\n",
       "  '7763900'],\n",
       " ['2019-12-13',\n",
       "  '361.049988',\n",
       "  '365.209991',\n",
       "  '354.640015',\n",
       "  '358.390015',\n",
       "  '358.390015',\n",
       "  '6570900'],\n",
       " ['2019-12-16',\n",
       "  '362.549988',\n",
       "  '383.609985',\n",
       "  '362.500000',\n",
       "  '381.500000',\n",
       "  '381.500000',\n",
       "  '18174200'],\n",
       " ['2019-12-17',\n",
       "  '378.989990',\n",
       "  '385.500000',\n",
       "  '375.899994',\n",
       "  '378.989990',\n",
       "  '378.989990',\n",
       "  '8496800'],\n",
       " ['2019-12-18',\n",
       "  '380.630005',\n",
       "  '395.220001',\n",
       "  '380.579987',\n",
       "  '393.149994',\n",
       "  '393.149994',\n",
       "  '14121000'],\n",
       " ['2019-12-19',\n",
       "  '397.320007',\n",
       "  '406.850006',\n",
       "  '396.500000',\n",
       "  '404.040009',\n",
       "  '404.040009',\n",
       "  '18107100'],\n",
       " ['2019-12-20',\n",
       "  '410.290009',\n",
       "  '413.000000',\n",
       "  '400.190002',\n",
       "  '405.589996',\n",
       "  '405.589996',\n",
       "  '14752700'],\n",
       " ['2019-12-23',\n",
       "  '411.779999',\n",
       "  '422.010010',\n",
       "  '410.000000',\n",
       "  '419.220001',\n",
       "  '419.220001',\n",
       "  '13319600'],\n",
       " ['2019-12-24',\n",
       "  '418.359985',\n",
       "  '425.470001',\n",
       "  '412.690002',\n",
       "  '425.250000',\n",
       "  '425.250000',\n",
       "  '8054700'],\n",
       " ['2019-12-26',\n",
       "  '427.910004',\n",
       "  '433.480011',\n",
       "  '426.350006',\n",
       "  '430.940002',\n",
       "  '430.940002',\n",
       "  '10633900'],\n",
       " ['2019-12-27',\n",
       "  '435.000000',\n",
       "  '435.309998',\n",
       "  '426.109985',\n",
       "  '430.380005',\n",
       "  '430.380005',\n",
       "  '9945700'],\n",
       " ['2019-12-30',\n",
       "  '428.790009',\n",
       "  '429.000000',\n",
       "  '409.260010',\n",
       "  '414.700012',\n",
       "  '414.700012',\n",
       "  '12586400'],\n",
       " ['2019-12-31',\n",
       "  '405.000000',\n",
       "  '421.290009',\n",
       "  '402.079987',\n",
       "  '418.329987',\n",
       "  '418.329987',\n",
       "  '10285700'],\n",
       " ['2020-01-02',\n",
       "  '424.500000',\n",
       "  '430.700012',\n",
       "  '421.709991',\n",
       "  '430.260010',\n",
       "  '430.260010',\n",
       "  '9532100'],\n",
       " ['2020-01-03',\n",
       "  '440.500000',\n",
       "  '454.000000',\n",
       "  '436.920013',\n",
       "  '443.010010',\n",
       "  '443.010010',\n",
       "  '17778500'],\n",
       " ['2020-01-06',\n",
       "  '440.470001',\n",
       "  '451.559998',\n",
       "  '440.000000',\n",
       "  '451.540009',\n",
       "  '451.540009',\n",
       "  '10133000'],\n",
       " ['2020-01-07',\n",
       "  '461.399994',\n",
       "  '471.630005',\n",
       "  '453.359985',\n",
       "  '469.059998',\n",
       "  '469.059998',\n",
       "  '17882100'],\n",
       " ['2020-01-08',\n",
       "  '473.700012',\n",
       "  '498.489990',\n",
       "  '468.230011',\n",
       "  '492.140015',\n",
       "  '492.140015',\n",
       "  '31144300'],\n",
       " ['2020-01-09',\n",
       "  '497.100006',\n",
       "  '498.799988',\n",
       "  '472.869995',\n",
       "  '481.339996',\n",
       "  '481.339996',\n",
       "  '28440400'],\n",
       " ['2020-01-10',\n",
       "  '481.790009',\n",
       "  '484.940002',\n",
       "  '473.700012',\n",
       "  '478.149994',\n",
       "  '478.149994',\n",
       "  '12959500'],\n",
       " ['2020-01-13',\n",
       "  '493.500000',\n",
       "  '525.630005',\n",
       "  '492.000000',\n",
       "  '524.859985',\n",
       "  '524.859985',\n",
       "  '26517600'],\n",
       " ['2020-01-14',\n",
       "  '544.260010',\n",
       "  '547.409973',\n",
       "  '524.900024',\n",
       "  '537.919983',\n",
       "  '537.919983',\n",
       "  '28996200'],\n",
       " ['2020-01-15',\n",
       "  '529.760010',\n",
       "  '537.840027',\n",
       "  '516.789978',\n",
       "  '518.500000',\n",
       "  '518.500000',\n",
       "  '17368800'],\n",
       " ['2020-01-16',\n",
       "  '493.750000',\n",
       "  '514.460022',\n",
       "  '492.170013',\n",
       "  '513.489990',\n",
       "  '513.489990',\n",
       "  '21736700'],\n",
       " ['2020-01-17',\n",
       "  '507.609985',\n",
       "  '515.669983',\n",
       "  '503.160004',\n",
       "  '510.500000',\n",
       "  '510.500000',\n",
       "  '13629100'],\n",
       " ['2020-01-21',\n",
       "  '530.250000',\n",
       "  '548.580017',\n",
       "  '528.409973',\n",
       "  '547.200012',\n",
       "  '547.200012',\n",
       "  '17803500'],\n",
       " ['2020-01-22',\n",
       "  '571.890015',\n",
       "  '594.500000',\n",
       "  '559.099976',\n",
       "  '569.559998',\n",
       "  '569.559998',\n",
       "  '31369000'],\n",
       " ['2020-01-23',\n",
       "  '564.250000',\n",
       "  '582.000000',\n",
       "  '555.599976',\n",
       "  '572.200012',\n",
       "  '572.200012',\n",
       "  '19651000'],\n",
       " ['2020-01-24',\n",
       "  '570.630005',\n",
       "  '573.859985',\n",
       "  '554.260010',\n",
       "  '564.820007',\n",
       "  '564.820007',\n",
       "  '14353600'],\n",
       " ['2020-01-27',\n",
       "  '541.989990',\n",
       "  '564.440002',\n",
       "  '539.280029',\n",
       "  '558.020020',\n",
       "  '558.020020',\n",
       "  '13608100'],\n",
       " ['2020-01-28',\n",
       "  '568.489990',\n",
       "  '576.809998',\n",
       "  '558.080017',\n",
       "  '566.900024',\n",
       "  '566.900024',\n",
       "  '11788500'],\n",
       " ['2020-01-29',\n",
       "  '575.690002',\n",
       "  '589.799988',\n",
       "  '567.429993',\n",
       "  '580.989990',\n",
       "  '580.989990',\n",
       "  '17801500'],\n",
       " ['2020-01-30',\n",
       "  '632.419983',\n",
       "  '650.880005',\n",
       "  '618.000000',\n",
       "  '640.809998',\n",
       "  '640.809998',\n",
       "  '29005700'],\n",
       " ['2020-01-31',\n",
       "  '640.000000',\n",
       "  '653.000000',\n",
       "  '632.520020',\n",
       "  '650.570007',\n",
       "  '650.570007',\n",
       "  '15719300'],\n",
       " ['2020-02-03',\n",
       "  '673.690002',\n",
       "  '786.140015',\n",
       "  '673.520020',\n",
       "  '780.000000',\n",
       "  '780.000000',\n",
       "  '47233500'],\n",
       " ['2020-02-04',\n",
       "  '882.960022',\n",
       "  '968.989990',\n",
       "  '833.880005',\n",
       "  '887.059998',\n",
       "  '887.059998',\n",
       "  '60938800'],\n",
       " ['2020-02-05',\n",
       "  '823.260010',\n",
       "  '845.979980',\n",
       "  '704.109985',\n",
       "  '734.700012',\n",
       "  '734.700012',\n",
       "  '48423800'],\n",
       " ['2020-02-06',\n",
       "  '699.919983',\n",
       "  '795.830017',\n",
       "  '687.000000',\n",
       "  '748.960022',\n",
       "  '748.960022',\n",
       "  '39880800'],\n",
       " ['2020-02-07',\n",
       "  '730.549988',\n",
       "  '769.750000',\n",
       "  '730.000000',\n",
       "  '748.070007',\n",
       "  '748.070007',\n",
       "  '17063500'],\n",
       " ['2020-02-10',\n",
       "  '800.000000',\n",
       "  '819.989990',\n",
       "  '752.400024',\n",
       "  '771.280029',\n",
       "  '771.280029',\n",
       "  '24689200'],\n",
       " ['2020-02-11',\n",
       "  '768.789978',\n",
       "  '783.510010',\n",
       "  '758.000000',\n",
       "  '774.380005',\n",
       "  '774.380005',\n",
       "  '11697500'],\n",
       " ['2020-02-12',\n",
       "  '777.869995',\n",
       "  '789.750000',\n",
       "  '763.369995',\n",
       "  '767.289978',\n",
       "  '767.289978',\n",
       "  '12022500'],\n",
       " ['2020-02-13',\n",
       "  '741.840027',\n",
       "  '818.000000',\n",
       "  '735.000000',\n",
       "  '804.000000',\n",
       "  '804.000000',\n",
       "  '26289300'],\n",
       " ['2020-02-14',\n",
       "  '787.219971',\n",
       "  '812.969971',\n",
       "  '785.500000',\n",
       "  '800.030029',\n",
       "  '800.030029',\n",
       "  '15693700'],\n",
       " ['2020-02-18',\n",
       "  '841.599976',\n",
       "  '860.000000',\n",
       "  '832.359985',\n",
       "  '858.400024',\n",
       "  '858.400024',\n",
       "  '16381700'],\n",
       " ['2020-02-19',\n",
       "  '923.500000',\n",
       "  '944.780029',\n",
       "  '901.020020',\n",
       "  '917.419983',\n",
       "  '917.419983',\n",
       "  '25423000'],\n",
       " ['2020-02-20',\n",
       "  '911.950012',\n",
       "  '912.000000',\n",
       "  '859.940002',\n",
       "  '899.409973',\n",
       "  '899.409973',\n",
       "  '17634900'],\n",
       " ['2020-02-21',\n",
       "  '906.979980',\n",
       "  '913.059998',\n",
       "  '880.450012',\n",
       "  '901.000000',\n",
       "  '901.000000',\n",
       "  '14314800'],\n",
       " ['2020-02-24',\n",
       "  '839.000000',\n",
       "  '863.500000',\n",
       "  '822.200012',\n",
       "  '833.789978',\n",
       "  '833.789978',\n",
       "  '15192200'],\n",
       " ['2020-02-25',\n",
       "  '849.000000',\n",
       "  '856.599976',\n",
       "  '787.000000',\n",
       "  '799.909973',\n",
       "  '799.909973',\n",
       "  '17290500'],\n",
       " ['2020-02-26',\n",
       "  '782.500000',\n",
       "  '813.309998',\n",
       "  '776.109985',\n",
       "  '778.799988',\n",
       "  '778.799988',\n",
       "  '14085500'],\n",
       " ['2020-02-27',\n",
       "  '730.000000',\n",
       "  '739.770020',\n",
       "  '669.000000',\n",
       "  '679.000000',\n",
       "  '679.000000',\n",
       "  '24149300'],\n",
       " ['2020-02-28',\n",
       "  '629.700012',\n",
       "  '690.520020',\n",
       "  '611.520020',\n",
       "  '667.989990',\n",
       "  '667.989990',\n",
       "  '24564200'],\n",
       " ['2020-03-02',\n",
       "  '711.260010',\n",
       "  '743.690002',\n",
       "  '686.669983',\n",
       "  '743.619995',\n",
       "  '743.619995',\n",
       "  '20195000'],\n",
       " ['2020-03-03',\n",
       "  '805.000000',\n",
       "  '806.979980',\n",
       "  '716.109985',\n",
       "  '745.510010',\n",
       "  '745.510010',\n",
       "  '25784000'],\n",
       " ['2020-03-04',\n",
       "  '763.960022',\n",
       "  '766.520020',\n",
       "  '724.729980',\n",
       "  '749.500000',\n",
       "  '749.500000',\n",
       "  '15049000'],\n",
       " ['2020-03-05',\n",
       "  '723.770020',\n",
       "  '745.750000',\n",
       "  '718.070007',\n",
       "  '724.539978',\n",
       "  '724.539978',\n",
       "  '10852700'],\n",
       " ['2020-03-06',\n",
       "  '690.000000',\n",
       "  '707.000000',\n",
       "  '684.270020',\n",
       "  '703.479980',\n",
       "  '703.479980',\n",
       "  '12662900'],\n",
       " ['2020-03-09',\n",
       "  '605.390015',\n",
       "  '663.000000',\n",
       "  '605.000000',\n",
       "  '608.000000',\n",
       "  '608.000000',\n",
       "  '17073700'],\n",
       " ['2020-03-10',\n",
       "  '659.429993',\n",
       "  '668.000000',\n",
       "  '608.000000',\n",
       "  '645.330017',\n",
       "  '645.330017',\n",
       "  '15594400'],\n",
       " ['2020-03-11',\n",
       "  '640.200012',\n",
       "  '653.580017',\n",
       "  '613.000000',\n",
       "  '634.229980',\n",
       "  '634.229980',\n",
       "  '13322500'],\n",
       " ['2020-03-12',\n",
       "  '580.890015',\n",
       "  '594.500000',\n",
       "  '546.250000',\n",
       "  '560.549988',\n",
       "  '560.549988',\n",
       "  '18909100'],\n",
       " ['2020-03-13',\n",
       "  '595.000000',\n",
       "  '607.570007',\n",
       "  '502.000000',\n",
       "  '546.619995',\n",
       "  '546.619995',\n",
       "  '22640300'],\n",
       " ['2020-03-16',\n",
       "  '469.500000',\n",
       "  '494.869995',\n",
       "  '442.170013',\n",
       "  '445.070007',\n",
       "  '445.070007',\n",
       "  '20489500'],\n",
       " ['2020-03-17',\n",
       "  '440.010010',\n",
       "  '471.850006',\n",
       "  '396.000000',\n",
       "  '430.200012',\n",
       "  '430.200012',\n",
       "  '23994600'],\n",
       " ['2020-03-18',\n",
       "  '389.000000',\n",
       "  '404.859985',\n",
       "  '350.510010',\n",
       "  '361.220001',\n",
       "  '361.220001',\n",
       "  '23786200'],\n",
       " ['2020-03-19',\n",
       "  '374.700012',\n",
       "  '452.000000',\n",
       "  '358.459991',\n",
       "  '427.640015',\n",
       "  '427.640015',\n",
       "  '30195500'],\n",
       " ['2020-03-20',\n",
       "  '438.200012',\n",
       "  '477.000000',\n",
       "  '425.790009',\n",
       "  '427.529999',\n",
       "  '427.529999',\n",
       "  '28285500'],\n",
       " ['2020-03-23',\n",
       "  '433.600006',\n",
       "  '442.000000',\n",
       "  '410.500000',\n",
       "  '434.290009',\n",
       "  '434.290009',\n",
       "  '16454500'],\n",
       " ['2020-03-24',\n",
       "  '477.299988',\n",
       "  '513.690002',\n",
       "  '474.000000',\n",
       "  '505.000000',\n",
       "  '505.000000',\n",
       "  '22895200'],\n",
       " ['2020-03-25',\n",
       "  '545.250000',\n",
       "  '557.000000',\n",
       "  '511.109985',\n",
       "  '539.250000',\n",
       "  '539.250000',\n",
       "  '21222700'],\n",
       " ['2020-03-26',\n",
       "  '547.390015',\n",
       "  '560.000000',\n",
       "  '512.250000',\n",
       "  '528.159973',\n",
       "  '528.159973',\n",
       "  '17380700'],\n",
       " ['2020-03-27',\n",
       "  '505.000000',\n",
       "  '525.799988',\n",
       "  '494.029999',\n",
       "  '514.359985',\n",
       "  '514.359985',\n",
       "  '14377400'],\n",
       " ['2020-03-30',\n",
       "  '510.260010',\n",
       "  '516.650024',\n",
       "  '491.230011',\n",
       "  '502.130005',\n",
       "  '502.130005',\n",
       "  '11998100'],\n",
       " ['2020-03-31',\n",
       "  '501.250000',\n",
       "  '542.960022',\n",
       "  '497.000000',\n",
       "  '524.000000',\n",
       "  '524.000000',\n",
       "  '17771500'],\n",
       " ['2020-04-01',\n",
       "  '504.000000',\n",
       "  '513.950012',\n",
       "  '475.100006',\n",
       "  '481.559998',\n",
       "  '481.559998',\n",
       "  '13353200'],\n",
       " ['2020-04-02',\n",
       "  '481.029999',\n",
       "  '494.260010',\n",
       "  '446.399994',\n",
       "  '454.470001',\n",
       "  '454.470001',\n",
       "  '19858400'],\n",
       " ['2020-04-03',\n",
       "  '509.500000',\n",
       "  '515.489990',\n",
       "  '468.390015',\n",
       "  '480.010010',\n",
       "  '480.010010',\n",
       "  '22562100'],\n",
       " ['2020-04-06',\n",
       "  '511.200012',\n",
       "  '521.000000',\n",
       "  '497.959991',\n",
       "  '516.239990',\n",
       "  '516.239990',\n",
       "  '14901800'],\n",
       " ['2020-04-07',\n",
       "  '545.000000',\n",
       "  '565.000000',\n",
       "  '532.340027',\n",
       "  '545.450012',\n",
       "  '545.450012',\n",
       "  '17919800'],\n",
       " ['2020-04-08',\n",
       "  '554.200012',\n",
       "  '557.210022',\n",
       "  '533.330017',\n",
       "  '548.840027',\n",
       "  '548.840027',\n",
       "  '12656000'],\n",
       " ['2020-04-09',\n",
       "  '562.090027',\n",
       "  '575.179993',\n",
       "  '557.109985',\n",
       "  '573.000000',\n",
       "  '573.000000',\n",
       "  '13650000'],\n",
       " ['2020-04-13',\n",
       "  '590.159973',\n",
       "  '652.000000',\n",
       "  '580.530029',\n",
       "  '650.950012',\n",
       "  '650.950012',\n",
       "  '22475400'],\n",
       " ['2020-04-14',\n",
       "  '698.969971',\n",
       "  '741.880005',\n",
       "  '692.429993',\n",
       "  '709.890015',\n",
       "  '709.890015',\n",
       "  '30576500'],\n",
       " ['2020-04-15',\n",
       "  '742.000000',\n",
       "  '753.130005',\n",
       "  '710.000000',\n",
       "  '729.830017',\n",
       "  '729.830017',\n",
       "  '23577000'],\n",
       " ['2020-04-16',\n",
       "  '716.940002',\n",
       "  '759.450012',\n",
       "  '706.719971',\n",
       "  '745.210022',\n",
       "  '745.210022',\n",
       "  '20657900'],\n",
       " ['2020-04-17',\n",
       "  '772.280029',\n",
       "  '774.950012',\n",
       "  '747.659973',\n",
       "  '753.890015',\n",
       "  '753.890015',\n",
       "  '13128200'],\n",
       " ['2020-04-20',\n",
       "  '732.700012',\n",
       "  '765.570007',\n",
       "  '712.210022',\n",
       "  '746.359985',\n",
       "  '746.359985',\n",
       "  '14746600'],\n",
       " ['2020-04-21',\n",
       "  '730.119995',\n",
       "  '753.330017',\n",
       "  '673.789978',\n",
       "  '686.719971',\n",
       "  '686.719971',\n",
       "  '20209100'],\n",
       " ['2020-04-22',\n",
       "  '703.979980',\n",
       "  '734.000000',\n",
       "  '688.710022',\n",
       "  '732.109985',\n",
       "  '732.109985',\n",
       "  '14224800'],\n",
       " ['2020-04-23',\n",
       "  '727.599976',\n",
       "  '734.000000',\n",
       "  '703.130005',\n",
       "  '705.630005',\n",
       "  '705.630005',\n",
       "  '13236700'],\n",
       " ['2020-04-24',\n",
       "  '710.809998',\n",
       "  '730.729980',\n",
       "  '698.179993',\n",
       "  '725.150024',\n",
       "  '725.150024',\n",
       "  '13237600'],\n",
       " ['2020-04-27',\n",
       "  '737.609985',\n",
       "  '799.489990',\n",
       "  '735.000000',\n",
       "  '798.750000',\n",
       "  '798.750000',\n",
       "  '20681400'],\n",
       " ['2020-04-28',\n",
       "  '795.640015',\n",
       "  '805.000000',\n",
       "  '756.690002',\n",
       "  '769.119995',\n",
       "  '769.119995',\n",
       "  '15222000'],\n",
       " ['2020-04-29',\n",
       "  '790.169983',\n",
       "  '803.200012',\n",
       "  '783.159973',\n",
       "  '800.510010',\n",
       "  '800.510010',\n",
       "  '16216000'],\n",
       " ['2020-04-30',\n",
       "  '855.190002',\n",
       "  '869.820007',\n",
       "  '763.500000',\n",
       "  '781.880005',\n",
       "  '781.880005',\n",
       "  '28400100'],\n",
       " ['2020-05-01',\n",
       "  '755.000000',\n",
       "  '772.770020',\n",
       "  '683.039978',\n",
       "  '701.320007',\n",
       "  '701.320007',\n",
       "  '32531800'],\n",
       " ['2020-05-04',\n",
       "  '701.000000',\n",
       "  '762.000000',\n",
       "  '698.000000',\n",
       "  '761.190002',\n",
       "  '761.190002',\n",
       "  '19237100'],\n",
       " ['2020-05-05',\n",
       "  '789.789978',\n",
       "  '798.919983',\n",
       "  '762.179993',\n",
       "  '768.210022',\n",
       "  '768.210022',\n",
       "  '16991700'],\n",
       " ['2020-05-06',\n",
       "  '776.500000',\n",
       "  '789.799988',\n",
       "  '761.109985',\n",
       "  '782.580017',\n",
       "  '782.580017',\n",
       "  '11123200'],\n",
       " ['2020-05-07',\n",
       "  '777.210022',\n",
       "  '796.400024',\n",
       "  '772.349976',\n",
       "  '780.039978',\n",
       "  '780.039978',\n",
       "  '11527700'],\n",
       " ['2020-05-08',\n",
       "  '793.770020',\n",
       "  '824.000000',\n",
       "  '787.010010',\n",
       "  '819.419983',\n",
       "  '819.419983',\n",
       "  '16130100'],\n",
       " ['2020-05-11',\n",
       "  '790.510010',\n",
       "  '824.000000',\n",
       "  '785.000000',\n",
       "  '811.289978',\n",
       "  '811.289978',\n",
       "  '16471100'],\n",
       " ['2020-05-12',\n",
       "  '827.000000',\n",
       "  '843.289978',\n",
       "  '808.000000',\n",
       "  '809.409973',\n",
       "  '809.409973',\n",
       "  '15906900'],\n",
       " ['2020-05-13',\n",
       "  '820.830017',\n",
       "  '826.000000',\n",
       "  '763.299988',\n",
       "  '790.960022',\n",
       "  '790.960022',\n",
       "  '19065500'],\n",
       " ['2020-05-14',\n",
       "  '780.000000',\n",
       "  '803.359985',\n",
       "  '764.000000',\n",
       "  '803.330017',\n",
       "  '803.330017',\n",
       "  '13682200'],\n",
       " ['2020-05-15',\n",
       "  '790.349976',\n",
       "  '805.049988',\n",
       "  '786.549988',\n",
       "  '799.169983',\n",
       "  '799.169983',\n",
       "  '10518400'],\n",
       " ['2020-05-18',\n",
       "  '827.780029',\n",
       "  '834.719971',\n",
       "  '803.880005',\n",
       "  '813.630005',\n",
       "  '813.630005',\n",
       "  '11698100'],\n",
       " ['2020-05-19',\n",
       "  '815.169983',\n",
       "  '822.070007',\n",
       "  '806.080017',\n",
       "  '808.010010',\n",
       "  '808.010010',\n",
       "  '9636500'],\n",
       " ['2020-05-20',\n",
       "  '820.500000',\n",
       "  '826.000000',\n",
       "  '811.799988',\n",
       "  '815.559998',\n",
       "  '815.559998',\n",
       "  '7309300'],\n",
       " ['2020-05-21',\n",
       "  '816.000000',\n",
       "  '832.500000',\n",
       "  '796.000000',\n",
       "  '827.599976',\n",
       "  '827.599976',\n",
       "  '12254600'],\n",
       " ['2020-05-22',\n",
       "  '822.169983',\n",
       "  '831.780029',\n",
       "  '812.000000',\n",
       "  '816.880005',\n",
       "  '816.880005',\n",
       "  '9987500'],\n",
       " ['2020-05-26',\n",
       "  '834.500000',\n",
       "  '834.599976',\n",
       "  '815.710022',\n",
       "  '818.869995',\n",
       "  '818.869995',\n",
       "  '8089700'],\n",
       " ['2020-05-27',\n",
       "  '820.859985',\n",
       "  '827.710022',\n",
       "  '785.000000',\n",
       "  '820.229980',\n",
       "  '820.229980',\n",
       "  '11549500'],\n",
       " ['2020-05-28',\n",
       "  '813.510010',\n",
       "  '824.750000',\n",
       "  '801.690002',\n",
       "  '805.809998',\n",
       "  '805.809998',\n",
       "  '7255600'],\n",
       " ['2020-05-29',\n",
       "  '808.750000',\n",
       "  '835.000000',\n",
       "  '804.210022',\n",
       "  '835.000000',\n",
       "  '835.000000',\n",
       "  '11812500'],\n",
       " ['2020-06-01',\n",
       "  '858.000000',\n",
       "  '899.000000',\n",
       "  '854.099976',\n",
       "  '898.099976',\n",
       "  '898.099976',\n",
       "  '14939500'],\n",
       " ['2020-06-02',\n",
       "  '894.700012',\n",
       "  '908.659973',\n",
       "  '871.000000',\n",
       "  '881.559998',\n",
       "  '881.559998',\n",
       "  '13565600'],\n",
       " ['2020-06-03',\n",
       "  '888.119995',\n",
       "  '897.940002',\n",
       "  '880.099976',\n",
       "  '882.960022',\n",
       "  '882.960022',\n",
       "  '7949500'],\n",
       " ['2020-06-04',\n",
       "  '889.880005',\n",
       "  '895.750000',\n",
       "  '858.440002',\n",
       "  '864.380005',\n",
       "  '864.380005',\n",
       "  '8887700'],\n",
       " ['2020-06-05',\n",
       "  '877.840027',\n",
       "  '886.520020',\n",
       "  '866.200012',\n",
       "  '885.659973',\n",
       "  '885.659973',\n",
       "  '7811900'],\n",
       " ['2020-06-08',\n",
       "  '919.000000',\n",
       "  '950.000000',\n",
       "  '909.159973',\n",
       "  '949.919983',\n",
       "  '949.919983',\n",
       "  '14174700'],\n",
       " ['2020-06-09',\n",
       "  '940.010010',\n",
       "  '954.440002',\n",
       "  '923.929993',\n",
       "  '940.669983',\n",
       "  '940.669983',\n",
       "  '11388200'],\n",
       " ['2020-06-10',\n",
       "  '991.880005',\n",
       "  '1027.479980',\n",
       "  '982.500000',\n",
       "  '1025.050049',\n",
       "  '1025.050049',\n",
       "  '18563400'],\n",
       " ['2020-06-11',\n",
       "  '990.200012',\n",
       "  '1018.960022',\n",
       "  '972.000000',\n",
       "  '972.840027',\n",
       "  '972.840027',\n",
       "  '15916500'],\n",
       " ['2020-06-12',\n",
       "  '980.000000',\n",
       "  '987.979980',\n",
       "  '912.599976',\n",
       "  '935.280029',\n",
       "  '935.280029',\n",
       "  '16730200'],\n",
       " ['2020-06-15',\n",
       "  '917.789978',\n",
       "  '998.840027',\n",
       "  '908.500000',\n",
       "  '990.900024',\n",
       "  '990.900024',\n",
       "  '15697200'],\n",
       " ['2020-06-16',\n",
       "  '1011.849976',\n",
       "  '1012.880005',\n",
       "  '962.390015',\n",
       "  '982.130005',\n",
       "  '982.130005',\n",
       "  '14051100'],\n",
       " ['2020-06-17',\n",
       "  '987.710022',\n",
       "  '1005.000000',\n",
       "  '982.570007',\n",
       "  '991.789978',\n",
       "  '991.789978',\n",
       "  '9869400'],\n",
       " ['2020-06-18',\n",
       "  '1003.000000',\n",
       "  '1019.200012',\n",
       "  '994.469971',\n",
       "  '1003.960022',\n",
       "  '1003.960022',\n",
       "  '9751900'],\n",
       " ['2020-06-19',\n",
       "  '1012.780029',\n",
       "  '1015.969971',\n",
       "  '991.340027',\n",
       "  '1000.900024',\n",
       "  '1000.900024',\n",
       "  '8679700'],\n",
       " ['2020-06-22',\n",
       "  '999.950012',\n",
       "  '1008.880005',\n",
       "  '990.020020',\n",
       "  '994.320007',\n",
       "  '994.320007',\n",
       "  '6362400'],\n",
       " ['2020-06-23',\n",
       "  '998.880005',\n",
       "  '1012.000000',\n",
       "  '994.010010',\n",
       "  '1001.780029',\n",
       "  '1001.780029',\n",
       "  '6365300'],\n",
       " ['2020-06-24',\n",
       "  '994.109985',\n",
       "  '1000.880005',\n",
       "  '953.140015',\n",
       "  '960.849976',\n",
       "  '960.849976',\n",
       "  '10959600'],\n",
       " ['2020-06-25',\n",
       "  '954.270020',\n",
       "  '985.979980',\n",
       "  '937.150024',\n",
       "  '985.979980',\n",
       "  '985.979980',\n",
       "  '9254500'],\n",
       " ['2020-06-26',\n",
       "  '994.780029',\n",
       "  '995.000000',\n",
       "  '954.869995',\n",
       "  '959.739990',\n",
       "  '959.739990',\n",
       "  '8854900'],\n",
       " ['2020-06-29',\n",
       "  '969.010010',\n",
       "  '1010.000000',\n",
       "  '948.520020',\n",
       "  '1009.349976',\n",
       "  '1009.349976',\n",
       "  '9026400'],\n",
       " ['2020-06-30',\n",
       "  '1006.500000',\n",
       "  '1087.689941',\n",
       "  '1003.729980',\n",
       "  '1079.810059',\n",
       "  '1079.810059',\n",
       "  '16918500'],\n",
       " ['2020-07-01',\n",
       "  '1083.000000',\n",
       "  '1135.329956',\n",
       "  '1080.500000',\n",
       "  '1119.630005',\n",
       "  '1119.630005',\n",
       "  '13326900'],\n",
       " ['2020-07-02',\n",
       "  '1221.479980',\n",
       "  '1228.000000',\n",
       "  '1185.599976',\n",
       "  '1208.660034',\n",
       "  '1208.660034',\n",
       "  '17250100'],\n",
       " ['2020-07-06',\n",
       "  '1276.689941',\n",
       "  '1377.790039',\n",
       "  '1266.040039',\n",
       "  '1371.579956',\n",
       "  '1371.579956',\n",
       "  '20569900'],\n",
       " ['2020-07-07',\n",
       "  '1405.010010',\n",
       "  '1429.500000',\n",
       "  '1336.709961',\n",
       "  '1389.859985',\n",
       "  '1389.859985',\n",
       "  '21489700'],\n",
       " ['2020-07-08',\n",
       "  '1405.000000',\n",
       "  '1417.260010',\n",
       "  '1311.339966',\n",
       "  '1365.880005',\n",
       "  '1365.880005',\n",
       "  '16311300'],\n",
       " ['2020-07-09',\n",
       "  '1396.989990',\n",
       "  '1408.560059',\n",
       "  '1351.280029',\n",
       "  '1394.280029',\n",
       "  '1394.280029',\n",
       "  '11717600'],\n",
       " ['2020-07-10',\n",
       "  '1396.000000',\n",
       "  '1548.920044',\n",
       "  '1376.010010',\n",
       "  '1544.650024',\n",
       "  '1544.650024',\n",
       "  '23337600'],\n",
       " ['2020-07-13',\n",
       "  '1659.000000',\n",
       "  '1794.989990',\n",
       "  '1471.109985',\n",
       "  '1497.060059',\n",
       "  '1497.060059',\n",
       "  '38725700'],\n",
       " ['2020-07-14',\n",
       "  '1556.000000',\n",
       "  '1590.000000',\n",
       "  '1431.000000',\n",
       "  '1516.800049',\n",
       "  '1516.800049',\n",
       "  '22833862']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_rdd.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPARK DATAFRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old way\n",
    "# from pyspark.sql import SQLContext\n",
    "# sqlContext = SQLContext(sc)\n",
    "\n",
    "# New way\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "li = [('Alice',1)]\n",
    "df = spark.createDataFrame(li, ['name', 'age'])\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(date='Date', open='Open', high='High', low='Low', close='Close', adjclose='AdjClose', volume='Volume'),\n",
       " Row(date='2019-07-15', open='248.000000', high='254.419998', low='244.860001', close='253.500000', adjclose='253.500000', volume='11000100'),\n",
       " Row(date='2019-07-16', open='249.300003', high='253.529999', low='247.929993', close='252.380005', adjclose='252.380005', volume='8149000'),\n",
       " Row(date='2019-07-17', open='255.669998', high='258.309998', low='253.350006', close='254.860001', adjclose='254.860001', volume='9764700'),\n",
       " Row(date='2019-07-18', open='255.050003', high='255.750000', low='251.889999', close='253.539993', adjclose='253.539993', volume='4764500')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tesla_df = csv_rdd.toDF(['date','open','high','low','close','adjclose','volume'])\n",
    "tesla_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_file = 'AMZN.csv'\n",
    "amazon_df = spark.read.load(amazon_file, format='com.databricks.spark.csv',\n",
    "                            header='true', inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.date(2019, 7, 15), Open=2021.400024, High=2022.900024, Low=2001.550049, Close=2020.98999, AdjClose=2020.98999, Volume=2981300),\n",
       " Row(Date=datetime.date(2019, 7, 16), Open=2010.579956, High=2026.319946, Low=2001.219971, Close=2009.900024, AdjClose=2009.900024, Volume=2618200),\n",
       " Row(Date=datetime.date(2019, 7, 17), Open=2007.050049, High=2012.0, Low=1992.030029, Close=1992.030029, AdjClose=1992.030029, Volume=2558800),\n",
       " Row(Date=datetime.date(2019, 7, 18), Open=1980.01001, High=1987.5, Low=1951.550049, Close=1977.900024, AdjClose=1977.900024, Volume=3504300),\n",
       " Row(Date=datetime.date(2019, 7, 19), Open=1991.209961, High=1996.0, Low=1962.22998, Close=1964.52002, AdjClose=1964.52002, Volume=3185600)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- AdjClose: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "amazon_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
      "|      Date|       Open|       High|        Low|      Close|   AdjClose| Volume|\n",
      "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
      "|2019-07-15|2021.400024|2022.900024|2001.550049| 2020.98999| 2020.98999|2981300|\n",
      "|2019-07-16|2010.579956|2026.319946|2001.219971|2009.900024|2009.900024|2618200|\n",
      "|2019-07-17|2007.050049|     2012.0|1992.030029|1992.030029|1992.030029|2558800|\n",
      "|2019-07-18| 1980.01001|     1987.5|1951.550049|1977.900024|1977.900024|3504300|\n",
      "|2019-07-19|1991.209961|     1996.0| 1962.22998| 1964.52002| 1964.52002|3185600|\n",
      "|2019-07-22|1971.140015|     1989.0| 1958.26001|1985.630005|1985.630005|2900000|\n",
      "|2019-07-23| 1995.98999|1997.790039|1973.130005| 1994.48999| 1994.48999|2703500|\n",
      "|2019-07-24|1969.300049|2001.300049|1965.869995|2000.810059|2000.810059|2631300|\n",
      "|2019-07-25|     2001.0|2001.199951|1972.719971|1973.819946|1973.819946|4136500|\n",
      "|2019-07-26|     1942.0|1950.900024| 1924.51001|1943.050049|1943.050049|4927100|\n",
      "|2019-07-29|     1930.0| 1932.22998|1890.540039|1912.449951|1912.449951|4493200|\n",
      "|2019-07-30|1891.119995|1909.890015| 1883.47998|1898.530029|1898.530029|2910900|\n",
      "|2019-07-31|1898.109985|1899.550049|1849.439941|1866.780029|1866.780029|4470700|\n",
      "|2019-08-01|1871.719971|1897.920044| 1844.01001|1855.319946|1855.319946|4713300|\n",
      "|2019-08-02|1845.069946|1846.359985| 1808.02002| 1823.23999| 1823.23999|4956200|\n",
      "|2019-08-05|1770.219971|1788.670044|1748.780029|1765.130005|1765.130005|6058200|\n",
      "|2019-08-06| 1792.22998| 1793.77002|1753.400024|1787.829956|1787.829956|5070300|\n",
      "|2019-08-07| 1773.98999|1798.930054|     1757.0|1793.400024|1793.400024|4526900|\n",
      "|2019-08-08|     1806.0| 1834.26001|1798.109985|1832.890015|1832.890015|3701200|\n",
      "|2019-08-09|1828.949951|1831.089966|1802.219971|1807.579956|1807.579956|2879800|\n",
      "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "amazon_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>AdjClose</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-07-15</td>\n",
       "      <td>2021.400024</td>\n",
       "      <td>2022.900024</td>\n",
       "      <td>2001.550049</td>\n",
       "      <td>2020.989990</td>\n",
       "      <td>2020.989990</td>\n",
       "      <td>2981300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-07-16</td>\n",
       "      <td>2010.579956</td>\n",
       "      <td>2026.319946</td>\n",
       "      <td>2001.219971</td>\n",
       "      <td>2009.900024</td>\n",
       "      <td>2009.900024</td>\n",
       "      <td>2618200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-07-17</td>\n",
       "      <td>2007.050049</td>\n",
       "      <td>2012.000000</td>\n",
       "      <td>1992.030029</td>\n",
       "      <td>1992.030029</td>\n",
       "      <td>1992.030029</td>\n",
       "      <td>2558800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-07-18</td>\n",
       "      <td>1980.010010</td>\n",
       "      <td>1987.500000</td>\n",
       "      <td>1951.550049</td>\n",
       "      <td>1977.900024</td>\n",
       "      <td>1977.900024</td>\n",
       "      <td>3504300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-07-19</td>\n",
       "      <td>1991.209961</td>\n",
       "      <td>1996.000000</td>\n",
       "      <td>1962.229980</td>\n",
       "      <td>1964.520020</td>\n",
       "      <td>1964.520020</td>\n",
       "      <td>3185600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date         Open         High          Low        Close   \n",
       "0  2019-07-15  2021.400024  2022.900024  2001.550049  2020.989990  \\\n",
       "1  2019-07-16  2010.579956  2026.319946  2001.219971  2009.900024   \n",
       "2  2019-07-17  2007.050049  2012.000000  1992.030029  1992.030029   \n",
       "3  2019-07-18  1980.010010  1987.500000  1951.550049  1977.900024   \n",
       "4  2019-07-19  1991.209961  1996.000000  1962.229980  1964.520020   \n",
       "\n",
       "      AdjClose   Volume  \n",
       "0  2020.989990  2981300  \n",
       "1  2009.900024  2618200  \n",
       "2  1992.030029  2558800  \n",
       "3  1977.900024  3504300  \n",
       "4  1964.520020  3185600  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "amazon_df.toPandas().head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPLORE AND QUERY DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_df = spark.read.csv('GOOG.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|  yr|        avg(Close)|\n",
      "+----+------------------+\n",
      "|2019|1245.3833654621849|\n",
      "|2020|1362.8286906865671|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month\n",
    "google_df.select(year(\"Date\").alias('yr'), \"Close\").groupby('yr').avg('Close').sort('yr').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DataFrame in module pyspark.sql.dataframe object:\n",
      "\n",
      "class DataFrame(pyspark.sql.pandas.map_ops.PandasMapOpsMixin, pyspark.sql.pandas.conversion.PandasConversionMixin)\n",
      " |  DataFrame(jdf: py4j.java_gateway.JavaObject, sql_ctx: Union[ForwardRef('SQLContext'), ForwardRef('SparkSession')])\n",
      " |  \n",
      " |  A distributed collection of data grouped into named columns.\n",
      " |  \n",
      " |  .. versionadded:: 1.3.0\n",
      " |  \n",
      " |  .. versionchanged:: 3.4.0\n",
      " |      Supports Spark Connect.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
      " |  and can be created using various functions in :class:`SparkSession`:\n",
      " |  \n",
      " |  >>> people = spark.createDataFrame([\n",
      " |  ...     {\"deptId\": 1, \"age\": 40, \"name\": \"Hyukjin Kwon\", \"gender\": \"M\", \"salary\": 50},\n",
      " |  ...     {\"deptId\": 1, \"age\": 50, \"name\": \"Takuya Ueshin\", \"gender\": \"M\", \"salary\": 100},\n",
      " |  ...     {\"deptId\": 2, \"age\": 60, \"name\": \"Xinrong Meng\", \"gender\": \"F\", \"salary\": 150},\n",
      " |  ...     {\"deptId\": 3, \"age\": 20, \"name\": \"Haejoon Lee\", \"gender\": \"M\", \"salary\": 200}\n",
      " |  ... ])\n",
      " |  \n",
      " |  Once created, it can be manipulated using the various domain-specific-language\n",
      " |  (DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
      " |  \n",
      " |  To select a column from the :class:`DataFrame`, use the apply method:\n",
      " |  \n",
      " |  >>> age_col = people.age\n",
      " |  \n",
      " |  A more concrete example:\n",
      " |  \n",
      " |  >>> # To create DataFrame using SparkSession\n",
      " |  ... department = spark.createDataFrame([\n",
      " |  ...     {\"id\": 1, \"name\": \"PySpark\"},\n",
      " |  ...     {\"id\": 2, \"name\": \"ML\"},\n",
      " |  ...     {\"id\": 3, \"name\": \"Spark SQL\"}\n",
      " |  ... ])\n",
      " |  \n",
      " |  >>> people.filter(people.age > 30).join(\n",
      " |  ...     department, people.deptId == department.id).groupBy(\n",
      " |  ...     department.name, \"gender\").agg({\"salary\": \"avg\", \"age\": \"max\"}).show()\n",
      " |  +-------+------+-----------+--------+\n",
      " |  |   name|gender|avg(salary)|max(age)|\n",
      " |  +-------+------+-----------+--------+\n",
      " |  |     ML|     F|      150.0|      60|\n",
      " |  |PySpark|     M|       75.0|      50|\n",
      " |  +-------+------+-----------+--------+\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  A DataFrame should only be created as described above. It should not be directly\n",
      " |  created via using the constructor.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DataFrame\n",
      " |      pyspark.sql.pandas.map_ops.PandasMapOpsMixin\n",
      " |      pyspark.sql.pandas.conversion.PandasConversionMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> pyspark.sql.column.Column\n",
      " |      Returns the :class:`Column` denoted by ``name``.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          Column name to return as :class:`Column`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`Column`\n",
      " |          Requested column.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      \n",
      " |      Retrieve a column instance.\n",
      " |      \n",
      " |      >>> df.select(df.age).show()\n",
      " |      +---+\n",
      " |      |age|\n",
      " |      +---+\n",
      " |      |  2|\n",
      " |      |  5|\n",
      " |      +---+\n",
      " |  \n",
      " |  __getitem__(self, item: Union[int, str, pyspark.sql.column.Column, List, Tuple]) -> Union[pyspark.sql.column.Column, ForwardRef('DataFrame')]\n",
      " |      Returns the column as a :class:`Column`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      item : int, str, :class:`Column`, list or tuple\n",
      " |          column index, column name, column, or a list or tuple of columns\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`Column` or :class:`DataFrame`\n",
      " |          a specified column, or a filtered or projected dataframe.\n",
      " |      \n",
      " |          * If the input `item` is an int or str, the output is a :class:`Column`.\n",
      " |      \n",
      " |          * If the input `item` is a :class:`Column`, the output is a :class:`DataFrame`\n",
      " |              filtered by this given :class:`Column`.\n",
      " |      \n",
      " |          * If the input `item` is a list or tuple, the output is a :class:`DataFrame`\n",
      " |              projected by this given list or tuple.\n",
      " |      \n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      \n",
      " |      Retrieve a column instance.\n",
      " |      \n",
      " |      >>> df.select(df['age']).show()\n",
      " |      +---+\n",
      " |      |age|\n",
      " |      +---+\n",
      " |      |  2|\n",
      " |      |  5|\n",
      " |      +---+\n",
      " |      \n",
      " |      >>> df.select(df[1]).show()\n",
      " |      +-----+\n",
      " |      | name|\n",
      " |      +-----+\n",
      " |      |Alice|\n",
      " |      |  Bob|\n",
      " |      +-----+\n",
      " |      \n",
      " |      Select multiple string columns as index.\n",
      " |      \n",
      " |      >>> df[[\"name\", \"age\"]].show()\n",
      " |      +-----+---+\n",
      " |      | name|age|\n",
      " |      +-----+---+\n",
      " |      |Alice|  2|\n",
      " |      |  Bob|  5|\n",
      " |      +-----+---+\n",
      " |      >>> df[df.age > 3].show()\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      |  5| Bob|\n",
      " |      +---+----+\n",
      " |      >>> df[df[0] > 3].show()\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      |  5| Bob|\n",
      " |      +---+----+\n",
      " |  \n",
      " |  __init__(self, jdf: py4j.java_gateway.JavaObject, sql_ctx: Union[ForwardRef('SQLContext'), ForwardRef('SparkSession')])\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  agg(self, *exprs: Union[pyspark.sql.column.Column, Dict[str, str]]) -> 'DataFrame'\n",
      " |      Aggregate on the entire :class:`DataFrame` without groups\n",
      " |      (shorthand for ``df.groupBy().agg()``).\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      exprs : :class:`Column` or dict of key and value strings\n",
      " |          Columns or expressions to aggregate DataFrame by.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Aggregated DataFrame.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import functions as F\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.agg({\"age\": \"max\"}).show()\n",
      " |      +--------+\n",
      " |      |max(age)|\n",
      " |      +--------+\n",
      " |      |       5|\n",
      " |      +--------+\n",
      " |      >>> df.agg(F.min(df.age)).show()\n",
      " |      +--------+\n",
      " |      |min(age)|\n",
      " |      +--------+\n",
      " |      |       2|\n",
      " |      +--------+\n",
      " |  \n",
      " |  alias(self, alias: str) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` with an alias set.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      alias : str\n",
      " |          an alias name to be set for the :class:`DataFrame`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Aliased DataFrame.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql.functions import col, desc\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      >>> df_as1 = df.alias(\"df_as1\")\n",
      " |      >>> df_as2 = df.alias(\"df_as2\")\n",
      " |      >>> joined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), 'inner')\n",
      " |      >>> joined_df.select(\n",
      " |      ...     \"df_as1.name\", \"df_as2.name\", \"df_as2.age\").sort(desc(\"df_as1.name\")).show()\n",
      " |      +-----+-----+---+\n",
      " |      | name| name|age|\n",
      " |      +-----+-----+---+\n",
      " |      |  Tom|  Tom| 14|\n",
      " |      |  Bob|  Bob| 16|\n",
      " |      |Alice|Alice| 23|\n",
      " |      +-----+-----+---+\n",
      " |  \n",
      " |  approxQuantile(self, col: Union[str, List[str], Tuple[str]], probabilities: Union[List[float], Tuple[float]], relativeError: float) -> Union[List[float], List[List[float]]]\n",
      " |      Calculates the approximate quantiles of numerical columns of a\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      The result of this algorithm has the following deterministic bound:\n",
      " |      If the :class:`DataFrame` has N elements and if we request the quantile at\n",
      " |      probability `p` up to error `err`, then the algorithm will return\n",
      " |      a sample `x` from the :class:`DataFrame` so that the *exact* rank of `x` is\n",
      " |      close to (p * N). More precisely,\n",
      " |      \n",
      " |        floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).\n",
      " |      \n",
      " |      This method implements a variation of the Greenwald-Khanna\n",
      " |      algorithm (with some speed optimizations). The algorithm was first\n",
      " |      present in [[https://doi.org/10.1145/375663.375670\n",
      " |      Space-efficient Online Computation of Quantile Summaries]]\n",
      " |      by Greenwald and Khanna.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      col: str, tuple or list\n",
      " |          Can be a single column name, or a list of names for multiple columns.\n",
      " |      \n",
      " |          .. versionchanged:: 2.2.0\n",
      " |             Added support for multiple columns.\n",
      " |      probabilities : list or tuple\n",
      " |          a list of quantile probabilities\n",
      " |          Each number must belong to [0, 1].\n",
      " |          For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n",
      " |      relativeError : float\n",
      " |          The relative target precision to achieve\n",
      " |          (>= 0). If set to zero, the exact quantiles are computed, which\n",
      " |          could be very expensive. Note that values greater than 1 are\n",
      " |          accepted but gives the same result as 1.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          the approximate quantiles at the given probabilities.\n",
      " |      \n",
      " |          * If the input `col` is a string, the output is a list of floats.\n",
      " |      \n",
      " |          * If the input `col` is a list or tuple of strings, the output is also a\n",
      " |              list, but each element in it is a list of floats, i.e., the output\n",
      " |              is a list of list of floats.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Null values will be ignored in numerical columns before calculation.\n",
      " |      For columns only containing null values, an empty list is returned.\n",
      " |  \n",
      " |  cache(self) -> 'DataFrame'\n",
      " |      Persists the :class:`DataFrame` with the default storage level (`MEMORY_AND_DISK`).\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The default storage level has changed to `MEMORY_AND_DISK` to match Scala in 2.0.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Cached DataFrame.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.range(1)\n",
      " |      >>> df.cache()\n",
      " |      DataFrame[id: bigint]\n",
      " |      \n",
      " |      >>> df.explain()\n",
      " |      == Physical Plan ==\n",
      " |      InMemoryTableScan ...\n",
      " |  \n",
      " |  checkpoint(self, eager: bool = True) -> 'DataFrame'\n",
      " |      Returns a checkpointed version of this :class:`DataFrame`. Checkpointing can be used to\n",
      " |      truncate the logical plan of this :class:`DataFrame`, which is especially useful in\n",
      " |      iterative algorithms where the plan may grow exponentially. It will be saved to files\n",
      " |      inside the checkpoint directory set with :meth:`SparkContext.setCheckpointDir`.\n",
      " |      \n",
      " |      .. versionadded:: 2.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      eager : bool, optional, default True\n",
      " |          Whether to checkpoint this :class:`DataFrame` immediately.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Checkpointed DataFrame.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is experimental.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import tempfile\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     (14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     spark.sparkContext.setCheckpointDir(\"/tmp/bb\")\n",
      " |      ...     df.checkpoint(False)\n",
      " |      DataFrame[age: bigint, name: string]\n",
      " |  \n",
      " |  coalesce(self, numPartitions: int) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` that has exactly `numPartitions` partitions.\n",
      " |      \n",
      " |      Similar to coalesce defined on an :class:`RDD`, this operation results in a\n",
      " |      narrow dependency, e.g. if you go from 1000 partitions to 100 partitions,\n",
      " |      there will not be a shuffle, instead each of the 100 new partitions will\n",
      " |      claim 10 of the current partitions. If a larger number of partitions is requested,\n",
      " |      it will stay at the current number of partitions.\n",
      " |      \n",
      " |      However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,\n",
      " |      this may result in your computation taking place on fewer nodes than\n",
      " |      you like (e.g. one node in the case of numPartitions = 1). To avoid this,\n",
      " |      you can call repartition(). This will add a shuffle step, but means the\n",
      " |      current upstream partitions will be executed in parallel (per whatever\n",
      " |      the current partitioning is).\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      numPartitions : int\n",
      " |          specify the target number of partitions\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.range(10)\n",
      " |      >>> df.coalesce(1).rdd.getNumPartitions()\n",
      " |      1\n",
      " |  \n",
      " |  colRegex(self, colName: str) -> pyspark.sql.column.Column\n",
      " |      Selects column based on the column name specified as a regex and returns it\n",
      " |      as :class:`Column`.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      colName : str\n",
      " |          string, column name specified as a regex.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`Column`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"c\",  3)], [\"Col1\", \"Col2\"])\n",
      " |      >>> df.select(df.colRegex(\"`(Col1)?+.+`\")).show()\n",
      " |      +----+\n",
      " |      |Col2|\n",
      " |      +----+\n",
      " |      |   1|\n",
      " |      |   2|\n",
      " |      |   3|\n",
      " |      +----+\n",
      " |  \n",
      " |  collect(self) -> List[pyspark.sql.types.Row]\n",
      " |      Returns all the records as a list of :class:`Row`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          List of rows.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      >>> df.collect()\n",
      " |      [Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\n",
      " |  \n",
      " |  corr(self, col1: str, col2: str, method: Optional[str] = None) -> float\n",
      " |      Calculates the correlation of two columns of a :class:`DataFrame` as a double value.\n",
      " |      Currently only supports the Pearson Correlation Coefficient.\n",
      " |      :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      col1 : str\n",
      " |          The name of the first column\n",
      " |      col2 : str\n",
      " |          The name of the second column\n",
      " |      method : str, optional\n",
      " |          The correlation method. Currently only supports \"pearson\"\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Pearson Correlation Coefficient of two columns.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(1, 12), (10, 1), (19, 8)], [\"c1\", \"c2\"])\n",
      " |      >>> df.corr(\"c1\", \"c2\")\n",
      " |      -0.3592106040535498\n",
      " |      >>> df = spark.createDataFrame([(11, 12), (10, 11), (9, 10)], [\"small\", \"bigger\"])\n",
      " |      >>> df.corr(\"small\", \"bigger\")\n",
      " |      1.0\n",
      " |  \n",
      " |  count(self) -> int\n",
      " |      Returns the number of rows in this :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      int\n",
      " |          Number of rows.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      \n",
      " |      Return the number of rows in the :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.count()\n",
      " |      3\n",
      " |  \n",
      " |  cov(self, col1: str, col2: str) -> float\n",
      " |      Calculate the sample covariance for the given columns, specified by their names, as a\n",
      " |      double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      col1 : str\n",
      " |          The name of the first column\n",
      " |      col2 : str\n",
      " |          The name of the second column\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Covariance of two columns.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(1, 12), (10, 1), (19, 8)], [\"c1\", \"c2\"])\n",
      " |      >>> df.cov(\"c1\", \"c2\")\n",
      " |      -18.0\n",
      " |      >>> df = spark.createDataFrame([(11, 12), (10, 11), (9, 10)], [\"small\", \"bigger\"])\n",
      " |      >>> df.cov(\"small\", \"bigger\")\n",
      " |      1.0\n",
      " |  \n",
      " |  createGlobalTempView(self, name: str) -> None\n",
      " |      Creates a global temporary view with this :class:`DataFrame`.\n",
      " |      \n",
      " |      The lifetime of this temporary view is tied to this Spark application.\n",
      " |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
      " |      catalog.\n",
      " |      \n",
      " |      .. versionadded:: 2.1.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          Name of the view.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Create a global temporary view.\n",
      " |      \n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.createGlobalTempView(\"people\")\n",
      " |      >>> df2 = spark.sql(\"SELECT * FROM global_temp.people\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      \n",
      " |      Throws an exception if the global temporary view already exists.\n",
      " |      \n",
      " |      >>> df.createGlobalTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |      ...\n",
      " |      AnalysisException: \"Temporary table 'people' already exists;\"\n",
      " |      >>> spark.catalog.dropGlobalTempView(\"people\")\n",
      " |      True\n",
      " |  \n",
      " |  createOrReplaceGlobalTempView(self, name: str) -> None\n",
      " |      Creates or replaces a global temporary view using the given name.\n",
      " |      \n",
      " |      The lifetime of this temporary view is tied to this Spark application.\n",
      " |      \n",
      " |      .. versionadded:: 2.2.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          Name of the view.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Create a global temporary view.\n",
      " |      \n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.createOrReplaceGlobalTempView(\"people\")\n",
      " |      \n",
      " |      Replace the global temporary view.\n",
      " |      \n",
      " |      >>> df2 = df.filter(df.age > 3)\n",
      " |      >>> df2.createOrReplaceGlobalTempView(\"people\")\n",
      " |      >>> df3 = spark.sql(\"SELECT * FROM global_temp.people\")\n",
      " |      >>> sorted(df3.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> spark.catalog.dropGlobalTempView(\"people\")\n",
      " |      True\n",
      " |  \n",
      " |  createOrReplaceTempView(self, name: str) -> None\n",
      " |      Creates or replaces a local temporary view with this :class:`DataFrame`.\n",
      " |      \n",
      " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      " |      that was used to create this :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          Name of the view.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Create a local temporary view named 'people'.\n",
      " |      \n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.createOrReplaceTempView(\"people\")\n",
      " |      \n",
      " |      Replace the local temporary view.\n",
      " |      \n",
      " |      >>> df2 = df.filter(df.age > 3)\n",
      " |      >>> df2.createOrReplaceTempView(\"people\")\n",
      " |      >>> df3 = spark.sql(\"SELECT * FROM people\")\n",
      " |      >>> sorted(df3.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> spark.catalog.dropTempView(\"people\")\n",
      " |      True\n",
      " |  \n",
      " |  createTempView(self, name: str) -> None\n",
      " |      Creates a local temporary view with this :class:`DataFrame`.\n",
      " |      \n",
      " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      " |      that was used to create this :class:`DataFrame`.\n",
      " |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
      " |      catalog.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          Name of the view.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Create a local temporary view.\n",
      " |      \n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.createTempView(\"people\")\n",
      " |      >>> df2 = spark.sql(\"SELECT * FROM people\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      \n",
      " |      Throw an exception if the table already exists.\n",
      " |      \n",
      " |      >>> df.createTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |      ...\n",
      " |      AnalysisException: \"Temporary table 'people' already exists;\"\n",
      " |      >>> spark.catalog.dropTempView(\"people\")\n",
      " |      True\n",
      " |  \n",
      " |  crossJoin(self, other: 'DataFrame') -> 'DataFrame'\n",
      " |      Returns the cartesian product with another :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 2.1.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`DataFrame`\n",
      " |          Right side of the cartesian product.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Joined DataFrame.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      >>> df2 = spark.createDataFrame(\n",
      " |      ...     [Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
      " |      >>> df.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", \"height\").show()\n",
      " |      +---+-----+------+\n",
      " |      |age| name|height|\n",
      " |      +---+-----+------+\n",
      " |      | 14|  Tom|    80|\n",
      " |      | 14|  Tom|    85|\n",
      " |      | 23|Alice|    80|\n",
      " |      | 23|Alice|    85|\n",
      " |      | 16|  Bob|    80|\n",
      " |      | 16|  Bob|    85|\n",
      " |      +---+-----+------+\n",
      " |  \n",
      " |  crosstab(self, col1: str, col2: str) -> 'DataFrame'\n",
      " |      Computes a pair-wise frequency table of the given columns. Also known as a contingency\n",
      " |      table.\n",
      " |      The first column of each row will be the distinct values of `col1` and the column names\n",
      " |      will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\n",
      " |      Pairs that have no occurrences will have zero as their counts.\n",
      " |      :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      col1 : str\n",
      " |          The name of the first column. Distinct items will make the first item of\n",
      " |          each row.\n",
      " |      col2 : str\n",
      " |          The name of the second column. Distinct items will make the column names\n",
      " |          of the :class:`DataFrame`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Frequency matrix of two columns.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(1, 11), (1, 11), (3, 10), (4, 8), (4, 8)], [\"c1\", \"c2\"])\n",
      " |      >>> df.crosstab(\"c1\", \"c2\").sort(\"c1_c2\").show()\n",
      " |      +-----+---+---+---+\n",
      " |      |c1_c2| 10| 11|  8|\n",
      " |      +-----+---+---+---+\n",
      " |      |    1|  0|  2|  0|\n",
      " |      |    3|  1|  0|  0|\n",
      " |      |    4|  0|  0|  2|\n",
      " |      +-----+---+---+---+\n",
      " |  \n",
      " |  cube(self, *cols: 'ColumnOrName') -> 'GroupedData'\n",
      " |      Create a multi-dimensional cube for the current :class:`DataFrame` using\n",
      " |      the specified columns, so we can run aggregations on them.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : list, str or :class:`Column`\n",
      " |          columns to create cube by.\n",
      " |          Each element should be a column name (string) or an expression (:class:`Column`)\n",
      " |          or list of them.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`GroupedData`\n",
      " |          Cube of the data by given columns.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.cube(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
      " |      +-----+----+-----+\n",
      " |      | name| age|count|\n",
      " |      +-----+----+-----+\n",
      " |      | null|null|    2|\n",
      " |      | null|   2|    1|\n",
      " |      | null|   5|    1|\n",
      " |      |Alice|null|    1|\n",
      " |      |Alice|   2|    1|\n",
      " |      |  Bob|null|    1|\n",
      " |      |  Bob|   5|    1|\n",
      " |      +-----+----+-----+\n",
      " |  \n",
      " |  describe(self, *cols: Union[str, List[str]]) -> 'DataFrame'\n",
      " |      Computes basic statistics for numeric and string columns.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      This includes count, mean, stddev, min, and max. If no columns are\n",
      " |      given, this function computes statistics for all numerical or string columns.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This function is meant for exploratory data analysis, as we make no\n",
      " |      guarantee about the backward compatibility of the schema of the resulting\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      Use summary for expanded statistics and control over which statistics to compute.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : str, list, optional\n",
      " |           Column name or list of column names to describe by (default All columns).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          A new DataFrame that describes (provides statistics) given DataFrame.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(\"Bob\", 13, 40.3, 150.5), (\"Alice\", 12, 37.8, 142.3), (\"Tom\", 11, 44.1, 142.2)],\n",
      " |      ...     [\"name\", \"age\", \"weight\", \"height\"],\n",
      " |      ... )\n",
      " |      >>> df.describe(['age']).show()\n",
      " |      +-------+----+\n",
      " |      |summary| age|\n",
      " |      +-------+----+\n",
      " |      |  count|   3|\n",
      " |      |   mean|12.0|\n",
      " |      | stddev| 1.0|\n",
      " |      |    min|  11|\n",
      " |      |    max|  13|\n",
      " |      +-------+----+\n",
      " |      \n",
      " |      >>> df.describe(['age', 'weight', 'height']).show()\n",
      " |      +-------+----+------------------+-----------------+\n",
      " |      |summary| age|            weight|           height|\n",
      " |      +-------+----+------------------+-----------------+\n",
      " |      |  count|   3|                 3|                3|\n",
      " |      |   mean|12.0| 40.73333333333333|            145.0|\n",
      " |      | stddev| 1.0|3.1722757341273704|4.763402145525822|\n",
      " |      |    min|  11|              37.8|            142.2|\n",
      " |      |    max|  13|              44.1|            150.5|\n",
      " |      +-------+----+------------------+-----------------+\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.summary\n",
      " |  \n",
      " |  distinct(self) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` containing the distinct rows in this :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          DataFrame with distinct records.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (23, \"Alice\")], [\"age\", \"name\"])\n",
      " |      \n",
      " |      Return the number of distinct rows in the :class:`DataFrame`\n",
      " |      \n",
      " |      >>> df.distinct().count()\n",
      " |      2\n",
      " |  \n",
      " |  drop(self, *cols: 'ColumnOrName') -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` without specified columns.\n",
      " |      This is a no-op if the schema doesn't contain the given column name(s).\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols: str or :class:`Column`\n",
      " |          a name of the column, or the :class:`Column` to drop\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          DataFrame without given columns.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      >>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
      " |      \n",
      " |      >>> df.drop('age').show()\n",
      " |      +-----+\n",
      " |      | name|\n",
      " |      +-----+\n",
      " |      |  Tom|\n",
      " |      |Alice|\n",
      " |      |  Bob|\n",
      " |      +-----+\n",
      " |      >>> df.drop(df.age).show()\n",
      " |      +-----+\n",
      " |      | name|\n",
      " |      +-----+\n",
      " |      |  Tom|\n",
      " |      |Alice|\n",
      " |      |  Bob|\n",
      " |      +-----+\n",
      " |      \n",
      " |      Drop the column that joined both DataFrames on.\n",
      " |      \n",
      " |      >>> df.join(df2, df.name == df2.name, 'inner').drop('name').sort('age').show()\n",
      " |      +---+------+\n",
      " |      |age|height|\n",
      " |      +---+------+\n",
      " |      | 14|    80|\n",
      " |      | 16|    85|\n",
      " |      +---+------+\n",
      " |  \n",
      " |  dropDuplicates(self, subset: Optional[List[str]] = None) -> 'DataFrame'\n",
      " |      Return a new :class:`DataFrame` with duplicate rows removed,\n",
      " |      optionally only considering certain columns.\n",
      " |      \n",
      " |      For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n",
      " |      :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n",
      " |      duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n",
      " |      be and the system will accordingly limit the state. In addition, data older than\n",
      " |      watermark will be dropped to avoid any possibility of duplicates.\n",
      " |      \n",
      " |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      subset : List of column names, optional\n",
      " |          List of columns to use for duplicate comparison (default All columns).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          DataFrame without duplicates.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     Row(name='Alice', age=5, height=80),\n",
      " |      ...     Row(name='Alice', age=5, height=80),\n",
      " |      ...     Row(name='Alice', age=10, height=80)\n",
      " |      ... ])\n",
      " |      \n",
      " |      Deduplicate the same rows.\n",
      " |      \n",
      " |      >>> df.dropDuplicates().show()\n",
      " |      +-----+---+------+\n",
      " |      | name|age|height|\n",
      " |      +-----+---+------+\n",
      " |      |Alice|  5|    80|\n",
      " |      |Alice| 10|    80|\n",
      " |      +-----+---+------+\n",
      " |      \n",
      " |      Deduplicate values on 'name' and 'height' columns.\n",
      " |      \n",
      " |      >>> df.dropDuplicates(['name', 'height']).show()\n",
      " |      +-----+---+------+\n",
      " |      | name|age|height|\n",
      " |      +-----+---+------+\n",
      " |      |Alice|  5|    80|\n",
      " |      +-----+---+------+\n",
      " |  \n",
      " |  drop_duplicates = dropDuplicates(self, subset=None)\n",
      " |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  dropna(self, how: str = 'any', thresh: Optional[int] = None, subset: Union[str, Tuple[str, ...], List[str], NoneType] = None) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` omitting rows with null values.\n",
      " |      :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      how : str, optional\n",
      " |          'any' or 'all'.\n",
      " |          If 'any', drop a row if it contains any nulls.\n",
      " |          If 'all', drop a row only if all its values are null.\n",
      " |      thresh: int, optional\n",
      " |          default None\n",
      " |          If specified, drop rows that have less than `thresh` non-null values.\n",
      " |          This overwrites the `how` parameter.\n",
      " |      subset : str, tuple or list, optional\n",
      " |          optional list of column names to consider.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          DataFrame with null only rows excluded.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     Row(age=10, height=80, name=\"Alice\"),\n",
      " |      ...     Row(age=5, height=None, name=\"Bob\"),\n",
      " |      ...     Row(age=None, height=None, name=\"Tom\"),\n",
      " |      ...     Row(age=None, height=None, name=None),\n",
      " |      ... ])\n",
      " |      >>> df.na.drop().show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      | 10|    80|Alice|\n",
      " |      +---+------+-----+\n",
      " |  \n",
      " |  exceptAll(self, other: 'DataFrame') -> 'DataFrame'\n",
      " |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame` but\n",
      " |      not in another :class:`DataFrame` while preserving duplicates.\n",
      " |      \n",
      " |      This is equivalent to `EXCEPT ALL` in SQL.\n",
      " |      As standard in SQL, this function resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`DataFrame`\n",
      " |          The other :class:`DataFrame` to compare to.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df1 = spark.createDataFrame(\n",
      " |      ...         [(\"a\", 1), (\"a\", 1), (\"a\", 1), (\"a\", 2), (\"b\",  3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      " |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      " |      >>> df1.exceptAll(df2).show()\n",
      " |      +---+---+\n",
      " |      | C1| C2|\n",
      " |      +---+---+\n",
      " |      |  a|  1|\n",
      " |      |  a|  1|\n",
      " |      |  a|  2|\n",
      " |      |  c|  4|\n",
      " |      +---+---+\n",
      " |  \n",
      " |  explain(self, extended: Union[bool, str, NoneType] = None, mode: Optional[str] = None) -> None\n",
      " |      Prints the (logical and physical) plans to the console for debugging purposes.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      extended : bool, optional\n",
      " |          default ``False``. If ``False``, prints only the physical plan.\n",
      " |          When this is a string without specifying the ``mode``, it works as the mode is\n",
      " |          specified.\n",
      " |      mode : str, optional\n",
      " |          specifies the expected output format of plans.\n",
      " |      \n",
      " |          * ``simple``: Print only a physical plan.\n",
      " |          * ``extended``: Print both logical and physical plans.\n",
      " |          * ``codegen``: Print a physical plan and generated codes if they are available.\n",
      " |          * ``cost``: Print a logical plan and statistics if they are available.\n",
      " |          * ``formatted``: Split explain output into two sections: a physical plan outline                 and node details.\n",
      " |      \n",
      " |          .. versionchanged:: 3.0.0\n",
      " |             Added optional argument `mode` to specify the expected output format of plans.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      \n",
      " |      Print out the physical plan only (default).\n",
      " |      \n",
      " |      >>> df.explain()  # doctest: +SKIP\n",
      " |      == Physical Plan ==\n",
      " |      *(1) Scan ExistingRDD[age...,name...]\n",
      " |      \n",
      " |      Print out all of the parsed, analyzed, optimized and physical plans.\n",
      " |      \n",
      " |      >>> df.explain(True)\n",
      " |      == Parsed Logical Plan ==\n",
      " |      ...\n",
      " |      == Analyzed Logical Plan ==\n",
      " |      ...\n",
      " |      == Optimized Logical Plan ==\n",
      " |      ...\n",
      " |      == Physical Plan ==\n",
      " |      ...\n",
      " |      \n",
      " |      Print out the plans with two sections: a physical plan outline and node details\n",
      " |      \n",
      " |      >>> df.explain(mode=\"formatted\")  # doctest: +SKIP\n",
      " |      == Physical Plan ==\n",
      " |      * Scan ExistingRDD (...)\n",
      " |      (1) Scan ExistingRDD [codegen id : ...]\n",
      " |      Output [2]: [age..., name...]\n",
      " |      ...\n",
      " |      \n",
      " |      Print a logical plan and statistics if they are available.\n",
      " |      \n",
      " |      >>> df.explain(\"cost\")\n",
      " |      == Optimized Logical Plan ==\n",
      " |      ...Statistics...\n",
      " |      ...\n",
      " |  \n",
      " |  fillna(self, value: Union[ForwardRef('LiteralType'), Dict[str, ForwardRef('LiteralType')]], subset: Union[str, Tuple[str, ...], List[str], NoneType] = None) -> 'DataFrame'\n",
      " |      Replace null values, alias for ``na.fill()``.\n",
      " |      :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      value : int, float, string, bool or dict\n",
      " |          Value to replace null values with.\n",
      " |          If the value is a dict, then `subset` is ignored and `value` must be a mapping\n",
      " |          from column name (string) to replacement value. The replacement value must be\n",
      " |          an int, float, boolean, or string.\n",
      " |      subset : str, tuple or list, optional\n",
      " |          optional list of column names to consider.\n",
      " |          Columns specified in subset that do not have matching data types are ignored.\n",
      " |          For example, if `value` is a string, and subset contains a non-string column,\n",
      " |          then the non-string column is simply ignored.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          DataFrame with replaced null values.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     (10, 80.5, \"Alice\", None),\n",
      " |      ...     (5, None, \"Bob\", None),\n",
      " |      ...     (None, None, \"Tom\", None),\n",
      " |      ...     (None, None, None, True)],\n",
      " |      ...     schema=[\"age\", \"height\", \"name\", \"bool\"])\n",
      " |      \n",
      " |      Fill all null values with 50 for numeric columns.\n",
      " |      \n",
      " |      >>> df.na.fill(50).show()\n",
      " |      +---+------+-----+----+\n",
      " |      |age|height| name|bool|\n",
      " |      +---+------+-----+----+\n",
      " |      | 10|  80.5|Alice|null|\n",
      " |      |  5|  50.0|  Bob|null|\n",
      " |      | 50|  50.0|  Tom|null|\n",
      " |      | 50|  50.0| null|true|\n",
      " |      +---+------+-----+----+\n",
      " |      \n",
      " |      Fill all null values with ``False`` for boolean columns.\n",
      " |      \n",
      " |      >>> df.na.fill(False).show()\n",
      " |      +----+------+-----+-----+\n",
      " |      | age|height| name| bool|\n",
      " |      +----+------+-----+-----+\n",
      " |      |  10|  80.5|Alice|false|\n",
      " |      |   5|  null|  Bob|false|\n",
      " |      |null|  null|  Tom|false|\n",
      " |      |null|  null| null| true|\n",
      " |      +----+------+-----+-----+\n",
      " |      \n",
      " |      Fill all null values with to 50 and \"unknown\" for 'age' and 'name' column respectively.\n",
      " |      \n",
      " |      >>> df.na.fill({'age': 50, 'name': 'unknown'}).show()\n",
      " |      +---+------+-------+----+\n",
      " |      |age|height|   name|bool|\n",
      " |      +---+------+-------+----+\n",
      " |      | 10|  80.5|  Alice|null|\n",
      " |      |  5|  null|    Bob|null|\n",
      " |      | 50|  null|    Tom|null|\n",
      " |      | 50|  null|unknown|true|\n",
      " |      +---+------+-------+----+\n",
      " |  \n",
      " |  filter(self, condition: 'ColumnOrName') -> 'DataFrame'\n",
      " |      Filters rows using the given condition.\n",
      " |      \n",
      " |      :func:`where` is an alias for :func:`filter`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      condition : :class:`Column` or str\n",
      " |          a :class:`Column` of :class:`types.BooleanType`\n",
      " |          or a string of SQL expressions.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Filtered DataFrame.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      \n",
      " |      Filter by :class:`Column` instances.\n",
      " |      \n",
      " |      >>> df.filter(df.age > 3).show()\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      |  5| Bob|\n",
      " |      +---+----+\n",
      " |      >>> df.where(df.age == 2).show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      +---+-----+\n",
      " |      \n",
      " |      Filter by SQL expression in a string.\n",
      " |      \n",
      " |      >>> df.filter(\"age > 3\").show()\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      |  5| Bob|\n",
      " |      +---+----+\n",
      " |      >>> df.where(\"age = 2\").show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      +---+-----+\n",
      " |  \n",
      " |  first(self) -> Optional[pyspark.sql.types.Row]\n",
      " |      Returns the first row as a :class:`Row`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`Row`\n",
      " |          First row if :class:`DataFrame` is not empty, otherwise ``None``.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.first()\n",
      " |      Row(age=2, name='Alice')\n",
      " |  \n",
      " |  foreach(self, f: Callable[[pyspark.sql.types.Row], NoneType]) -> None\n",
      " |      Applies the ``f`` function to all :class:`Row` of this :class:`DataFrame`.\n",
      " |      \n",
      " |      This is a shorthand for ``df.rdd.foreach()``.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |          A function that accepts one parameter which will\n",
      " |          receive each row to process.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      >>> def func(person):\n",
      " |      ...     print(person.name)\n",
      " |      >>> df.foreach(func)\n",
      " |  \n",
      " |  foreachPartition(self, f: Callable[[Iterator[pyspark.sql.types.Row]], NoneType]) -> None\n",
      " |      Applies the ``f`` function to each partition of this :class:`DataFrame`.\n",
      " |      \n",
      " |      This a shorthand for ``df.rdd.foreachPartition()``.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |          A function that accepts one parameter which will receive\n",
      " |          each partition to process.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      >>> def func(itr):\n",
      " |      ...     for person in itr:\n",
      " |      ...         print(person.name)\n",
      " |      >>> df.foreachPartition(func)\n",
      " |  \n",
      " |  freqItems(self, cols: Union[List[str], Tuple[str]], support: Optional[float] = None) -> 'DataFrame'\n",
      " |      Finding frequent items for columns, possibly with false positives. Using the\n",
      " |      frequent element count algorithm described in\n",
      " |      \"https://doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou\".\n",
      " |      :func:`DataFrame.freqItems` and :func:`DataFrameStatFunctions.freqItems` are aliases.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : list or tuple\n",
      " |          Names of the columns to calculate frequent items for as a list or tuple of\n",
      " |          strings.\n",
      " |      support : float, optional\n",
      " |          The frequency with which to consider an item 'frequent'. Default is 1%.\n",
      " |          The support must be greater than 1e-4.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          DataFrame with frequent items.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This function is meant for exploratory data analysis, as we make no\n",
      " |      guarantee about the backward compatibility of the schema of the resulting\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(1, 11), (1, 11), (3, 10), (4, 8), (4, 8)], [\"c1\", \"c2\"])\n",
      " |      >>> df.freqItems([\"c1\", \"c2\"]).show()  # doctest: +SKIP\n",
      " |      +------------+------------+\n",
      " |      |c1_freqItems|c2_freqItems|\n",
      " |      +------------+------------+\n",
      " |      |   [4, 1, 3]| [8, 11, 10]|\n",
      " |      +------------+------------+\n",
      " |  \n",
      " |  groupBy(self, *cols: 'ColumnOrName') -> 'GroupedData'\n",
      " |      Groups the :class:`DataFrame` using the specified columns,\n",
      " |      so we can run aggregation on them. See :class:`GroupedData`\n",
      " |      for all the available aggregate functions.\n",
      " |      \n",
      " |      :func:`groupby` is an alias for :func:`groupBy`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : list, str or :class:`Column`\n",
      " |          columns to group by.\n",
      " |          Each element should be a column name (string) or an expression (:class:`Column`)\n",
      " |          or list of them.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`GroupedData`\n",
      " |          Grouped data by given columns.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     (2, \"Alice\"), (2, \"Bob\"), (2, \"Bob\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      \n",
      " |      Empty grouping columns triggers a global aggregation.\n",
      " |      \n",
      " |      >>> df.groupBy().avg().show()\n",
      " |      +--------+\n",
      " |      |avg(age)|\n",
      " |      +--------+\n",
      " |      |    2.75|\n",
      " |      +--------+\n",
      " |      \n",
      " |      Group-by 'name', and specify a dictionary to calculate the summation of 'age'.\n",
      " |      \n",
      " |      >>> df.groupBy(\"name\").agg({\"age\": \"sum\"}).sort(\"name\").show()\n",
      " |      +-----+--------+\n",
      " |      | name|sum(age)|\n",
      " |      +-----+--------+\n",
      " |      |Alice|       2|\n",
      " |      |  Bob|       9|\n",
      " |      +-----+--------+\n",
      " |      \n",
      " |      Group-by 'name', and calculate maximum values.\n",
      " |      \n",
      " |      >>> df.groupBy(df.name).max().sort(\"name\").show()\n",
      " |      +-----+--------+\n",
      " |      | name|max(age)|\n",
      " |      +-----+--------+\n",
      " |      |Alice|       2|\n",
      " |      |  Bob|       5|\n",
      " |      +-----+--------+\n",
      " |      \n",
      " |      Group-by 'name' and 'age', and calculate the number of rows in each group.\n",
      " |      \n",
      " |      >>> df.groupBy([\"name\", df.age]).count().sort(\"name\", \"age\").show()\n",
      " |      +-----+---+-----+\n",
      " |      | name|age|count|\n",
      " |      +-----+---+-----+\n",
      " |      |Alice|  2|    1|\n",
      " |      |  Bob|  2|    2|\n",
      " |      |  Bob|  5|    1|\n",
      " |      +-----+---+-----+\n",
      " |  \n",
      " |  groupby = groupBy(self, *cols)\n",
      " |      :func:`groupby` is an alias for :func:`groupBy`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  head(self, n: Optional[int] = None) -> Union[pyspark.sql.types.Row, NoneType, List[pyspark.sql.types.Row]]\n",
      " |      Returns the first ``n`` rows.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method should only be used if the resulting array is expected\n",
      " |      to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : int, optional\n",
      " |          default 1. Number of rows to return.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      If n is greater than 1, return a list of :class:`Row`.\n",
      " |      If n is 1, return a single Row.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.head()\n",
      " |      Row(age=2, name='Alice')\n",
      " |      >>> df.head(1)\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |  \n",
      " |  hint(self, name: str, *parameters: Union[ForwardRef('PrimitiveType'), List[ForwardRef('PrimitiveType')]]) -> 'DataFrame'\n",
      " |      Specifies some hint on the current :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 2.2.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          A name of the hint.\n",
      " |      parameters : str, list, float or int\n",
      " |          Optional parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Hinted DataFrame\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
      " |      >>> df.join(df2, \"name\").explain()  # doctest: +SKIP\n",
      " |      == Physical Plan ==\n",
      " |      ...\n",
      " |      ... +- SortMergeJoin ...\n",
      " |      ...\n",
      " |      \n",
      " |      Explicitly trigger the broadcast hashjoin by providing the hint in ``df2``.\n",
      " |      \n",
      " |      >>> df.join(df2.hint(\"broadcast\"), \"name\").explain()\n",
      " |      == Physical Plan ==\n",
      " |      ...\n",
      " |      ... +- BroadcastHashJoin ...\n",
      " |      ...\n",
      " |  \n",
      " |  inputFiles(self) -> List[str]\n",
      " |      Returns a best-effort snapshot of the files that compose this :class:`DataFrame`.\n",
      " |      This method simply asks each constituent BaseRelation for its respective files and\n",
      " |      takes the union of all results. Depending on the source relations, this may not find\n",
      " |      all input files. Duplicates are removed.\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          List of file paths.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a single-row DataFrame into a JSON file\n",
      " |      ...     spark.createDataFrame(\n",
      " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      " |      ...     ).repartition(1).write.json(d, mode=\"overwrite\")\n",
      " |      ...\n",
      " |      ...     # Read the JSON file as a DataFrame.\n",
      " |      ...     df = spark.read.format(\"json\").load(d)\n",
      " |      ...\n",
      " |      ...     # Returns the number of input files.\n",
      " |      ...     len(df.inputFiles())\n",
      " |      1\n",
      " |  \n",
      " |  intersect(self, other: 'DataFrame') -> 'DataFrame'\n",
      " |      Return a new :class:`DataFrame` containing rows only in\n",
      " |      both this :class:`DataFrame` and another :class:`DataFrame`.\n",
      " |      Note that any duplicates are removed. To preserve duplicates\n",
      " |      use :func:`intersectAll`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`DataFrame`\n",
      " |          Another :class:`DataFrame` that needs to be combined.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Combined DataFrame.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This is equivalent to `INTERSECT` in SQL.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      " |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      " |      >>> df1.intersect(df2).sort(df1.C1.desc()).show()\n",
      " |      +---+---+\n",
      " |      | C1| C2|\n",
      " |      +---+---+\n",
      " |      |  b|  3|\n",
      " |      |  a|  1|\n",
      " |      +---+---+\n",
      " |  \n",
      " |  intersectAll(self, other: 'DataFrame') -> 'DataFrame'\n",
      " |      Return a new :class:`DataFrame` containing rows in both this :class:`DataFrame`\n",
      " |      and another :class:`DataFrame` while preserving duplicates.\n",
      " |      \n",
      " |      This is equivalent to `INTERSECT ALL` in SQL. As standard in SQL, this function\n",
      " |      resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`DataFrame`\n",
      " |          Another :class:`DataFrame` that needs to be combined.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Combined DataFrame.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      " |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      " |      >>> df1.intersectAll(df2).sort(\"C1\", \"C2\").show()\n",
      " |      +---+---+\n",
      " |      | C1| C2|\n",
      " |      +---+---+\n",
      " |      |  a|  1|\n",
      " |      |  a|  1|\n",
      " |      |  b|  3|\n",
      " |      +---+---+\n",
      " |  \n",
      " |  isEmpty(self) -> bool\n",
      " |      Returns ``True`` if this :class:`DataFrame` is empty.\n",
      " |      \n",
      " |      .. versionadded:: 3.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool\n",
      " |          Whether it's empty DataFrame or not.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df_empty = spark.createDataFrame([], 'a STRING')\n",
      " |      >>> df_non_empty = spark.createDataFrame([\"a\"], 'STRING')\n",
      " |      >>> df_empty.isEmpty()\n",
      " |      True\n",
      " |      >>> df_non_empty.isEmpty()\n",
      " |      False\n",
      " |  \n",
      " |  isLocal(self) -> bool\n",
      " |      Returns ``True`` if the :func:`collect` and :func:`take` methods can be run locally\n",
      " |      (without any Spark executors).\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.sql(\"SHOW TABLES\")\n",
      " |      >>> df.isLocal()\n",
      " |      True\n",
      " |  \n",
      " |  join(self, other: 'DataFrame', on: Union[str, List[str], pyspark.sql.column.Column, List[pyspark.sql.column.Column], NoneType] = None, how: Optional[str] = None) -> 'DataFrame'\n",
      " |      Joins with another :class:`DataFrame`, using the given join expression.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`DataFrame`\n",
      " |          Right side of the join\n",
      " |      on : str, list or :class:`Column`, optional\n",
      " |          a string for the join column name, a list of column names,\n",
      " |          a join expression (Column), or a list of Columns.\n",
      " |          If `on` is a string or a list of strings indicating the name of the join column(s),\n",
      " |          the column(s) must exist on both sides, and this performs an equi-join.\n",
      " |      how : str, optional\n",
      " |          default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n",
      " |          ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,\n",
      " |          ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,\n",
      " |          ``anti``, ``leftanti`` and ``left_anti``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Joined DataFrame.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      The following performs a full outer join between ``df1`` and ``df2``.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> from pyspark.sql.functions import desc\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")]).toDF(\"age\", \"name\")\n",
      " |      >>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
      " |      >>> df3 = spark.createDataFrame([Row(age=2, name=\"Alice\"), Row(age=5, name=\"Bob\")])\n",
      " |      >>> df4 = spark.createDataFrame([\n",
      " |      ...     Row(age=10, height=80, name=\"Alice\"),\n",
      " |      ...     Row(age=5, height=None, name=\"Bob\"),\n",
      " |      ...     Row(age=None, height=None, name=\"Tom\"),\n",
      " |      ...     Row(age=None, height=None, name=None),\n",
      " |      ... ])\n",
      " |      \n",
      " |      Inner join on columns (default)\n",
      " |      \n",
      " |      >>> df.join(df2, 'name').select(df.name, df2.height).show()\n",
      " |      +----+------+\n",
      " |      |name|height|\n",
      " |      +----+------+\n",
      " |      | Bob|    85|\n",
      " |      +----+------+\n",
      " |      >>> df.join(df4, ['name', 'age']).select(df.name, df.age).show()\n",
      " |      +----+---+\n",
      " |      |name|age|\n",
      " |      +----+---+\n",
      " |      | Bob|  5|\n",
      " |      +----+---+\n",
      " |      \n",
      " |      Outer join for both DataFrames on the 'name' column.\n",
      " |      \n",
      " |      >>> df.join(df2, df.name == df2.name, 'outer').select(\n",
      " |      ...     df.name, df2.height).sort(desc(\"name\")).show()\n",
      " |      +-----+------+\n",
      " |      | name|height|\n",
      " |      +-----+------+\n",
      " |      |  Bob|    85|\n",
      " |      |Alice|  null|\n",
      " |      | null|    80|\n",
      " |      +-----+------+\n",
      " |      >>> df.join(df2, 'name', 'outer').select('name', 'height').sort(desc(\"name\")).show()\n",
      " |      +-----+------+\n",
      " |      | name|height|\n",
      " |      +-----+------+\n",
      " |      |  Tom|    80|\n",
      " |      |  Bob|    85|\n",
      " |      |Alice|  null|\n",
      " |      +-----+------+\n",
      " |      \n",
      " |      Outer join for both DataFrams with multiple columns.\n",
      " |      \n",
      " |      >>> df.join(\n",
      " |      ...     df3,\n",
      " |      ...     [df.name == df3.name, df.age == df3.age],\n",
      " |      ...     'outer'\n",
      " |      ... ).select(df.name, df3.age).show()\n",
      " |      +-----+---+\n",
      " |      | name|age|\n",
      " |      +-----+---+\n",
      " |      |Alice|  2|\n",
      " |      |  Bob|  5|\n",
      " |      +-----+---+\n",
      " |  \n",
      " |  limit(self, num: int) -> 'DataFrame'\n",
      " |      Limits the result count to the number specified.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      num : int\n",
      " |          Number of records to return. Will return this number of records\n",
      " |          or all records if the DataFrame contains less than this number of records.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Subset of the records\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      >>> df.limit(1).show()\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      | 14| Tom|\n",
      " |      +---+----+\n",
      " |      >>> df.limit(0).show()\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      +---+----+\n",
      " |  \n",
      " |  localCheckpoint(self, eager: bool = True) -> 'DataFrame'\n",
      " |      Returns a locally checkpointed version of this :class:`DataFrame`. Checkpointing can be\n",
      " |      used to truncate the logical plan of this :class:`DataFrame`, which is especially useful in\n",
      " |      iterative algorithms where the plan may grow exponentially. Local checkpoints are\n",
      " |      stored in the executors using the caching subsystem and therefore they are not reliable.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      eager : bool, optional, default True\n",
      " |          Whether to checkpoint this :class:`DataFrame` immediately.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Checkpointed DataFrame.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is experimental.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     (14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      >>> df.localCheckpoint(False)\n",
      " |      DataFrame[age: bigint, name: string]\n",
      " |  \n",
      " |  melt(self, ids: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName')], Tuple[ForwardRef('ColumnOrName'), ...]], values: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName')], Tuple[ForwardRef('ColumnOrName'), ...], NoneType], variableColumnName: str, valueColumnName: str) -> 'DataFrame'\n",
      " |      Unpivot a DataFrame from wide format to long format, optionally leaving\n",
      " |      identifier columns set. This is the reverse to `groupBy(...).pivot(...).agg(...)`,\n",
      " |      except for the aggregation, which cannot be reversed.\n",
      " |      \n",
      " |      :func:`melt` is an alias for :func:`unpivot`.\n",
      " |      \n",
      " |      .. versionadded:: 3.4.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ids : str, Column, tuple, list, optional\n",
      " |          Column(s) to use as identifiers. Can be a single column or column name,\n",
      " |          or a list or tuple for multiple columns.\n",
      " |      values : str, Column, tuple, list, optional\n",
      " |          Column(s) to unpivot. Can be a single column or column name, or a list or tuple\n",
      " |          for multiple columns. If not specified or empty, use all columns that\n",
      " |          are not set as `ids`.\n",
      " |      variableColumnName : str\n",
      " |          Name of the variable column.\n",
      " |      valueColumnName : str\n",
      " |          Name of the value column.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Unpivoted DataFrame.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.unpivot\n",
      " |  \n",
      " |  observe(self, observation: Union[ForwardRef('Observation'), str], *exprs: pyspark.sql.column.Column) -> 'DataFrame'\n",
      " |      Define (named) metrics to observe on the DataFrame. This method returns an 'observed'\n",
      " |      DataFrame that returns the same result as the input, with the following guarantees:\n",
      " |      \n",
      " |      * It will compute the defined aggregates (metrics) on all the data that is flowing through\n",
      " |          the Dataset at that point.\n",
      " |      \n",
      " |      * It will report the value of the defined aggregate columns as soon as we reach a completion\n",
      " |          point. A completion point is either the end of a query (batch mode) or the end of a\n",
      " |          streaming epoch. The value of the aggregates only reflects the data processed since\n",
      " |          the previous completion point.\n",
      " |      \n",
      " |      The metrics columns must either contain a literal (e.g. lit(42)), or should contain one or\n",
      " |      more aggregate functions (e.g. sum(a) or sum(a + b) + avg(c) - lit(1)). Expressions that\n",
      " |      contain references to the input Dataset's columns must always be wrapped in an aggregate\n",
      " |      function.\n",
      " |      \n",
      " |      A user can observe these metrics by adding\n",
      " |      Python's :class:`~pyspark.sql.streaming.StreamingQueryListener`,\n",
      " |      Scala/Java's ``org.apache.spark.sql.streaming.StreamingQueryListener`` or Scala/Java's\n",
      " |      ``org.apache.spark.sql.util.QueryExecutionListener`` to the spark session.\n",
      " |      \n",
      " |      .. versionadded:: 3.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      observation : :class:`Observation` or str\n",
      " |          `str` to specify the name, or an :class:`Observation` instance to obtain the metric.\n",
      " |      \n",
      " |          .. versionchanged:: 3.4.0\n",
      " |             Added support for `str` in this parameter.\n",
      " |      exprs : :class:`Column`\n",
      " |          column expressions (:class:`Column`).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          the observed :class:`DataFrame`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      When ``observation`` is :class:`Observation`, this method only supports batch queries.\n",
      " |      When ``observation`` is a string, this method works for both batch and streaming queries.\n",
      " |      Continuous execution is currently not supported yet.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      When ``observation`` is :class:`Observation`, only batch queries work as below.\n",
      " |      \n",
      " |      >>> from pyspark.sql.functions import col, count, lit, max\n",
      " |      >>> from pyspark.sql import Observation\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> observation = Observation(\"my metrics\")\n",
      " |      >>> observed_df = df.observe(observation, count(lit(1)).alias(\"count\"), max(col(\"age\")))\n",
      " |      >>> observed_df.count()\n",
      " |      2\n",
      " |      >>> observation.get\n",
      " |      {'count': 2, 'max(age)': 5}\n",
      " |      \n",
      " |      When ``observation`` is a string, streaming queries also work as below.\n",
      " |      \n",
      " |      >>> from pyspark.sql.streaming import StreamingQueryListener\n",
      " |      >>> class MyErrorListener(StreamingQueryListener):\n",
      " |      ...    def onQueryStarted(self, event):\n",
      " |      ...        pass\n",
      " |      ...\n",
      " |      ...    def onQueryProgress(self, event):\n",
      " |      ...        row = event.progress.observedMetrics.get(\"my_event\")\n",
      " |      ...        # Trigger if the number of errors exceeds 5 percent\n",
      " |      ...        num_rows = row.rc\n",
      " |      ...        num_error_rows = row.erc\n",
      " |      ...        ratio = num_error_rows / num_rows\n",
      " |      ...        if ratio > 0.05:\n",
      " |      ...            # Trigger alert\n",
      " |      ...            pass\n",
      " |      ...\n",
      " |      ...    def onQueryTerminated(self, event):\n",
      " |      ...        pass\n",
      " |      ...\n",
      " |      >>> spark.streams.addListener(MyErrorListener())\n",
      " |      >>> # Observe row count (rc) and error row count (erc) in the streaming Dataset\n",
      " |      ... observed_ds = df.observe(\n",
      " |      ...     \"my_event\",\n",
      " |      ...     count(lit(1)).alias(\"rc\"),\n",
      " |      ...     count(col(\"error\")).alias(\"erc\"))  # doctest: +SKIP\n",
      " |      >>> observed_ds.writeStream.format(\"console\").start()  # doctest: +SKIP\n",
      " |  \n",
      " |  orderBy = sort(self, *cols: Union[str, pyspark.sql.column.Column, List[Union[str, pyspark.sql.column.Column]]], **kwargs: Any) -> 'DataFrame'\n",
      " |  \n",
      " |  pandas_api(self, index_col: Union[str, List[str], NoneType] = None) -> 'PandasOnSparkDataFrame'\n",
      " |      Converts the existing DataFrame into a pandas-on-Spark DataFrame.\n",
      " |      \n",
      " |      If a pandas-on-Spark DataFrame is converted to a Spark DataFrame and then back\n",
      " |      to pandas-on-Spark, it will lose the index information and the original index\n",
      " |      will be turned into a normal column.\n",
      " |      \n",
      " |      This is only available if Pandas is installed and available.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      index_col: str or list of str, optional, default: None\n",
      " |          Index column of table in Spark.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`PandasOnSparkDataFrame`\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pyspark.pandas.frame.DataFrame.to_spark\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      \n",
      " |      >>> df.pandas_api()  # doctest: +SKIP\n",
      " |         age   name\n",
      " |      0   14    Tom\n",
      " |      1   23  Alice\n",
      " |      2   16    Bob\n",
      " |      \n",
      " |      We can specify the index columns.\n",
      " |      \n",
      " |      >>> df.pandas_api(index_col=\"age\")  # doctest: +SKIP\n",
      " |            name\n",
      " |      age\n",
      " |      14     Tom\n",
      " |      23   Alice\n",
      " |      16     Bob\n",
      " |  \n",
      " |  persist(self, storageLevel: pyspark.storagelevel.StorageLevel = StorageLevel(True, True, False, True, 1)) -> 'DataFrame'\n",
      " |      Sets the storage level to persist the contents of the :class:`DataFrame` across\n",
      " |      operations after the first time it is computed. This can only be used to assign\n",
      " |      a new storage level if the :class:`DataFrame` does not have a storage level set yet.\n",
      " |      If no storage level is specified defaults to (`MEMORY_AND_DISK_DESER`)\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The default storage level has changed to `MEMORY_AND_DISK_DESER` to match Scala in 3.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      storageLevel : :class:`StorageLevel`\n",
      " |          Storage level to set for persistence. Default is MEMORY_AND_DISK_DESER.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Persisted DataFrame.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.range(1)\n",
      " |      >>> df.persist()\n",
      " |      DataFrame[id: bigint]\n",
      " |      \n",
      " |      >>> df.explain()\n",
      " |      == Physical Plan ==\n",
      " |      InMemoryTableScan ...\n",
      " |      \n",
      " |      Persists the data in the disk by specifying the storage level.\n",
      " |      \n",
      " |      >>> from pyspark.storagelevel import StorageLevel\n",
      " |      >>> df.persist(StorageLevel.DISK_ONLY)\n",
      " |      DataFrame[id: bigint]\n",
      " |  \n",
      " |  printSchema(self) -> None\n",
      " |      Prints out the schema in the tree format.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      >>> df.printSchema()\n",
      " |      root\n",
      " |       |-- age: long (nullable = true)\n",
      " |       |-- name: string (nullable = true)\n",
      " |  \n",
      " |  randomSplit(self, weights: List[float], seed: Optional[int] = None) -> List[ForwardRef('DataFrame')]\n",
      " |      Randomly splits this :class:`DataFrame` with the provided weights.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      weights : list\n",
      " |          list of doubles as weights with which to split the :class:`DataFrame`.\n",
      " |          Weights will be normalized if they don't sum up to 1.0.\n",
      " |      seed : int, optional\n",
      " |          The seed for sampling.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          List of DataFrames.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     Row(age=10, height=80, name=\"Alice\"),\n",
      " |      ...     Row(age=5, height=None, name=\"Bob\"),\n",
      " |      ...     Row(age=None, height=None, name=\"Tom\"),\n",
      " |      ...     Row(age=None, height=None, name=None),\n",
      " |      ... ])\n",
      " |      \n",
      " |      >>> splits = df.randomSplit([1.0, 2.0], 24)\n",
      " |      >>> splits[0].count()\n",
      " |      2\n",
      " |      >>> splits[1].count()\n",
      " |      2\n",
      " |  \n",
      " |  registerTempTable(self, name: str) -> None\n",
      " |      Registers this :class:`DataFrame` as a temporary table using the given name.\n",
      " |      \n",
      " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      " |      that was used to create this :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      .. deprecated:: 2.0.0\n",
      " |          Use :meth:`DataFrame.createOrReplaceTempView` instead.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          Name of the temporary table to register.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.registerTempTable(\"people\")\n",
      " |      >>> df2 = spark.sql(\"SELECT * FROM people\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> spark.catalog.dropTempView(\"people\")\n",
      " |      True\n",
      " |  \n",
      " |  repartition(self, numPartitions: Union[int, ForwardRef('ColumnOrName')], *cols: 'ColumnOrName') -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
      " |      resulting :class:`DataFrame` is hash partitioned.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      numPartitions : int\n",
      " |          can be an int to specify the target number of partitions or a Column.\n",
      " |          If it is a Column, it will be used as the first partitioning column. If not specified,\n",
      " |          the default number of partitions is used.\n",
      " |      cols : str or :class:`Column`\n",
      " |          partitioning columns.\n",
      " |      \n",
      " |          .. versionchanged:: 1.6.0\n",
      " |             Added optional arguments to specify the partitioning columns. Also made numPartitions\n",
      " |             optional if partitioning columns are specified.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Repartitioned DataFrame.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      \n",
      " |      Repartition the data into 10 partitions.\n",
      " |      \n",
      " |      >>> df.repartition(10).rdd.getNumPartitions()\n",
      " |      10\n",
      " |      \n",
      " |      Repartition the data into 7 partitions by 'age' column.\n",
      " |      \n",
      " |      >>> df.repartition(7, \"age\").rdd.getNumPartitions()\n",
      " |      7\n",
      " |      \n",
      " |      Repartition the data into 7 partitions by 'age' and 'name columns.\n",
      " |      \n",
      " |      >>> df.repartition(3, \"name\", \"age\").rdd.getNumPartitions()\n",
      " |      3\n",
      " |  \n",
      " |  repartitionByRange(self, numPartitions: Union[int, ForwardRef('ColumnOrName')], *cols: 'ColumnOrName') -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
      " |      resulting :class:`DataFrame` is range partitioned.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      numPartitions : int\n",
      " |          can be an int to specify the target number of partitions or a Column.\n",
      " |          If it is a Column, it will be used as the first partitioning column. If not specified,\n",
      " |          the default number of partitions is used.\n",
      " |      cols : str or :class:`Column`\n",
      " |          partitioning columns.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Repartitioned DataFrame.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      At least one partition-by expression must be specified.\n",
      " |      When no explicit sort order is specified, \"ascending nulls first\" is assumed.\n",
      " |      \n",
      " |      Due to performance reasons this method uses sampling to estimate the ranges.\n",
      " |      Hence, the output may not be consistent, since sampling can return different values.\n",
      " |      The sample size can be controlled by the config\n",
      " |      `spark.sql.execution.rangeExchange.sampleSizePerPartition`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      \n",
      " |      Repartition the data into 2 partitions by range in 'age' column.\n",
      " |      For example, the first partition can have ``(14, \"Tom\")``, and the second\n",
      " |      partition would have ``(16, \"Bob\")`` and ``(23, \"Alice\")``.\n",
      " |      \n",
      " |      >>> df.repartitionByRange(2, \"age\").rdd.getNumPartitions()\n",
      " |      2\n",
      " |  \n",
      " |  replace(self, to_replace: Union[ForwardRef('LiteralType'), List[ForwardRef('LiteralType')], Dict[ForwardRef('LiteralType'), ForwardRef('OptionalPrimitiveType')]], value: Union[ForwardRef('OptionalPrimitiveType'), List[ForwardRef('OptionalPrimitiveType')], pyspark._globals._NoValueType, NoneType] = <no value>, subset: Optional[List[str]] = None) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` replacing a value with another value.\n",
      " |      :func:`DataFrame.replace` and :func:`DataFrameNaFunctions.replace` are\n",
      " |      aliases of each other.\n",
      " |      Values to_replace and value must have the same type and can only be numerics, booleans,\n",
      " |      or strings. Value can have None. When replacing, the new value will be cast\n",
      " |      to the type of the existing column.\n",
      " |      For numeric replacements all values to be replaced should have unique\n",
      " |      floating point representation. In case of conflicts (for example with `{42: -1, 42.0: 1}`)\n",
      " |      and arbitrary replacement will be used.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      to_replace : bool, int, float, string, list or dict\n",
      " |          Value to be replaced.\n",
      " |          If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`\n",
      " |          must be a mapping between a value and a replacement.\n",
      " |      value : bool, int, float, string or None, optional\n",
      " |          The replacement value must be a bool, int, float, string or None. If `value` is a\n",
      " |          list, `value` should be of the same length and type as `to_replace`.\n",
      " |          If `value` is a scalar and `to_replace` is a sequence, then `value` is\n",
      " |          used as a replacement for each item in `to_replace`.\n",
      " |      subset : list, optional\n",
      " |          optional list of column names to consider.\n",
      " |          Columns specified in subset that do not have matching data types are ignored.\n",
      " |          For example, if `value` is a string, and subset contains a non-string column,\n",
      " |          then the non-string column is simply ignored.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          DataFrame with replaced values.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     (10, 80, \"Alice\"),\n",
      " |      ...     (5, None, \"Bob\"),\n",
      " |      ...     (None, 10, \"Tom\"),\n",
      " |      ...     (None, None, None)],\n",
      " |      ...     schema=[\"age\", \"height\", \"name\"])\n",
      " |      \n",
      " |      Replace 10 to 20 in all columns.\n",
      " |      \n",
      " |      >>> df.na.replace(10, 20).show()\n",
      " |      +----+------+-----+\n",
      " |      | age|height| name|\n",
      " |      +----+------+-----+\n",
      " |      |  20|    80|Alice|\n",
      " |      |   5|  null|  Bob|\n",
      " |      |null|    20|  Tom|\n",
      " |      |null|  null| null|\n",
      " |      +----+------+-----+\n",
      " |      \n",
      " |      Replace 'Alice' to null in all columns.\n",
      " |      \n",
      " |      >>> df.na.replace('Alice', None).show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|null|\n",
      " |      |   5|  null| Bob|\n",
      " |      |null|    10| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |      \n",
      " |      Replace 'Alice' to 'A', and 'Bob' to 'B' in the 'name' column.\n",
      " |      \n",
      " |      >>> df.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|   A|\n",
      " |      |   5|  null|   B|\n",
      " |      |null|    10| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |  \n",
      " |  rollup(self, *cols: 'ColumnOrName') -> 'GroupedData'\n",
      " |      Create a multi-dimensional rollup for the current :class:`DataFrame` using\n",
      " |      the specified columns, so we can run aggregation on them.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : list, str or :class:`Column`\n",
      " |          Columns to roll-up by.\n",
      " |          Each element should be a column name (string) or an expression (:class:`Column`)\n",
      " |          or list of them.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`GroupedData`\n",
      " |          Rolled-up data by given columns.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.rollup(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
      " |      +-----+----+-----+\n",
      " |      | name| age|count|\n",
      " |      +-----+----+-----+\n",
      " |      | null|null|    2|\n",
      " |      |Alice|null|    1|\n",
      " |      |Alice|   2|    1|\n",
      " |      |  Bob|null|    1|\n",
      " |      |  Bob|   5|    1|\n",
      " |      +-----+----+-----+\n",
      " |  \n",
      " |  sameSemantics(self, other: 'DataFrame') -> bool\n",
      " |      Returns `True` when the logical query plans inside both :class:`DataFrame`\\s are equal and\n",
      " |      therefore return the same results.\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The equality comparison here is simplified by tolerating the cosmetic differences\n",
      " |      such as attribute names.\n",
      " |      \n",
      " |      This API can compare both :class:`DataFrame`\\s very fast but can still return\n",
      " |      `False` on the :class:`DataFrame` that return the same results, for instance, from\n",
      " |      different plans. Such false negative semantic can be useful when caching as an example.\n",
      " |      \n",
      " |      This API is a developer API.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`DataFrame`\n",
      " |          The other DataFrame to compare against.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool\n",
      " |          Whether these two DataFrames are similar.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df1 = spark.range(10)\n",
      " |      >>> df2 = spark.range(10)\n",
      " |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id * 2))\n",
      " |      True\n",
      " |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col1\", df2.id + 2))\n",
      " |      False\n",
      " |      >>> df1.withColumn(\"col1\", df1.id * 2).sameSemantics(df2.withColumn(\"col0\", df2.id * 2))\n",
      " |      True\n",
      " |  \n",
      " |  sample(self, withReplacement: Union[float, bool, NoneType] = None, fraction: Union[int, float, NoneType] = None, seed: Optional[int] = None) -> 'DataFrame'\n",
      " |      Returns a sampled subset of this :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      withReplacement : bool, optional\n",
      " |          Sample with replacement or not (default ``False``).\n",
      " |      fraction : float, optional\n",
      " |          Fraction of rows to generate, range [0.0, 1.0].\n",
      " |      seed : int, optional\n",
      " |          Seed for sampling (default a random seed).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Sampled rows from given DataFrame.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This is not guaranteed to provide exactly the fraction specified of the total\n",
      " |      count of the given :class:`DataFrame`.\n",
      " |      \n",
      " |      `fraction` is required and, `withReplacement` and `seed` are optional.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.range(10)\n",
      " |      >>> df.sample(0.5, 3).count()\n",
      " |      7\n",
      " |      >>> df.sample(fraction=0.5, seed=3).count()\n",
      " |      7\n",
      " |      >>> df.sample(withReplacement=True, fraction=0.5, seed=3).count()\n",
      " |      1\n",
      " |      >>> df.sample(1.0).count()\n",
      " |      10\n",
      " |      >>> df.sample(fraction=1.0).count()\n",
      " |      10\n",
      " |      >>> df.sample(False, fraction=1.0).count()\n",
      " |      10\n",
      " |  \n",
      " |  sampleBy(self, col: 'ColumnOrName', fractions: Dict[Any, float], seed: Optional[int] = None) -> 'DataFrame'\n",
      " |      Returns a stratified sample without replacement based on the\n",
      " |      fraction given on each stratum.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      col : :class:`Column` or str\n",
      " |          column that defines strata\n",
      " |      \n",
      " |          .. versionchanged:: 3.0.0\n",
      " |             Added sampling by a column of :class:`Column`\n",
      " |      fractions : dict\n",
      " |          sampling fraction for each stratum. If a stratum is not\n",
      " |          specified, we treat its fraction as zero.\n",
      " |      seed : int, optional\n",
      " |          random seed\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      a new :class:`DataFrame` that represents the stratified sample\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql.functions import col\n",
      " |      >>> dataset = spark.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n",
      " |      >>> sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n",
      " |      >>> sampled.groupBy(\"key\").count().orderBy(\"key\").show()\n",
      " |      +---+-----+\n",
      " |      |key|count|\n",
      " |      +---+-----+\n",
      " |      |  0|    3|\n",
      " |      |  1|    6|\n",
      " |      +---+-----+\n",
      " |      >>> dataset.sampleBy(col(\"key\"), fractions={2: 1.0}, seed=0).count()\n",
      " |      33\n",
      " |  \n",
      " |  select(self, *cols: 'ColumnOrName') -> 'DataFrame'\n",
      " |      Projects a set of expressions and returns a new :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : str, :class:`Column`, or list\n",
      " |          column names (string) or expressions (:class:`Column`).\n",
      " |          If one of the column names is '*', that column is expanded to include all columns\n",
      " |          in the current :class:`DataFrame`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          A DataFrame with subset (or all) of columns.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      \n",
      " |      Select all columns in the DataFrame.\n",
      " |      \n",
      " |      >>> df.select('*').show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      \n",
      " |      Select a column with other expressions in the DataFrame.\n",
      " |      \n",
      " |      >>> df.select(df.name, (df.age + 10).alias('age')).show()\n",
      " |      +-----+---+\n",
      " |      | name|age|\n",
      " |      +-----+---+\n",
      " |      |Alice| 12|\n",
      " |      |  Bob| 15|\n",
      " |      +-----+---+\n",
      " |  \n",
      " |  selectExpr(self, *expr: Union[str, List[str]]) -> 'DataFrame'\n",
      " |      Projects a set of SQL expressions and returns a new :class:`DataFrame`.\n",
      " |      \n",
      " |      This is a variant of :func:`select` that accepts SQL expressions.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          A DataFrame with new/old columns transformed by expressions.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.selectExpr(\"age * 2\", \"abs(age)\").show()\n",
      " |      +---------+--------+\n",
      " |      |(age * 2)|abs(age)|\n",
      " |      +---------+--------+\n",
      " |      |        4|       2|\n",
      " |      |       10|       5|\n",
      " |      +---------+--------+\n",
      " |  \n",
      " |  semanticHash(self) -> int\n",
      " |      Returns a hash code of the logical query plan against this :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Unlike the standard hash code, the hash is calculated against the query plan\n",
      " |      simplified by tolerating the cosmetic differences such as attribute names.\n",
      " |      \n",
      " |      This API is a developer API.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      int\n",
      " |          Hash value.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> spark.range(10).selectExpr(\"id as col0\").semanticHash()  # doctest: +SKIP\n",
      " |      1855039936\n",
      " |      >>> spark.range(10).selectExpr(\"id as col1\").semanticHash()  # doctest: +SKIP\n",
      " |      1855039936\n",
      " |  \n",
      " |  show(self, n: int = 20, truncate: Union[bool, int] = True, vertical: bool = False) -> None\n",
      " |      Prints the first ``n`` rows to the console.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : int, optional\n",
      " |          Number of rows to show.\n",
      " |      truncate : bool or int, optional\n",
      " |          If set to ``True``, truncate strings longer than 20 chars by default.\n",
      " |          If set to a number greater than one, truncates long strings to length ``truncate``\n",
      " |          and align cells right.\n",
      " |      vertical : bool, optional\n",
      " |          If set to ``True``, print output rows vertically (one line\n",
      " |          per column value).\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     (14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      \n",
      " |      Show only top 2 rows.\n",
      " |      \n",
      " |      >>> df.show(2)\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      | 14|  Tom|\n",
      " |      | 23|Alice|\n",
      " |      +---+-----+\n",
      " |      only showing top 2 rows\n",
      " |      \n",
      " |      Show :class:`DataFrame` where the maximum number of characters is 3.\n",
      " |      \n",
      " |      >>> df.show(truncate=3)\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      | 14| Tom|\n",
      " |      | 23| Ali|\n",
      " |      | 16| Bob|\n",
      " |      +---+----+\n",
      " |      \n",
      " |      Show :class:`DataFrame` vertically.\n",
      " |      \n",
      " |      >>> df.show(vertical=True)\n",
      " |      -RECORD 0-----\n",
      " |      age  | 14\n",
      " |      name | Tom\n",
      " |      -RECORD 1-----\n",
      " |      age  | 23\n",
      " |      name | Alice\n",
      " |      -RECORD 2-----\n",
      " |      age  | 16\n",
      " |      name | Bob\n",
      " |  \n",
      " |  sort(self, *cols: Union[str, pyspark.sql.column.Column, List[Union[str, pyspark.sql.column.Column]]], **kwargs: Any) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` sorted by the specified column(s).\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : str, list, or :class:`Column`, optional\n",
      " |           list of :class:`Column` or column names to sort by.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      ascending : bool or list, optional, default True\n",
      " |          boolean or list of boolean.\n",
      " |          Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      " |          If a list is specified, the length of the list must equal the length of the `cols`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Sorted DataFrame.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql.functions import desc, asc\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      \n",
      " |      Sort the DataFrame in ascending order.\n",
      " |      \n",
      " |      >>> df.sort(asc(\"age\")).show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      \n",
      " |      Sort the DataFrame in descending order.\n",
      " |      \n",
      " |      >>> df.sort(df.age.desc()).show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  5|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      +---+-----+\n",
      " |      >>> df.orderBy(df.age.desc()).show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  5|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      +---+-----+\n",
      " |      >>> df.sort(\"age\", ascending=False).show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  5|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      +---+-----+\n",
      " |      \n",
      " |      Specify multiple columns\n",
      " |      \n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     (2, \"Alice\"), (2, \"Bob\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.orderBy(desc(\"age\"), \"name\").show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  5|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      |  2|  Bob|\n",
      " |      +---+-----+\n",
      " |      \n",
      " |      Specify multiple columns for sorting order at `ascending`.\n",
      " |      \n",
      " |      >>> df.orderBy([\"age\", \"name\"], ascending=[False, False]).show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  5|  Bob|\n",
      " |      |  2|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      +---+-----+\n",
      " |  \n",
      " |  sortWithinPartitions(self, *cols: Union[str, pyspark.sql.column.Column, List[Union[str, pyspark.sql.column.Column]]], **kwargs: Any) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` with each partition sorted by the specified column(s).\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : str, list or :class:`Column`, optional\n",
      " |          list of :class:`Column` or column names to sort by.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      ascending : bool or list, optional, default True\n",
      " |          boolean or list of boolean.\n",
      " |          Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      " |          If a list is specified, the length of the list must equal the length of the `cols`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          DataFrame sorted by partitions.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.sortWithinPartitions(\"age\", ascending=False)\n",
      " |      DataFrame[age: bigint, name: string]\n",
      " |  \n",
      " |  subtract(self, other: 'DataFrame') -> 'DataFrame'\n",
      " |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame`\n",
      " |      but not in another :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`DataFrame`\n",
      " |          Another :class:`DataFrame` that needs to be subtracted.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Subtracted DataFrame.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This is equivalent to `EXCEPT DISTINCT` in SQL.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      " |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      " |      >>> df1.subtract(df2).show()\n",
      " |      +---+---+\n",
      " |      | C1| C2|\n",
      " |      +---+---+\n",
      " |      |  c|  4|\n",
      " |      +---+---+\n",
      " |  \n",
      " |  summary(self, *statistics: str) -> 'DataFrame'\n",
      " |      Computes specified statistics for numeric and string columns. Available statistics are:\n",
      " |      - count\n",
      " |      - mean\n",
      " |      - stddev\n",
      " |      - min\n",
      " |      - max\n",
      " |      - arbitrary approximate percentiles specified as a percentage (e.g., 75%)\n",
      " |      \n",
      " |      If no statistics are given, this function computes count, mean, stddev, min,\n",
      " |      approximate quartiles (percentiles at 25%, 50%, and 75%), and max.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      statistics : str, optional\n",
      " |           Column names to calculate statistics by (default All columns).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          A new DataFrame that provides statistics for the given DataFrame.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This function is meant for exploratory data analysis, as we make no\n",
      " |      guarantee about the backward compatibility of the schema of the resulting\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(\"Bob\", 13, 40.3, 150.5), (\"Alice\", 12, 37.8, 142.3), (\"Tom\", 11, 44.1, 142.2)],\n",
      " |      ...     [\"name\", \"age\", \"weight\", \"height\"],\n",
      " |      ... )\n",
      " |      >>> df.select(\"age\", \"weight\", \"height\").summary().show()\n",
      " |      +-------+----+------------------+-----------------+\n",
      " |      |summary| age|            weight|           height|\n",
      " |      +-------+----+------------------+-----------------+\n",
      " |      |  count|   3|                 3|                3|\n",
      " |      |   mean|12.0| 40.73333333333333|            145.0|\n",
      " |      | stddev| 1.0|3.1722757341273704|4.763402145525822|\n",
      " |      |    min|  11|              37.8|            142.2|\n",
      " |      |    25%|  11|              37.8|            142.2|\n",
      " |      |    50%|  12|              40.3|            142.3|\n",
      " |      |    75%|  13|              44.1|            150.5|\n",
      " |      |    max|  13|              44.1|            150.5|\n",
      " |      +-------+----+------------------+-----------------+\n",
      " |      \n",
      " |      >>> df.select(\"age\", \"weight\", \"height\").summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()\n",
      " |      +-------+---+------+------+\n",
      " |      |summary|age|weight|height|\n",
      " |      +-------+---+------+------+\n",
      " |      |  count|  3|     3|     3|\n",
      " |      |    min| 11|  37.8| 142.2|\n",
      " |      |    25%| 11|  37.8| 142.2|\n",
      " |      |    75%| 13|  44.1| 150.5|\n",
      " |      |    max| 13|  44.1| 150.5|\n",
      " |      +-------+---+------+------+\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.display\n",
      " |  \n",
      " |  tail(self, num: int) -> List[pyspark.sql.types.Row]\n",
      " |      Returns the last ``num`` rows as a :class:`list` of :class:`Row`.\n",
      " |      \n",
      " |      Running tail requires moving data into the application's driver process, and doing so with\n",
      " |      a very large ``num`` can crash the driver process with OutOfMemoryError.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      num : int\n",
      " |          Number of records to return. Will return this number of records\n",
      " |          or all records if the DataFrame contains less than this number of records.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          List of rows\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      \n",
      " |      >>> df.tail(2)\n",
      " |      [Row(age=23, name='Alice'), Row(age=16, name='Bob')]\n",
      " |  \n",
      " |  take(self, num: int) -> List[pyspark.sql.types.Row]\n",
      " |      Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      num : int\n",
      " |          Number of records to return. Will return this number of records\n",
      " |          or all records if the DataFrame contains less than this number of records..\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          List of rows\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      \n",
      " |      Return the first 2 rows of the :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.take(2)\n",
      " |      [Row(age=14, name='Tom'), Row(age=23, name='Alice')]\n",
      " |  \n",
      " |  to(self, schema: pyspark.sql.types.StructType) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` where each row is reconciled to match the specified\n",
      " |      schema.\n",
      " |      \n",
      " |      .. versionadded:: 3.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      schema : :class:`StructType`\n",
      " |          Specified schema.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Reconciled DataFrame.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      * Reorder columns and/or inner fields by name to match the specified schema.\n",
      " |      \n",
      " |      * Project away columns and/or inner fields that are not needed by the specified schema.\n",
      " |          Missing columns and/or inner fields (present in the specified schema but not input\n",
      " |          DataFrame) lead to failures.\n",
      " |      \n",
      " |      * Cast the columns and/or inner fields to match the data types in the specified schema,\n",
      " |          if the types are compatible, e.g., numeric to numeric (error if overflows), but\n",
      " |          not string to int.\n",
      " |      \n",
      " |      * Carry over the metadata from the specified schema, while the columns and/or inner fields\n",
      " |          still keep their own metadata if not overwritten by the specified schema.\n",
      " |      \n",
      " |      * Fail if the nullability is not compatible. For example, the column and/or inner field\n",
      " |          is nullable but the specified schema requires them to be not nullable.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql.types import StructField, StringType\n",
      " |      >>> df = spark.createDataFrame([(\"a\", 1)], [\"i\", \"j\"])\n",
      " |      >>> df.schema\n",
      " |      StructType([StructField('i', StringType(), True), StructField('j', LongType(), True)])\n",
      " |      \n",
      " |      >>> schema = StructType([StructField(\"j\", StringType()), StructField(\"i\", StringType())])\n",
      " |      >>> df2 = df.to(schema)\n",
      " |      >>> df2.schema\n",
      " |      StructType([StructField('j', StringType(), True), StructField('i', StringType(), True)])\n",
      " |      >>> df2.show()\n",
      " |      +---+---+\n",
      " |      |  j|  i|\n",
      " |      +---+---+\n",
      " |      |  1|  a|\n",
      " |      +---+---+\n",
      " |  \n",
      " |  toDF(self, *cols: str) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` that with new specified column names\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      *cols : tuple\n",
      " |          a tuple of string new column name. The length of the\n",
      " |          list needs to be the same as the number of columns in the initial\n",
      " |          :class:`DataFrame`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          DataFrame with new column names.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(14, \"Tom\"), (23, \"Alice\"),\n",
      " |      ...     (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      >>> df.toDF('f1', 'f2').show()\n",
      " |      +---+-----+\n",
      " |      | f1|   f2|\n",
      " |      +---+-----+\n",
      " |      | 14|  Tom|\n",
      " |      | 23|Alice|\n",
      " |      | 16|  Bob|\n",
      " |      +---+-----+\n",
      " |  \n",
      " |  toJSON(self, use_unicode: bool = True) -> pyspark.rdd.RDD[str]\n",
      " |      Converts a :class:`DataFrame` into a :class:`RDD` of string.\n",
      " |      \n",
      " |      Each row is turned into a JSON document as one element in the returned RDD.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      use_unicode : bool, optional, default True\n",
      " |          Whether to convert to unicode or not.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.toJSON().first()\n",
      " |      '{\"age\":2,\"name\":\"Alice\"}'\n",
      " |  \n",
      " |  toLocalIterator(self, prefetchPartitions: bool = False) -> Iterator[pyspark.sql.types.Row]\n",
      " |      Returns an iterator that contains all of the rows in this :class:`DataFrame`.\n",
      " |      The iterator will consume as much memory as the largest partition in this\n",
      " |      :class:`DataFrame`. With prefetch it may consume up to the memory of the 2 largest\n",
      " |      partitions.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      prefetchPartitions : bool, optional\n",
      " |          If Spark should pre-fetch the next partition before it is needed.\n",
      " |      \n",
      " |          .. versionchanged:: 3.4.0\n",
      " |              This argument does not take effect for Spark Connect.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Iterator\n",
      " |          Iterator of rows.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      >>> list(df.toLocalIterator())\n",
      " |      [Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\n",
      " |  \n",
      " |  to_koalas(self, index_col: Union[str, List[str], NoneType] = None) -> 'PandasOnSparkDataFrame'\n",
      " |      # Keep to_koalas for backward compatibility for now.\n",
      " |  \n",
      " |  to_pandas_on_spark(self, index_col: Union[str, List[str], NoneType] = None) -> 'PandasOnSparkDataFrame'\n",
      " |      # Keep to_pandas_on_spark for backward compatibility for now.\n",
      " |  \n",
      " |  transform(self, func: Callable[..., ForwardRef('DataFrame')], *args: Any, **kwargs: Any) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame`. Concise syntax for chaining custom transformations.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : function\n",
      " |          a function that takes and returns a :class:`DataFrame`.\n",
      " |      *args\n",
      " |          Positional arguments to pass to func.\n",
      " |      \n",
      " |          .. versionadded:: 3.3.0\n",
      " |      **kwargs\n",
      " |          Keyword arguments to pass to func.\n",
      " |      \n",
      " |          .. versionadded:: 3.3.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Transformed DataFrame.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql.functions import col\n",
      " |      >>> df = spark.createDataFrame([(1, 1.0), (2, 2.0)], [\"int\", \"float\"])\n",
      " |      >>> def cast_all_to_int(input_df):\n",
      " |      ...     return input_df.select([col(col_name).cast(\"int\") for col_name in input_df.columns])\n",
      " |      >>> def sort_columns_asc(input_df):\n",
      " |      ...     return input_df.select(*sorted(input_df.columns))\n",
      " |      >>> df.transform(cast_all_to_int).transform(sort_columns_asc).show()\n",
      " |      +-----+---+\n",
      " |      |float|int|\n",
      " |      +-----+---+\n",
      " |      |    1|  1|\n",
      " |      |    2|  2|\n",
      " |      +-----+---+\n",
      " |      \n",
      " |      >>> def add_n(input_df, n):\n",
      " |      ...     return input_df.select([(col(col_name) + n).alias(col_name)\n",
      " |      ...                             for col_name in input_df.columns])\n",
      " |      >>> df.transform(add_n, 1).transform(add_n, n=10).show()\n",
      " |      +---+-----+\n",
      " |      |int|float|\n",
      " |      +---+-----+\n",
      " |      | 12| 12.0|\n",
      " |      | 13| 13.0|\n",
      " |      +---+-----+\n",
      " |  \n",
      " |  union(self, other: 'DataFrame') -> 'DataFrame'\n",
      " |      Return a new :class:`DataFrame` containing union of rows in this and another\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`DataFrame`\n",
      " |          Another :class:`DataFrame` that needs to be unioned\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.unionAll\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n",
      " |      (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      " |      \n",
      " |      Also as standard in SQL, this function resolves columns by position (not by name).\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
      " |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
      " |      >>> df1.union(df2).show()\n",
      " |      +----+----+----+\n",
      " |      |col0|col1|col2|\n",
      " |      +----+----+----+\n",
      " |      |   1|   2|   3|\n",
      " |      |   4|   5|   6|\n",
      " |      +----+----+----+\n",
      " |      >>> df1.union(df1).show()\n",
      " |      +----+----+----+\n",
      " |      |col0|col1|col2|\n",
      " |      +----+----+----+\n",
      " |      |   1|   2|   3|\n",
      " |      |   1|   2|   3|\n",
      " |      +----+----+----+\n",
      " |  \n",
      " |  unionAll(self, other: 'DataFrame') -> 'DataFrame'\n",
      " |      Return a new :class:`DataFrame` containing union of rows in this and another\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`DataFrame`\n",
      " |          Another :class:`DataFrame` that needs to be combined\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Combined DataFrame\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n",
      " |      (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      " |      \n",
      " |      Also as standard in SQL, this function resolves columns by position (not by name).\n",
      " |      \n",
      " |      :func:`unionAll` is an alias to :func:`union`\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.union\n",
      " |  \n",
      " |  unionByName(self, other: 'DataFrame', allowMissingColumns: bool = False) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` containing union of rows in this and another\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set\n",
      " |      union (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`DataFrame`\n",
      " |          Another :class:`DataFrame` that needs to be combined.\n",
      " |      allowMissingColumns : bool, optional, default False\n",
      " |         Specify whether to allow missing columns.\n",
      " |      \n",
      " |         .. versionadded:: 3.1.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Combined DataFrame.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      The difference between this function and :func:`union` is that this function\n",
      " |      resolves columns by name (not by position):\n",
      " |      \n",
      " |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
      " |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
      " |      >>> df1.unionByName(df2).show()\n",
      " |      +----+----+----+\n",
      " |      |col0|col1|col2|\n",
      " |      +----+----+----+\n",
      " |      |   1|   2|   3|\n",
      " |      |   6|   4|   5|\n",
      " |      +----+----+----+\n",
      " |      \n",
      " |      When the parameter `allowMissingColumns` is ``True``, the set of column names\n",
      " |      in this and other :class:`DataFrame` can differ; missing columns will be filled with null.\n",
      " |      Further, the missing columns of this :class:`DataFrame` will be added at the end\n",
      " |      in the schema of the union result:\n",
      " |      \n",
      " |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
      " |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col3\"])\n",
      " |      >>> df1.unionByName(df2, allowMissingColumns=True).show()\n",
      " |      +----+----+----+----+\n",
      " |      |col0|col1|col2|col3|\n",
      " |      +----+----+----+----+\n",
      " |      |   1|   2|   3|null|\n",
      " |      |null|   4|   5|   6|\n",
      " |      +----+----+----+----+\n",
      " |  \n",
      " |  unpersist(self, blocking: bool = False) -> 'DataFrame'\n",
      " |      Marks the :class:`DataFrame` as non-persistent, and remove all blocks for it from\n",
      " |      memory and disk.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      `blocking` default has changed to ``False`` to match Scala in 2.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      blocking : bool\n",
      " |          Whether to block until all blocks are deleted.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Unpersisted DataFrame.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.range(1)\n",
      " |      >>> df.persist()\n",
      " |      DataFrame[id: bigint]\n",
      " |      >>> df.unpersist()\n",
      " |      DataFrame[id: bigint]\n",
      " |      >>> df = spark.range(1)\n",
      " |      >>> df.unpersist(True)\n",
      " |      DataFrame[id: bigint]\n",
      " |  \n",
      " |  unpivot(self, ids: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName')], Tuple[ForwardRef('ColumnOrName'), ...]], values: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName')], Tuple[ForwardRef('ColumnOrName'), ...], NoneType], variableColumnName: str, valueColumnName: str) -> 'DataFrame'\n",
      " |      Unpivot a DataFrame from wide format to long format, optionally leaving\n",
      " |      identifier columns set. This is the reverse to `groupBy(...).pivot(...).agg(...)`,\n",
      " |      except for the aggregation, which cannot be reversed.\n",
      " |      \n",
      " |      This function is useful to massage a DataFrame into a format where some\n",
      " |      columns are identifier columns (\"ids\"), while all other columns (\"values\")\n",
      " |      are \"unpivoted\" to the rows, leaving just two non-id columns, named as given\n",
      " |      by `variableColumnName` and `valueColumnName`.\n",
      " |      \n",
      " |      When no \"id\" columns are given, the unpivoted DataFrame consists of only the\n",
      " |      \"variable\" and \"value\" columns.\n",
      " |      \n",
      " |      The `values` columns must not be empty so at least one value must be given to be unpivoted.\n",
      " |      When `values` is `None`, all non-id columns will be unpivoted.\n",
      " |      \n",
      " |      All \"value\" columns must share a least common data type. Unless they are the same data type,\n",
      " |      all \"value\" columns are cast to the nearest common data type. For instance, types\n",
      " |      `IntegerType` and `LongType` are cast to `LongType`, while `IntegerType` and `StringType`\n",
      " |      do not have a common data type and `unpivot` fails.\n",
      " |      \n",
      " |      .. versionadded:: 3.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ids : str, Column, tuple, list\n",
      " |          Column(s) to use as identifiers. Can be a single column or column name,\n",
      " |          or a list or tuple for multiple columns.\n",
      " |      values : str, Column, tuple, list, optional\n",
      " |          Column(s) to unpivot. Can be a single column or column name, or a list or tuple\n",
      " |          for multiple columns. If specified, must not be empty. If not specified, uses all\n",
      " |          columns that are not set as `ids`.\n",
      " |      variableColumnName : str\n",
      " |          Name of the variable column.\n",
      " |      valueColumnName : str\n",
      " |          Name of the value column.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Unpivoted DataFrame.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(1, 11, 1.1), (2, 12, 1.2)],\n",
      " |      ...     [\"id\", \"int\", \"double\"],\n",
      " |      ... )\n",
      " |      >>> df.show()\n",
      " |      +---+---+------+\n",
      " |      | id|int|double|\n",
      " |      +---+---+------+\n",
      " |      |  1| 11|   1.1|\n",
      " |      |  2| 12|   1.2|\n",
      " |      +---+---+------+\n",
      " |      \n",
      " |      >>> df.unpivot(\"id\", [\"int\", \"double\"], \"var\", \"val\").show()\n",
      " |      +---+------+----+\n",
      " |      | id|   var| val|\n",
      " |      +---+------+----+\n",
      " |      |  1|   int|11.0|\n",
      " |      |  1|double| 1.1|\n",
      " |      |  2|   int|12.0|\n",
      " |      |  2|double| 1.2|\n",
      " |      +---+------+----+\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataFrame.melt\n",
      " |  \n",
      " |  where = filter(self, condition)\n",
      " |      :func:`where` is an alias for :func:`filter`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  withColumn(self, colName: str, col: pyspark.sql.column.Column) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` by adding a column or replacing the\n",
      " |      existing column that has the same name.\n",
      " |      \n",
      " |      The column expression must be an expression over this :class:`DataFrame`; attempting to add\n",
      " |      a column from some other :class:`DataFrame` will raise an error.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      colName : str\n",
      " |          string, name of the new column.\n",
      " |      col : :class:`Column`\n",
      " |          a :class:`Column` expression for the new column.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          DataFrame with new or replaced column.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method introduces a projection internally. Therefore, calling it multiple\n",
      " |      times, for instance, via loops in order to add multiple columns can generate big\n",
      " |      plans which can cause performance issues and even `StackOverflowException`.\n",
      " |      To avoid this, use :func:`select` with multiple columns at once.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.withColumn('age2', df.age + 2).show()\n",
      " |      +---+-----+----+\n",
      " |      |age| name|age2|\n",
      " |      +---+-----+----+\n",
      " |      |  2|Alice|   4|\n",
      " |      |  5|  Bob|   7|\n",
      " |      +---+-----+----+\n",
      " |  \n",
      " |  withColumnRenamed(self, existing: str, new: str) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` by renaming an existing column.\n",
      " |      This is a no-op if the schema doesn't contain the given column name.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      existing : str\n",
      " |          string, name of the existing column to rename.\n",
      " |      new : str\n",
      " |          string, new name of the column.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          DataFrame with renamed column.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.withColumnRenamed('age', 'age2').show()\n",
      " |      +----+-----+\n",
      " |      |age2| name|\n",
      " |      +----+-----+\n",
      " |      |   2|Alice|\n",
      " |      |   5|  Bob|\n",
      " |      +----+-----+\n",
      " |  \n",
      " |  withColumns(self, *colsMap: Dict[str, pyspark.sql.column.Column]) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` by adding multiple columns or replacing the\n",
      " |      existing columns that have the same names.\n",
      " |      \n",
      " |      The colsMap is a map of column name and column, the column must only refer to attributes\n",
      " |      supplied by this Dataset. It is an error to add columns that refer to some other Dataset.\n",
      " |      \n",
      " |      .. versionadded:: 3.3.0\n",
      " |         Added support for multiple columns adding\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      colsMap : dict\n",
      " |          a dict of column name and :class:`Column`. Currently, only a single map is supported.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          DataFrame with new or replaced columns.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df.withColumns({'age2': df.age + 2, 'age3': df.age + 3}).show()\n",
      " |      +---+-----+----+----+\n",
      " |      |age| name|age2|age3|\n",
      " |      +---+-----+----+----+\n",
      " |      |  2|Alice|   4|   5|\n",
      " |      |  5|  Bob|   7|   8|\n",
      " |      +---+-----+----+----+\n",
      " |  \n",
      " |  withColumnsRenamed(self, colsMap: Dict[str, str]) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` by renaming multiple columns.\n",
      " |      This is a no-op if the schema doesn't contain the given column names.\n",
      " |      \n",
      " |      .. versionadded:: 3.4.0\n",
      " |         Added support for multiple columns renaming\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      colsMap : dict\n",
      " |          a dict of existing column names and corresponding desired column names.\n",
      " |          Currently, only a single map is supported.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          DataFrame with renamed columns.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`withColumnRenamed`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df = df.withColumns({'age2': df.age + 2, 'age3': df.age + 3})\n",
      " |      >>> df.withColumnsRenamed({'age2': 'age4', 'age3': 'age5'}).show()\n",
      " |      +---+-----+----+----+\n",
      " |      |age| name|age4|age5|\n",
      " |      +---+-----+----+----+\n",
      " |      |  2|Alice|   4|   5|\n",
      " |      |  5|  Bob|   7|   8|\n",
      " |      +---+-----+----+----+\n",
      " |  \n",
      " |  withMetadata(self, columnName: str, metadata: Dict[str, Any]) -> 'DataFrame'\n",
      " |      Returns a new :class:`DataFrame` by updating an existing column with metadata.\n",
      " |      \n",
      " |      .. versionadded:: 3.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      columnName : str\n",
      " |          string, name of the existing column to update the metadata.\n",
      " |      metadata : dict\n",
      " |          dict, new metadata to be assigned to df.schema[columnName].metadata\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          DataFrame with updated metadata column.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> df_meta = df.withMetadata('age', {'foo': 'bar'})\n",
      " |      >>> df_meta.schema['age'].metadata\n",
      " |      {'foo': 'bar'}\n",
      " |  \n",
      " |  withWatermark(self, eventTime: str, delayThreshold: str) -> 'DataFrame'\n",
      " |      Defines an event time watermark for this :class:`DataFrame`. A watermark tracks a point\n",
      " |      in time before which we assume no more late data is going to arrive.\n",
      " |      \n",
      " |      Spark will use this watermark for several purposes:\n",
      " |        - To know when a given time window aggregation can be finalized and thus can be emitted\n",
      " |          when using output modes that do not allow updates.\n",
      " |      \n",
      " |        - To minimize the amount of state that we need to keep for on-going aggregations.\n",
      " |      \n",
      " |      The current watermark is computed by looking at the `MAX(eventTime)` seen across\n",
      " |      all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost\n",
      " |      of coordinating this value across partitions, the actual watermark used is only guaranteed\n",
      " |      to be at least `delayThreshold` behind the actual event time.  In some cases we may still\n",
      " |      process records that arrive more than `delayThreshold` late.\n",
      " |      \n",
      " |      .. versionadded:: 2.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      eventTime : str\n",
      " |          the name of the column that contains the event time of the row.\n",
      " |      delayThreshold : str\n",
      " |          the minimum delay to wait to data to arrive late, relative to the\n",
      " |          latest record that has been processed in the form of an interval\n",
      " |          (e.g. \"1 minute\" or \"5 hours\").\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrame`\n",
      " |          Watermarked DataFrame\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This is a feature only for Structured Streaming.\n",
      " |      \n",
      " |      This API is evolving.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> from pyspark.sql.functions import timestamp_seconds\n",
      " |      >>> df = spark.readStream.format(\"rate\").load().selectExpr(\n",
      " |      ...     \"value % 5 AS value\", \"timestamp\")\n",
      " |      >>> df.select(\"value\", df.timestamp.alias(\"time\")).withWatermark(\"time\", '10 minutes')\n",
      " |      DataFrame[value: bigint, time: timestamp]\n",
      " |      \n",
      " |      Group the data by window and value (0 - 4), and compute the count of each group.\n",
      " |      \n",
      " |      >>> import time\n",
      " |      >>> from pyspark.sql.functions import window\n",
      " |      >>> query = (df\n",
      " |      ...     .withWatermark(\"timestamp\", \"10 minutes\")\n",
      " |      ...     .groupBy(\n",
      " |      ...         window(df.timestamp, \"10 minutes\", \"5 minutes\"),\n",
      " |      ...         df.value)\n",
      " |      ...     ).count().writeStream.outputMode(\"complete\").format(\"console\").start()\n",
      " |      >>> time.sleep(3)\n",
      " |      >>> query.stop()\n",
      " |  \n",
      " |  writeTo(self, table: str) -> pyspark.sql.readwriter.DataFrameWriterV2\n",
      " |      Create a write configuration builder for v2 sources.\n",
      " |      \n",
      " |      This builder is used to configure and execute write operations.\n",
      " |      \n",
      " |      For example, to append or create or replace existing tables.\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      table : str\n",
      " |          Target table name to write to.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrameWriterV2`\n",
      " |          DataFrameWriterV2 to use further to specify how to save the data\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      >>> df.writeTo(\"catalog.db.table\").append()  # doctest: +SKIP\n",
      " |      >>> df.writeTo(                              # doctest: +SKIP\n",
      " |      ...     \"catalog.db.table\"\n",
      " |      ... ).partitionedBy(\"col\").createOrReplace()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  columns\n",
      " |      Returns all column names as a list.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          List of column names.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      >>> df.columns\n",
      " |      ['age', 'name']\n",
      " |  \n",
      " |  dtypes\n",
      " |      Returns all column names and their data types as a list.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          List of columns as tuple pairs.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      >>> df.dtypes\n",
      " |      [('age', 'bigint'), ('name', 'string')]\n",
      " |  \n",
      " |  isStreaming\n",
      " |      Returns ``True`` if this :class:`DataFrame` contains one or more sources that\n",
      " |      continuously return data as it arrives. A :class:`DataFrame` that reads data from a\n",
      " |      streaming source must be executed as a :class:`StreamingQuery` using the :func:`start`\n",
      " |      method in :class:`DataStreamWriter`.  Methods that return a single answer, (e.g.,\n",
      " |      :func:`count` or :func:`collect`) will throw an :class:`AnalysisException` when there\n",
      " |      is a streaming source present.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is evolving.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool\n",
      " |          Whether it's streaming DataFrame or not.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.readStream.format(\"rate\").load()\n",
      " |      >>> df.isStreaming\n",
      " |      True\n",
      " |  \n",
      " |  na\n",
      " |      Returns a :class:`DataFrameNaFunctions` for handling missing values.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrameNaFunctions`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.sql(\"SELECT 1 AS c1, int(NULL) AS c2\")\n",
      " |      >>> type(df.na)\n",
      " |      <class '...dataframe.DataFrameNaFunctions'>\n",
      " |      \n",
      " |      Replace the missing values as 2.\n",
      " |      \n",
      " |      >>> df.na.fill(2).show()\n",
      " |      +---+---+\n",
      " |      | c1| c2|\n",
      " |      +---+---+\n",
      " |      |  1|  2|\n",
      " |      +---+---+\n",
      " |  \n",
      " |  rdd\n",
      " |      Returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.range(1)\n",
      " |      >>> type(df.rdd)\n",
      " |      <class 'pyspark.rdd.RDD'>\n",
      " |  \n",
      " |  schema\n",
      " |      Returns the schema of this :class:`DataFrame` as a :class:`pyspark.sql.types.StructType`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`StructType`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame(\n",
      " |      ...     [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
      " |      \n",
      " |      Retrieve the schema of the current DataFrame.\n",
      " |      \n",
      " |      >>> df.schema\n",
      " |      StructType([StructField('age', LongType(), True),\n",
      " |                  StructField('name', StringType(), True)])\n",
      " |  \n",
      " |  sparkSession\n",
      " |      Returns Spark session that created this :class:`DataFrame`.\n",
      " |      \n",
      " |      .. versionadded:: 3.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`SparkSession`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.range(1)\n",
      " |      >>> type(df.sparkSession)\n",
      " |      <class '...session.SparkSession'>\n",
      " |  \n",
      " |  sql_ctx\n",
      " |  \n",
      " |  stat\n",
      " |      Returns a :class:`DataFrameStatFunctions` for statistic functions.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrameStatFunctions`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import pyspark.sql.functions as f\n",
      " |      >>> df = spark.range(3).withColumn(\"c\", f.expr(\"id + 1\"))\n",
      " |      >>> type(df.stat)\n",
      " |      <class '...dataframe.DataFrameStatFunctions'>\n",
      " |      >>> df.stat.corr(\"id\", \"c\")\n",
      " |      1.0\n",
      " |  \n",
      " |  storageLevel\n",
      " |      Get the :class:`DataFrame`'s current storage level.\n",
      " |      \n",
      " |      .. versionadded:: 2.1.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`StorageLevel`\n",
      " |          Currently defined storage level.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df1 = spark.range(10)\n",
      " |      >>> df1.storageLevel\n",
      " |      StorageLevel(False, False, False, False, 1)\n",
      " |      >>> df1.cache().storageLevel\n",
      " |      StorageLevel(True, True, False, True, 1)\n",
      " |      \n",
      " |      >>> df2 = spark.range(5)\n",
      " |      >>> df2.persist(StorageLevel.DISK_ONLY_2).storageLevel\n",
      " |      StorageLevel(True, False, False, False, 2)\n",
      " |  \n",
      " |  write\n",
      " |      Interface for saving the content of the non-streaming :class:`DataFrame` out into external\n",
      " |      storage.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataFrameWriter`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      " |      >>> type(df.write)\n",
      " |      <class '...readwriter.DataFrameWriter'>\n",
      " |      \n",
      " |      Write the DataFrame as a table.\n",
      " |      \n",
      " |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tab2\")\n",
      " |      >>> df.write.saveAsTable(\"tab2\")\n",
      " |      >>> _ = spark.sql(\"DROP TABLE tab2\")\n",
      " |  \n",
      " |  writeStream\n",
      " |      Interface for saving the content of the streaming :class:`DataFrame` out into external\n",
      " |      storage.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is evolving.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`DataStreamWriter`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import tempfile\n",
      " |      >>> df = spark.readStream.format(\"rate\").load()\n",
      " |      >>> type(df.writeStream)\n",
      " |      <class 'pyspark.sql.streaming.readwriter.DataStreamWriter'>\n",
      " |      \n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Create a table with Rate source.\n",
      " |      ...     df.writeStream.toTable(\n",
      " |      ...         \"my_table\", checkpointLocation=d) # doctest: +ELLIPSIS\n",
      " |      <pyspark.sql.streaming.query.StreamingQuery object at 0x...>\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n",
      " |  \n",
      " |  mapInArrow(self, func: 'ArrowMapIterFunction', schema: Union[pyspark.sql.types.StructType, str]) -> 'DataFrame'\n",
      " |      Maps an iterator of batches in the current :class:`DataFrame` using a Python native\n",
      " |      function that takes and outputs a PyArrow's `RecordBatch`, and returns the result as a\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      The function should take an iterator of `pyarrow.RecordBatch`\\s and return\n",
      " |      another iterator of `pyarrow.RecordBatch`\\s. All columns are passed\n",
      " |      together as an iterator of `pyarrow.RecordBatch`\\s to the function and the\n",
      " |      returned iterator of `pyarrow.RecordBatch`\\s are combined as a :class:`DataFrame`.\n",
      " |      Each `pyarrow.RecordBatch` size can be controlled by\n",
      " |      `spark.sql.execution.arrow.maxRecordsPerBatch`. The size of the function's input and\n",
      " |      output can be different.\n",
      " |      \n",
      " |      .. versionadded:: 3.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : function\n",
      " |          a Python native function that takes an iterator of `pyarrow.RecordBatch`\\s, and\n",
      " |          outputs an iterator of `pyarrow.RecordBatch`\\s.\n",
      " |      schema : :class:`pyspark.sql.types.DataType` or str\n",
      " |          the return type of the `func` in PySpark. The value can be either a\n",
      " |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import pyarrow  # doctest: +SKIP\n",
      " |      >>> df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
      " |      >>> def filter_func(iterator):\n",
      " |      ...     for batch in iterator:\n",
      " |      ...         pdf = batch.to_pandas()\n",
      " |      ...         yield pyarrow.RecordBatch.from_pandas(pdf[pdf.id == 1])\n",
      " |      >>> df.mapInArrow(filter_func, df.schema).show()  # doctest: +SKIP\n",
      " |      +---+---+\n",
      " |      | id|age|\n",
      " |      +---+---+\n",
      " |      |  1| 21|\n",
      " |      +---+---+\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is unstable, and for developers.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pyspark.sql.functions.pandas_udf\n",
      " |      pyspark.sql.DataFrame.mapInPandas\n",
      " |  \n",
      " |  mapInPandas(self, func: 'PandasMapIterFunction', schema: Union[pyspark.sql.types.StructType, str]) -> 'DataFrame'\n",
      " |      Maps an iterator of batches in the current :class:`DataFrame` using a Python native\n",
      " |      function that takes and outputs a pandas DataFrame, and returns the result as a\n",
      " |      :class:`DataFrame`.\n",
      " |      \n",
      " |      The function should take an iterator of `pandas.DataFrame`\\s and return\n",
      " |      another iterator of `pandas.DataFrame`\\s. All columns are passed\n",
      " |      together as an iterator of `pandas.DataFrame`\\s to the function and the\n",
      " |      returned iterator of `pandas.DataFrame`\\s are combined as a :class:`DataFrame`.\n",
      " |      Each `pandas.DataFrame` size can be controlled by\n",
      " |      `spark.sql.execution.arrow.maxRecordsPerBatch`. The size of the function's input and\n",
      " |      output can be different.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : function\n",
      " |          a Python native function that takes an iterator of `pandas.DataFrame`\\s, and\n",
      " |          outputs an iterator of `pandas.DataFrame`\\s.\n",
      " |      schema : :class:`pyspark.sql.types.DataType` or str\n",
      " |          the return type of the `func` in PySpark. The value can be either a\n",
      " |          :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from pyspark.sql.functions import pandas_udf\n",
      " |      >>> df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
      " |      >>> def filter_func(iterator):\n",
      " |      ...     for pdf in iterator:\n",
      " |      ...         yield pdf[pdf.id == 1]\n",
      " |      >>> df.mapInPandas(filter_func, df.schema).show()  # doctest: +SKIP\n",
      " |      +---+---+\n",
      " |      | id|age|\n",
      " |      +---+---+\n",
      " |      |  1| 21|\n",
      " |      +---+---+\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is experimental\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pyspark.sql.functions.pandas_udf\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.sql.pandas.map_ops.PandasMapOpsMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.sql.pandas.conversion.PandasConversionMixin:\n",
      " |  \n",
      " |  toPandas(self) -> 'PandasDataFrameLike'\n",
      " |      Returns the contents of this :class:`DataFrame` as Pandas ``pandas.DataFrame``.\n",
      " |      \n",
      " |      This is only available if Pandas is installed and available.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method should only be used if the resulting Pandas ``pandas.DataFrame`` is\n",
      " |      expected to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      Usage with ``spark.sql.execution.arrow.pyspark.enabled=True`` is experimental.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> df.toPandas()  # doctest: +SKIP\n",
      " |         age   name\n",
      " |      0    2  Alice\n",
      " |      1    5    Bob\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(google_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['year ASC NULLS FIRST, 'month ASC NULLS FIRST], true\n",
      "+- Aggregate [year#248, month#249], [year#248, month#249, avg(Low#52) AS avg(Low)#257]\n",
      "   +- Project [year(Date#49) AS year#248, month(Date#49) AS month#249, Low#52]\n",
      "      +- Relation [Date#49,Open#50,High#51,Low#52,Close#53,AdjClose#54,Volume#55] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "year: int, month: int, avg(Low): double\n",
      "Sort [year#248 ASC NULLS FIRST, month#249 ASC NULLS FIRST], true\n",
      "+- Aggregate [year#248, month#249], [year#248, month#249, avg(Low#52) AS avg(Low)#257]\n",
      "   +- Project [year(Date#49) AS year#248, month(Date#49) AS month#249, Low#52]\n",
      "      +- Relation [Date#49,Open#50,High#51,Low#52,Close#53,AdjClose#54,Volume#55] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [year#248 ASC NULLS FIRST, month#249 ASC NULLS FIRST], true\n",
      "+- Aggregate [year#248, month#249], [year#248, month#249, avg(Low#52) AS avg(Low)#257]\n",
      "   +- Project [year(Date#49) AS year#248, month(Date#49) AS month#249, Low#52]\n",
      "      +- Relation [Date#49,Open#50,High#51,Low#52,Close#53,AdjClose#54,Volume#55] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [year#248 ASC NULLS FIRST, month#249 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(year#248 ASC NULLS FIRST, month#249 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=241]\n",
      "      +- HashAggregate(keys=[year#248, month#249], functions=[avg(Low#52)], output=[year#248, month#249, avg(Low)#257])\n",
      "         +- Exchange hashpartitioning(year#248, month#249, 200), ENSURE_REQUIREMENTS, [plan_id=238]\n",
      "            +- HashAggregate(keys=[year#248, month#249], functions=[partial_avg(Low#52)], output=[year#248, month#249, sum#263, count#264L])\n",
      "               +- Project [year(Date#49) AS year#248, month(Date#49) AS month#249, Low#52]\n",
      "                  +- FileScan csv [Date#49,Low#52] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/c:/Users/trand/OneDrive/Documents/After Zipcode/SQL SPARK/AMZN.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Date:date,Low:double>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "amazon_df.select(year('Date').alias('year'), month('Date').alias('month'),'Low').groupby('year','month').avg('Low').sort('year','month').explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
